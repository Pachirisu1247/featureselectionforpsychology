---
title: "Intermediate Models for Prediction and Variable Selection for Psychologists - Random Forest and LASSO"
author: Francisco N. Ramos
format:
  pdf: default
  html: default
  gfm: default
knitr:
  opts_chunk: 
    collapse: true
    echo: true
    results: "markup"
    message: true
    warning: true
    R.options:
      knitr.graphics.auto_pdf: true
---

## Intermediate Statistical Models for Prediction and Variable Selection for Psychologists - Random Forest and LASSO

Should start w explaining less about variable selection and way more about why LASSO and random forest are each good and great.

In this document, I will walk through a random forest variable selection procedure for a psychological dataset. The dataset is extremely characteristic of many psychology datasets - it has an okay but not at all large sample size (735 N before being split into test/training sets), a relatively high number of potential predictors (20+ collected as part of the survey, many of which are likely completely unhelpful in prediction), and what is likely middling to low effect sizes for predictors. The focus of the model will be regression and we will be primarily discussing model error and predictive power, and alternating focuses between the two. We'll review a few different ways of creating models, selecting features, and evaluating models.

However, be aware that this is not a tutorial on identifying the absolute best predictive model, which is described ad infinitum elsewhere. I will touch on using algorithmic methods to identify the "optimal" subset of predictors in a full model, but if you are merely looking for "the best model to fit my data" or "the best way to predict outcome from a set of predictors", this document will likely not be the most helpful.

Another note: we will be partially demonstrating what statisticians would call "stepwise selection", "backwards selection" to be specific. Let me just be clear - backwards selection is NEVER the best way to do variable selection. The best feature selection method will always be expert knowledge on which variables are likely to explain the majority of the variance/signal as well as which less-important variables are likely to provide valuable interactions. If possible, you should either a priori identify the subset of variables to evaluate OR if you must use data-driven selection, try to use automated methods with clear "reward" and "loss" functions so you better understand why it has penalized what it penalized and why it has selected what it selected. Backwards selection has the nasty little habit of occasionally overemphasizing variables that actually have little relationship with the DV because they happen to "fit in" nicely with the other variables, making the variable appear important in the specific models backwards selection evaluates, but the variable may be unimportant in the "true" best model for prediction (or the model containing the "true" subset of actually important variables, if you believe in that). We are going through stepwise selection because it is common but more importantly because it does help you better understand the relationships between variables in the specific model, and it is easy to run an automated method without really thinking about why those variables have been selected and what it is showing you. Examples of three various potential automated methods are provided before the end of this document, however.

I have not seen the data ahead of time. The analyses described hereafter are entirely novel.

## The Data

<!-- HL: suggest to use --- for em dashes -->
Importantly, we are conducting a regression task, not a classification task, as our dependent variable is continuous. The main task we'll be attempting to address is the following - we would like to predict arithmetic performance score in German students from a set of predictor variables including age, sex, self-reported math anxiety, self-reported math self-concept, and others. Our second main task is that we would like to identify which of these variables are actually useful in predicting arithmetic performance and which ones are not, so future researchers do not have to waste resources collecting unimportant variables for minimal improvement in prediction. In other words, we would like a concise model. This data was originally gathered and shared by Cipora et al. (2024). References are at the end of the document. <!-- HL: can use citation syntax in quarto: https://quarto.org/docs/authoring/citations.html -->

Since the dataset contains over 40 potential predictor variables and only around <!-- HL: this part is unclear --> 735 N pre-training/test split, I have selected ten variables from the dataset at pseudo-random for our analysis. For the purposes of this document, only these ten variables "exist". Below, I will list the ten variables we are interested in studying as predictors of arithmetic performance score followed by the syntax for their object in R. All assessments were in German.

***Dependent Variable:***\
Arithmetic performance/`sum_arith_perf` <!-- HL: use `` for variable names -->, as measured by "the number of correctly solved problems in order as instructed" on a simple arithmetic speed test.

***Predictors:***\
Age/`age`, as measured in years.

Sex/`sex`, where 1 = male, 2 = female, and 3=other. Participants who ignored this question were removed.

Neuroticism/`score_BFI_N`, as measured by the sum score of the 8 items of the Big Five Inventory (short version) pertaining to neuroticism.

Math anxiety/`score_AMAS_total`, as measured by the sum score on the Abbreviated Math Anxiety Scale.

General trait anxiety/`score_GAD`, as measured by sum score on the Generalized Anxiety Disorder Screener (GAD-7).

Math self-efficacy/`score_PISA_ME`, as measured in the PISA 2012 study using the sum score of six items.

General state anxiety/`score_STAI_state_short`, as assessed by the sum of the five-item scale STAI-SKD.

Test anxiety/`score_TAI_short`, as measured by the sum score of the 5 items on the short version of the Test Anxiety Inventory.

Math self-concept/`score_SDQ_M`, as measured by the sum score of the four math-related statements on the Self-Description Questionnaire III. Evaluates variables such as one's comfort/enjoyment/pride with math, whereas self-efficacy evaluates one's self-confidence in math abilities.

Language self-concept/`score_SDQ_L`, as measured by the sum score of the four language-related statements on the Self-Description Questionnaire III.

## <!-- HL: Add more subheads -->
***Cleaning the Data***

We'll start with importing and cleaning the data to make sure it fits our task.

<!-- HL: the `stats` package is built-in and doesn't need to be loaded. `tidyverse` loads some other packages too. -->

```{r}
#| label: Load packages, clean data
#| warning: false
library(here)
library(tidyverse)
library(caret)
library(readxl)
library(randomForest)
```

The `here` function is extremely convenient for creating paths and directories that won't break when files are moved or when you reproduce the work on another computer. "Here" on your computer will be whatever the top level folder of the current project folder is (which you set up when initializing a project). Looks like "here" for me is `r here()` You can also set it using the set_here function. You can specify subfolders using a comma, then quotation marks to define the name of each folder or file.You don't even need to worry about working directories if you use the here function. Yay!

Below, we'll again use the here function to tell the readr function where to go to get the dataset. Our data was stored in the Main Script folder, in a subfolder called OSF archive, and named "AMATUS_dataset.csv." Use that info to read the code below. Notice how we use a `normalizePath()` <!-- HL: we generally use `fun()` to denote R function --> function to convert relative or incomplete path names to absolute path names. That way, we don't have to worry too much about Mac vs. Windows or cloud vs. local.

<!-- HL: Since you'll be using the `amatusclean` dataset, would it make sense to do as.factor() on the cleaned data set? -->

```{r}
# here::i_am("Dr. Lai Feature Selection Project 9.13.24.rproj")
#If you'd like to, you can manually set your path with the command above. #I've set it to my rproj file
#Otherwise, just check with the following
#command:
# here::here()

# adjust as needed.
# file_path <- normalizePath(here::here("Main Script/OSF archive/AMATUS_dataset.csv"))
file_path <- url("https://osf.io/download/wzrqb/")
# adjust as needed.

amatus <- read_csv2(file_path, c("", "NA"), col_names = TRUE) 
# it's already in the "working directory" of the main script folder, 
# so we only need the subfolder OSF archive and the actual name of the dataset.

View(amatus)
# We can use this command to view our data.
table(amatus$sum_arith_perf)

#### Just some regular old data cleaning to start with.
# HL: Alternative code
amatus <- amatus |>
    mutate(across(c("sex", "age_range", "breaks", "honesty", "native_speaker", "noise"),
           .fns = as_factor))
# amatus$sex <- as.factor(amatus$sex)
# amatus$age_range <- as.factor(amatus$age_range)
# amatus$breaks <- as.factor(amatus$breaks)
# amatus$honesty <- as.factor(amatus$honesty)
# amatus$native_speaker <- as.factor(amatus$native_speaker)
# amatus$noise <- as.factor(amatus$noise)

amatusclean <- amatus[!is.na(amatus$sum_arith_perf), ] # removing the
# individuals who did not complete the performance test in order as instructed
amatusclean <- amatusclean[!(amatusclean$sample %in% 
c("german_teachers", "belgian_teachers")), ] 
# removing the other two samples from the dataset

View(amatusclean)
```

Now, we'll just check out a quick histogram of the dependent variable, arithmetic math performance.

<!-- HL: I think a histogram is more appropriate given the outcome is somewhat discrete. Also, should we use amatusclean? -->

<!-- HL: Consider adding a caption to the figures. I give an example below. -->

```{r}
#| label: fig-arith-perf
#| fig-cap: "Distribution of outcome variable: Arithmetic Math Performance"
#| warning: false
ggplot(
  amatus,
  aes(x = sum_arith_perf)
) +
  geom_density(fill = "lightblue", alpha = 0.7) +
  # HL: you can use theme_set(theme_minimal()) in the beginning to set the style for all graphs.
  theme_minimal()
```

As you might expect from test scores, our data is clearly skewed right (@fig-arith-perf). There's no missing data for our Y variable of performance since we are not interested in participants who answered out of order against instructions. Let's also look at some predictors.

```{r}
#| label: A Couple of Predictor Histograms
ggplot(
  amatus,
  aes(x = score_AMAS_total)
) +
  geom_density(fill = "lightblue", alpha = 0.7) +
  theme_minimal()

ggplot(
  amatus,
  aes(x = score_BFI_N)
) +
  geom_density(fill = "lightblue", alpha = 0.7) +
  theme_minimal()
```

One predictor's distribution is very normal, the other isn't. I'll save you the trouble and tell you that the rest of the features are a grab bag too. Normally we wouldn't care about standardization given that random forest is a tree-based model. However, because we are interested in variable importance metrics (since we are trying to focus on feature selection), we actually do need to scale our features. This is because variable importance metrics are skewed towards predictors that have a wider range of values. So a continuous variable may return a more biased estimate in the variable importance metric than a variable that can take 3 or 4 values. So we do want to scale our variables this time around.

<!-- HL: Is there a specific reason for choosing `scale()` over `preProcess()`? Wouldn't it be simpler with the latter instead having to type `scale()` every time? -->

We'll show code for an alternative way of scaling predictors, but the way we will choose to do it is use the `scale()` function to create a scaled training set. This will center (subtract mean from the value for a mean of 0) and scale (divide value by its standard deviation for an SD of 1) our continuous predictors. The training set will then only include the scaled variables. For our categorical variables, just remember not to worry about the scale function. We'll go ahead and get started by creating our first random forest model. Let's try using the `caret` package, a relatively general machine learning package.

A side note: There are several different validation methods you can use in `caret`, including bootstrapping, cross-validation, and repeated cross-validation. Repeated cross-validation should always be used when computationally appropriate. Bootstrapping may be compelling here due to the small dataset, but repeated cross-validation should work just as well as bootstrap in smaller samples and should do the job for us just fine. You can increase the `"number"` argument to increase the number of folds in the dataset and you can increase the `"repeat"` argument to increase the number of times the entire cross validation gets repeated, both at the expense of computational power.

## First Model Setup

<!-- HL: I'm missing `mtry` in the following code as it doesn't run -->

```{r}
#| label: First model setup
library(caret)
set.seed(39) # I'm going to put this before every random generation just for
# clarity. Before data partition, please.
inTrain <- createDataPartition(amatusclean$sum_arith_perf,
  p = 0.7, list = FALSE
)
# 0.7 selected to have a decent number of N in the test set

# create training vs test data.
training <- amatusclean[inTrain, ]
test <- amatusclean[-inTrain, ]
View(training)
View(test)


##### There are two methods to include scaling of predictor variables: manual
# scaling of predictors (and predictions later on), and preprocessing, which
# means using an R function to "process" your data to put it in the model.
# I will show an example with preProcess from the caret package, however
# the "recipes" package may be even more powerful for preprocessing and less
# inconvenient, since preProcess is usually used for scaling all variables
# in a dataset at once (and therefore is harder to use with subsequent models,
# since you need to tune the application of your preprocessing.)
#####
# I'm going to pull out the dependent variable.
trainnodv <- training
preProc <- preProcess(training, method = c("center", "scale"))
trainscaled <- predict(preProc, training)
# Again, we will not be using this, but training data would be standardized in
# trainscaled.

# Let's try doing it in a more general way.
numeric_predictors <- c(
  "score_BFI_N", "score_AMAS_total", "age", "score_GAD",
  "score_PISA_ME", "score_STAI_state_short",
  "score_TAI_short", "score_SDQ_L", "score_SDQ_M"
)
categorical_variable <- "sex"
response_variable <- "sum_arith_perf"

# Create a new dataframe by scaling and centering numeric predictors.
# library(dplyr)  # HL: redundant after library(tidyverse)
scaled_training <- training %>%
  mutate(across(all_of(numeric_predictors), ~ scale(.) %>% as.vector())) %>%
  # Scale and convert back to vector just in case
  select(all_of(numeric_predictors), all_of(categorical_variable), all_of(response_variable)) # Select only the relevant columns

# View the scaled data
View(scaled_training)

# mtry is a parameter of the number of randomly selected predictors to
# consider splitting at each node of the decision tree. 
# More on this after the code.
control <- trainControl(
  method = "repeatedcv", number = 10, repeats = 3,
  search = "random"
) 
# picks random mtries in the range 1 to # of predictors. 
# Can pick the same value twice, so not ideal for me.

control <- trainControl(
  method = "repeatedcv", number = 10, repeats = 3,
  search = "grid"
)
# this one picks preselected values for mtry, which you would define
# on your own. We will be using this
tunegrid <- expand.grid(.mtry = c(2:8)) 
# This function creates a data frame from all
# combinations of the supplied vectors/factors. In other words, this
#creates the grid of values for the hyperparameter mtry. 
# This one says try every value including 
# and between 2 and 8. You can't try more #mtries than there are predictors.
# We're going to stick with this for the most part throughout.


# let's start with all values of mtry (1 to number of predictors, which is 10) for the very first model.
tunegrid <- expand.grid(.mtry = c(1:10)) 


############
# You can also try to use "default" values. 
x <- amatusclean[, 1:10]
mtry <- ncol(x) / 3 
# in random forest for regression, predictors/3 is
# often the "default" value of this parameter. 
# In classification, it is the square root of the # of predictors.
# Just including this here in case you want to try it.
# But make sure to fix your tunegrid argument too!!!!!

# As you'll remember, the REAL parameters are below:
control <- trainControl(
  method = "repeatedcv", number = 10, repeats = 3,
  search = "grid"
)
tunegrid <- expand.grid(.mtry = c(1:10)) 

#Now, to run the random forest
set.seed(40)
# if you want to set seed for replicability, put this RIGHT 
# before every random thing
rf_scaled <- train(
  sum_arith_perf ~ scale(score_AMAS_total) + sex + scale(age) + 
    scale(score_BFI_N) + scale(score_GAD) + scale(score_PISA_ME) +
    scale(score_STAI_state_short) + scale(score_TAI_short) +
    scale(score_SDQ_M) + scale(score_SDQ_L),
  data = training,
  importance = TRUE, method = "rf", tuneGrid=tunegrid, trControl = control
)
# Notice that we are using data = training, NOT the scaled
# training set, as well as the scale() function.
# Also, no tuneLength argument. Just trControl.

print(rf_scaled)
varImpPlot(rf_scaled$finalModel, type = 1)
```

As you can see in the first model (`rf_scaled`), If it was up to `caret`, the best predictive method would have us randomly sample 1 predictor to consider splitting at each node of the decision tree. That means we probably have very few predictors that actually provide value in predicting math performance. This is expected in this dataset and in much psychological research, as we stated early on. Although an `mtry=1` is too random, we clearly need to use a low `mtry`. The difference between the default (# of predictors/3) number for `mtry` and `mtry=2` isn't too big, so we can stick with either for now. We'll try to set 2 as the minimum and make sure our code test a range of mtries, just in case another value ends up being better than 2 (which I don't expect to be the case).

Let's exclude 1 and go again and try more options.

```{r}
#| label: Excluding value of 1

control <- trainControl(
  method = "repeatedcv", number = 10, repeats = 3,
  search = "grid"
)
# the term "grid" means it will picks preselected values for mtry, 
# which you would define on your own.
# Remeber to change the control variable back to search = "random" if
# you aren't using a grid search.

tunegrid <- expand.grid(.mtry = c(2:8)) 
# This one says try values between and including 2-8.
# You can't try more mtries than there are predictors. We're going to stick with this throughout. Since our optimal mtry at first was 1, the optimal
# mtry is probably going to be low (and will likely be 2).


# I'm going to put this before every random generation just for clarity.
set.seed(40)
rf_mtry2 <- train(
  sum_arith_perf ~ score_AMAS_total + sex + age + score_BFI_N +
    score_GAD + score_PISA_ME + score_STAI_state_short + score_TAI_short +
    score_SDQ_M + score_SDQ_L,
  data = scaled_training, importance = TRUE,
  method = "rf", tuneGrid = tunegrid, trControl = control
)
# Notice we keep using the importance=TRUE parameter, which calculates variable
# importance metrics. We need this to be set to TRUE to be able to see
# varimp plots later.

print(rf_mtry2)
varImpPlot(rf_mtry2$finalModel, type = 1)

```

<!-- HL: Again, need more headings -->

R squared of around .14ish is obviously mildly low, but it's actually decent for psychological research. In published papers, you'll often happen to see r-squareds around .15-.25 or so, if that. It's very hard to build an extremely accurate model of human behavior namely because an incalculable number of factors influence our behavior in seen and unseen ways. Not to mention we care more about feature selection for than perfect prediction, which is why we are scaling variables in the first place. If we just wanted the best predictive model,

The `VarImpplot()` function assesses the variable importance for each of the variables in our model, measured by the increase in Mean square error should the variable be removed from the model. As you can see, the lowest variable is extremely irrelevant to the model, that variable being neuroticism. Neuroticism seems to be a bad predictor of arithmetic performance, but be careful---however unlikely, it may instead be the case that all the variance neuroticism explains is better explained by other variables in the model (if correlation is high). It may also improve performance through stronger correlation with the IVs than the DV. It could be a useful variable in a different or low-dimensional model (though it's unlikely).

In fact, here's something cool: when you run a model, variables with a negative value for variable importance means that the error was higher when using the original predictor variable values than when using the permuted ones. Obviously, this tends to mean the variable doesn't have a role in prediction. So, you can help identify unimportant variables in (usually low-dimension) models with variable importance. However, keep in mind that occasionally, low importance variables can become "important" in models with fewer dimensions (particularly if they correlate strongly with other independent variables or happen to explain the noise well). This can be an issue if it is overfitting random noise in the training data and does not generalize well to test data, but if it is instead just reducing the error in your other "important" variables significantly, it may still generalize well to test data.

Test anxiety also seems like an unimportant predictor. Test anxiety may also be a bad predictor of arithmetic performance, or maybe more likely, it is not doing anything the state, math or general anxiety variables aren't doing already. Our models also seem to agree (so far) that language self-concept may be pretty useless, which makes sense conceptually.

It also looks like the models we run think that math self-concept, sex, math self-efficacy and possibly math anxiety are the most important variables. Just looking at the graphs, there does indeed seem to be around 2-5ish variables here that actually predict a significant amount of variance in our DV, so that's probably a great number for a parsimonious model.

Now, to fit our goal of parsimony, we want to cut out the variables that don't help us very much because we run the risk of creating a very nonparsimonious model, and of overfitting of course. For example, age seems to be mildly important, but is it important enough that we need it in our model to have accurate estimates? Well, from the model above, it kind of seems that there's only one, maybe two predictors that are extremely irrelevant to the model. This is probably in part because the effect sizes of most variables are so weak that removing any one variable can significantly reduce the predictive power of the model. In other words, the best predictive model might be a 9 or 10 variable model, but the best parsimonious model (which contains only the most relevant predictors at the cost of retaining all variables with even minor predictive power) may be somewhere in the 2-5 variable range.

So anyways, we'll continue now. We'll start with a 9 variable model, which we'll create FIRST by cutting BFI.

## Continuing RFE

```{r}
#| label: First selection, cutting test anxiety
set.seed(41)
rf_firstselection <- train(
  sum_arith_perf ~ score_AMAS_total + sex + age +
    score_TAI_short + score_GAD + score_PISA_ME + score_STAI_state_short +
    score_SDQ_M + score_SDQ_L,
  data = scaled_training, method = "rf",
  importance = TRUE, tuneGrid = tunegrid, trControl = control
)
print(rf_firstselection)
# we'll look at variable importance again in a minute.
```

```{r}
varImpPlot(rf_firstselection$finalModel, type = 1)
# type 1 refers to estimates following perturbation.
```

We're seeing a similar pattern. Math self-efficacy and sex seem to be two of the most powerful variables in every model, while math self-concept and some math anxiety variables trail not too far behind (the rest seem to hang in the balance). It seems like of the ten we chose, those are some of the most important variables needed to predict math performance in German students. RMSE and R squared improved! But these numbers fluctuate anyways given the nature of random forest models. We'll take a look at how these models predict the test set later.

Now, we're going to try to run more random forest models with a reduced set of predictors since we are at least starting to identify the most important predictors. Let's see if we can find a more parsimonious model with still good predictive power.

Since AMAS measures math anxiety, which conceptually seems to map on to math performance better than general anxiety, it makes sense that state anxiety and general anxiety are also relatively unimportant predictors. General anxiety doesn't seem to add much value to predicting math performance when math anxiety is already accounted for, and state anxiety isn't much better. Test anxiety (from earlier) being a weak predictor is not as intuitive, but it's possible that the most salient aspect here is that the test is on math, not that there's a test itself, so anxiety of tests in general doesn't really have predictive power of performance the same way math anxiety does. Regardless, they likely cover a lot of each other's correlation.

Either way, we should start by cutting language self-efficacy. It's clearly just not a very good predictor.

```{r}
#| warning: false
#| label: Second selection

View(scaled_training)
set.seed(42)
rf_secondselection <- train(
  sum_arith_perf ~ score_AMAS_total + sex + age +
     score_TAI_short + score_STAI_state_short + score_PISA_ME + score_SDQ_M +
    score_GAD,
  data = scaled_training, method = "rf", importance = TRUE,
  tuneGrid = tunegrid, trControl = control
)
print(rf_secondselection)
```

```{r}
varImpPlot(rf_secondselection$finalModel, type = 1)
# type 1 refers to estimates following perturbation.
```

Slight reduction in RMSE and increase in (training data) R-squared! Importantly, the variable importance metrics are starting to look SLIGHTLY better - the lowest incMSE% is around 3%. great news for our goal of parsimonious modeling. However, I'm not sure if this will continue. It's okay if while going through variable selection, your model doesn't necessarily have a massive reduction in RMSE or massive increase in r squared. Remember that we're looking to explain the most variance with as few variables (parsimony), so sometimes parsimony is worth slight decreases in predictive accuracy (though you should make sure you're not cutting a seemingly low-predictive power variable that nonetheless has explanatory importance).

This may be a decent model. The fact that the \$incMSE is starting to approach reasonable numbers means we're starting to run out of questionable predictors. This model is likely better than the full model, but also likely not the best model of sheer prediction. We may not need all those measures of anxiety, and like we said earlier, we likely are looking for a 2-5 variable model to cover almost all of the variance with the fewest variables possible. So we should keep going before we start prediction.

Let's cut the next least important variable - score_GAD.

```{r}
#| warning: false
#| label: Third selection
set.seed(43) #
rf_thirdselection <- train(
  sum_arith_perf ~ score_AMAS_total + sex + age +
    score_STAI_state_short + score_PISA_ME + score_TAI_short + score_SDQ_M,
  data = scaled_training, method = "rf", importance = TRUE,
  tuneGrid = tunegrid, trControl = control
)
print(rf_thirdselection)

# we'll look at variable importance again later.
```

Slight reduction in R squared seems odd, but it's definitely possible that the model got worse after removing GAD. It's also possible that this model fits the train data well but will not generalize well to the test data. We'll see.

```{r}
varImpPlot(rf_thirdselection$finalModel, type = 1)
# type 1 refers to estimates following perturbation.
```

The RMSE did go up. This doesn't seem better than the previous models at the moment. Let's keep going. By the way, at this point the highest mtry being tried (7) is greater than number of predictors (6). It still works without it but it produces a lot of annoying warnings, so we'll start to include a cap line at the beginning of the chunk of code for your convenience (before the set seed, importantly). We're cutting test anxiety next.

```{r}
#| label: Fourth selection
tunegrid <- expand.grid(.mtry = c(2:6))
set.seed(44)
rf_fourthselection <- train(
  sum_arith_perf ~ score_AMAS_total + sex + age +
    score_STAI_state_short + score_PISA_ME + score_SDQ_M,
  data = scaled_training,
  method = "rf", importance = TRUE, tuneGrid = tunegrid, trControl = control
)
print(rf_fourthselection)
# we'll look at variable importance again later.
```

```{r}
varImpPlot(rf_fourthselection$finalModel, type = 1)
# type 1 refers to estimates following perturbation.
```

Hmm, looks like the anxiety variables aren't really helping us all that much. However, it is a bit odd that these anxiety variables tend to flip flop with their importance in these graphs. I've also noticed that age, which was once near the bottom of the "importance" graph, continues to move upwards. At this point, I start to wonder if the math anxiety variables may add predictive power, despite seemingly being not the variables with the best zero-order correlations with the DV. Not to mention that the %IncMSE is relatively high, even enough that this sort of model starts to look pretty appealing. However, let's keep going, as this doesn't seem to be a great model either.

I suspect the MOST parsimonious model may either have only one anxiety variable (likely math anxiety) or no anxiety variables. Let's cut state anxiety, then.

```{r}
#| label: Fifth selection
tunegrid <- expand.grid(.mtry = c(2:5))
set.seed(45)
rf_fifthselection <- train(
  sum_arith_perf ~ sex + age + score_AMAS_total +
    score_PISA_ME + score_SDQ_M,
  data = scaled_training, method = "rf",
  importance = TRUE, tuneGrid = tunegrid, trControl = control
)
print(rf_fifthselection)

```

```{r}
varImpPlot(rf_fifthselection$finalModel, type = 1)
# type 1 refers to estimates following perturbation.
```

The models continue to worsen. It seems like some of these variables that we've removed for seeming middling may contribute to predictive accuracy, but we will check this with test data. However, it does seem that we have identified the key variables as being math self-efficacy and self-concept as well as sex, possibly with math (or state) anxiety and age. Also, if you haven't noticed, age used to be one of the predictors with the lowest incMSE% in the full model, but has stuck in the models we are testing. That indicates to me that age may have value in some models, but it seems to generally have little relationship with the dependent variable and should only be used in the model if it happens to be in the best predictive model or if you are convinced of its key relationship with another variable. If this were an actual research project, I would have conducted exploratory data analysis to check age's correlation with other variables as well as evaluated the part correlation in the full model to see if variables such as age were acting as suppressors of key variables such as math self-efficacy.

```{r}
#| label: Sixth selection
# just for fun, we'll use the in-formula scale() method to scale data.
tunegrid <- expand.grid(.mtry = c(2:4))
set.seed(46)
rf_sixthselection <- train(
  sum_arith_perf ~ sex + score_PISA_ME + score_SDQ_M +
    score_AMAS_total,
  data = scaled_training, method = "rf", importance = TRUE,
  tuneGrid = tunegrid, trControl = control
)
print(rf_sixthselection)

```

```{r}
varImpPlot(rf_sixthselection$finalModel, type = 1)
# type 1 refers to estimates following perturbation.
```

The MSE went up again, and by a decent amount. At this point, it seems safe to say that models are not getting any better - in fact, they're getting worse. The four variables above do seem to be the most "important" variables in terms of identifying predictors that genuinely correlate with the dependent variable, and math self-efficacy and sex are likely the top two. Of course, this has nothing to do with accuracy of prediction. The 4 or 5 variable models seem to be pretty good parsimonious models if we want to collect more than only 2 variables. In other words, we've identified 5 variables that can produce a good model that captures a good amount of the variability in scores. This model may be the best combination of parsimony and predictive power (in fact, it seems like it may be flat out the best predictive model), but this model is likely not the most parsimonious (that seems to be a model with only a couple predictors, as we've indicated before). There is also a chance it is not the most predictive (which we'll test soon).

<!-- HL: Given the length, I suggest splitting into multiple documents, and include them here. -->

<!-- {{< include auto-selection.qmd >>}} -->

<!-- {{< include predictions-test-set.qmd >>}} -->

I want to finish by reminding you that random forests are random, so you will get a new model every time you run the train command if you do not set the seed beforehand. With good predictors, a large data set and intensive repeated cross validation, it won't change much, but it will change. The "stepwise" procedure I followed here was primarily for narrative purposes and may not be the best choice to build the single best model at prediction or another task, but it will help you to evaluate the variable importance at each step and see how the different models predict the data (and which variables they seem emphasize). This will help you work to select the model that best fits your goal of parsimony, variable selection, prediction, all three, or another goal. Of course, if you have a lot of predictors, it may be best for you to use an automatic method such as VSURF, which is very similar to the process we utilized but tuned with an algorithm it attempts to optimize rather than with our statistical knowledge.

Like I mentioned earlier, this form of backwards selection we engaged in was primarily helpful for identifying the "key" predictors. This selection helped us identify the three most important predictors, as well as three additional helpful tentative predictors (math anxiety, state anxiety, age), of which the first two were identified by the Boruta algorithm as being predictors that seem to have true non-zero correlation with the DV. We probably would have saved time by leading with the Boruta algorithm, which goes to show that backwards selection is an uncertain and inexact process. However, it did help us learn to read random forest outputs and variable importance graphs, and hopefully

To finish: Just for fun and to show you the value of Boruta and other automated methods, here are dozens of other models using different combinations of the main 6ish predictors, as well as some other variables we identified as unimportant earlier. As you can see, some seem to perform more accurately and with less error on the training set than even some of our earlier models. However, they all perform worse on the test set than our first two models. You'll notice a 5-variable model with state anxiety and SDQ_L displaying pretty great numbers on the train set, as well as a few larger models with test anxiety included also performing well. However, especially given poor performance on the train set, I suspect that the model is happening to overfit certain patterns of noise with SDQ_L's odd distribution, and it would be unlikely to be a good model extrapolated to a larger training dataset (as ours only has around 200 N).

LASSO is up next after this brief detour.

```{r}
#| warning: false

# including sdq_L and age, but no AMAS.
set.seed(59)
rf_lassopretest <- train(
  sum_arith_perf ~ sex + score_TAI_short +
    score_STAI_state_short + score_PISA_ME + score_SDQ_M + score_SDQ_L + age,
  data = scaled_training, method = "rf", importance = TRUE,
  tuneGrid = tunegrid, trControl = control
)
print(rf_lassopretest)
varImpPlot(rf_lassopretest$finalModel, type = 1)

# Look at that variable importance. Clearly SDQ_L and TAI_short are not important,
# and the model is seeing patterns when they aren't there.

#No SDQ L but still including TAI short.
set.seed(58)
rf_lassopretest2 <- train(
  sum_arith_perf ~ sex + score_TAI_short +
    score_STAI_state_short + score_PISA_ME + score_SDQ_M + age,
  data = scaled_training, method = "rf", importance = TRUE,
  tuneGrid = tunegrid, trControl = control
)
print(rf_lassopretest2)
varImpPlot(rf_lassopretest2$finalModel, type = 1)

#Okay yeah, clearly test anxiety isn't the best predictor here.
#including age, STAI and AMAS but no TAI or sdq_L. 
set.seed(60)
rf_lassotest <- train(
  sum_arith_perf ~ sex +
    score_STAI_state_short + score_PISA_ME + score_SDQ_M + score_AMAS_total + age,
  data =
    scaled_training, method = "rf", importance = TRUE, tuneGrid = tunegrid,
  trControl = control
)
print(rf_lassotest)
varImpPlot(rf_lassotest$finalModel, type = 1)

#just math and State anxiety now..
set.seed(61)
rf_lassotest2 <- train(
  sum_arith_perf ~ sex + score_AMAS_total +
    score_STAI_state_short + score_PISA_ME + score_SDQ_M,
  data = scaled_training,
  method = "rf", importance = TRUE, tuneGrid = tunegrid, trControl = control
)
print(rf_lassotest2)
varImpPlot(rf_lassotest2$finalModel, type = 1)

#just state and SDQ_L.
set.seed(62)
rf_lassotest3 <- train(
  sum_arith_perf ~ sex + score_STAI_state_short +
    score_PISA_ME + score_SDQ_M + score_SDQ_L,
  data = scaled_training,
  method = "rf", importance = TRUE, tuneGrid = tunegrid, trControl = control
)
print(rf_lassotest3)
varImpPlot(rf_lassotest3$finalModel, type = 1)

# Look at that variable importance. Clearly SDQ_L and STAI_short are overfitting badly in this model. STAI_short less so - I wonder
# if STAI_short is actually above the threshold for 
# "important variables, like Boruta told us earlier.

# Just with state anxiety added.
set.seed(63)
rf_lassotest4 <- train(
  sum_arith_perf ~ sex + score_STAI_state_short +
    score_PISA_ME + score_SDQ_M,
  data = scaled_training, method = "rf",
  importance = TRUE, tuneGrid = tunegrid, trControl = control
)
print(rf_lassotest4)
varImpPlot(rf_lassotest4$finalModel, type = 1)

# Just with math anxiety added.
set.seed(64)
rf_lassotest5 <- train(
  sum_arith_perf ~ sex  +
    score_AMAS_total + score_PISA_ME + score_SDQ_M,
  data =
    scaled_training, method = "rf", importance = TRUE, tuneGrid = tunegrid,
  trControl = control
)
print(rf_lassotest5)
varImpPlot(rf_lassotest5$finalModel, type = 1)

# Both math and state anxiety
set.seed(65)
rf_lassotest6 <- train(
  sum_arith_perf ~ sex  +
    score_AMAS_total + score_PISA_ME + score_SDQ_M + score_STAI_state_short,
  data =
    scaled_training, method = "rf", importance = TRUE, tuneGrid = tunegrid,
  trControl = control
)
print(rf_lassotest6)
varImpPlot(rf_lassotest6$finalModel, type = 1)

# age and math anxiety added.
set.seed(66)
rf_lassotest7 <- train(
  sum_arith_perf ~ sex  +
    score_AMAS_total + score_PISA_ME + score_SDQ_M + age,
  data =
    scaled_training, method = "rf", importance = TRUE, tuneGrid = tunegrid,
  trControl = control
)
print(rf_lassotest7)
varImpPlot(rf_lassotest7$finalModel, type = 1)

######
#### predictions
predictionslassotest <- predict(rf_lassotest, test)
validationlassotest <- data.frame(
  R2 = R2(predictionslassotest, test$ sum_arith_perf),
  RMSE = RMSE(predictionslassotest, test$ sum_arith_perf),
  MAE = MAE(predictionslassotest, test$ sum_arith_perf)
)
print(validationlassotest)


predictionslassotest2 <- predict(rf_lassotest2, test)
validationlassotest2 <- data.frame(
  R2 = R2(predictionslassotest2, test$ sum_arith_perf),
  RMSE = RMSE(predictionslassotest2, test$ sum_arith_perf),
  MAE = MAE(predictionslassotest2, test$ sum_arith_perf)
)
print(validationlassotest2)

predictionslassotest3 <- predict(rf_lassotest3, test)
validationlassotest3 <- data.frame(
  R2 = R2(predictionslassotest3, test$ sum_arith_perf),
  RMSE = RMSE(predictionslassotest3, test$ sum_arith_perf),
  MAE = MAE(predictionslassotest3, test$ sum_arith_perf)
)
print(validationlassotest3)

predictionslassotest4 <- predict(rf_lassotest4, test)
validationlassotest4 <- data.frame(
  R2 = R2(predictionslassotest4, test$ sum_arith_perf),
  RMSE = RMSE(predictionslassotest4, test$ sum_arith_perf),
  MAE = MAE(predictionslassotest4, test$ sum_arith_perf)
)
print(validationlassotest4)

predictionslassopretest <- predict(rf_lassopretest, test)
validationlassopretest <- data.frame(
  R2 = R2(predictionslassopretest, test$ sum_arith_perf),
  RMSE = RMSE(predictionslassopretest, test$ sum_arith_perf),
  MAE = MAE(predictionslassopretest, test$ sum_arith_perf)
)
print(validationlassopretest)


predictionslassopretest2 <- predict(rf_lassopretest2, test)
validationlassopretest2 <- data.frame(
  R2 = R2(predictionslassopretest2, test$ sum_arith_perf),
  RMSE = RMSE(predictionslassopretest2, test$ sum_arith_perf),
  MAE = MAE(predictionslassopretest2, test$ sum_arith_perf)
)
print(validationlassopretest2)


#Let's continue to try models
predictionslassotest5 <- predict(rf_lassotest5, test)
validationlassotest5 <- data.frame(
  R2 = R2(predictionslassotest5, test$ sum_arith_perf),
  RMSE = RMSE(predictionslassotest5, test$ sum_arith_perf),
  MAE = MAE(predictionslassotest5, test$ sum_arith_perf)
)
print(validationlassotest5)

predictionslassotest6 <- predict(rf_lassotest6, test)
validationlassotest6 <- data.frame(
  R2 = R2(predictionslassotest6, test$ sum_arith_perf),
  RMSE = RMSE(predictionslassotest6, test$ sum_arith_perf),
  MAE = MAE(predictionslassotest6, test$ sum_arith_perf)
)
print(validationlassotest6)

predictionslassotest7 <- predict(rf_lassotest7, test)
validationlassotest7 <- data.frame(
  R2 = R2(predictionslassotest7, test$ sum_arith_perf),
  RMSE = RMSE(predictionslassotest7, test$ sum_arith_perf),
  MAE = MAE(predictionslassotest7, test$ sum_arith_perf)
)
print(validationlassotest7)
```

I think I should add more interpretation, at least of like. Maybe variable relationships and strength of those relationships IF possible.

Long lines. Gotta make long lines shorter (maybe can automatically wrap it, but u probably can do it yourself)

```{r}
library(styler)

# style_file("/Users/frankie/Desktop/Dr. Lai Feature Selection Project 9.13.24/Main Script/hello.qmd", scope = "line_breaks", strict = TRUE)
##################
# Load required packages
# library(future)
# library(rmarkdown)

# Set the plan to use all available cores
# plan(multisession)

# Define the knit function
# knit_document <- function(file) {
# rmarkdown::render(file)
# }

# Specify the Quarto file you want to knit
# quarto_file <- "hello.qmd" # Change this to your actual Quarto file

# Run the knitting in parallel using future
# result <- future({
# knit_document(quarto_file)
# })

# Collect the result (optional, depending on what you want to do with it)
# result <- value(result) # This will retrieve the result, if needed
```
