---
title: "Intermediate Models for Prediction and Variable Selection for Psychologists - Random Forest and LASSO"
author: Francisco N. Ramos
format: pdf
knitr:
  opts_chunk: 
    collapse: true
    echo: true
    results: "markup"
    message: true
    warning: true
    R.options:
      knitr.graphics.auto_pdf: true
---

```{r setup, include=FALSE}
# Set the theme globally for all plots
library(ggplot2)
theme_set(theme_minimal())
```

## Intermediate Statistical Models for Prediction and Variable Selection for Psychologists - Random Forest and LASSO

Should start w explaining less about variable selection and way more about why LASSO and random forest are each good and great.

In this document, I will walk through a random forest variable selection procedure for a psychological dataset. The dataset is extremely characteristic of many psychology datasets --- it has an okay but not at all large sample size (735 N before being split into test/training sets), a relatively high number of potential predictors (20+ collected as part of the survey, many of which are likely completely unhelpful in prediction), and what is likely middling to low effect sizes for predictors. The focus of the model will be regression and we will be primarily discussing model error and predictive power, and alternating focuses between the two. We'll review a few different ways of creating models, selecting features, and evaluating models.

However, be aware that this is not a tutorial on identifying the absolute best predictive model, which is described ad infinitum elsewhere. I will touch on using algorithmic methods to identify the "optimal" subset of predictors in a full model, but if you are merely looking for "the best model to fit my data" or "the best way to predict outcome from a set of predictors", this document will likely not be the most helpful.

Another note: we will be partially demonstrating what statisticians would call "stepwise selection", "backwards selection" to be specific. Let me just be clear - backwards selection is NEVER the best way to do variable selection. The best feature selection method will always be expert knowledge on which variables are likely to explain the majority of the variance/signal as well as which less-important variables are likely to provide valuable interactions. If possible, you should either a priori identify the subset of variables to evaluate OR if you must use data-driven selection, try to use automated methods with clear "reward" and "loss" functions so you better understand why it has penalized what it penalized and why it has selected what it selected. Backwards selection has the nasty little habit of occasionally overemphasizing variables that actually have little relationship with the DV because they happen to "fit in" nicely with the other variables, making the variable appear important in the specific models backwards selection evaluates, but the variable may be unimportant in the "true" best model for prediction (or the model containing the "true" subset of actually important variables, if you believe in that). We are going through stepwise selection because it is common but more importantly because it does help you better understand the relationships between variables in the specific model, and it is easy to run an automated method without really thinking about why those variables have been selected and what it is showing you. Examples of three various potential automated methods are provided before the end of this document, however.

I have not seen the data ahead of time. The analyses described hereafter are entirely novel.

## The Data

Importantly, we are conducting a regression task, not a classification task, as our dependent variable is continuous. The main task we'll be attempting to address is the following --- we would like to predict arithmetic performance score in German students from a set of predictor variables including age, sex, self-reported math anxiety, self-reported math self-concept, and others. Our second main task is that we would like to identify which of these variables are actually useful in predicting arithmetic performance and which ones are not, so future researchers do not have to waste resources collecting unimportant variables for minimal improvement in prediction. In other words, we would like a concise model. This data was originally gathered and shared by Cipora et al. (2024). References are at the end of the LASSO addendum to this document.

The dataset contains over 40 variables that are candidates to be predictor variables, and only contains a total of 735 participants pre-training/test split (ignoring the separate sample contained in the dataset which we will remove before analysis). To simplify this explanation, I have selected ten variables from the dataset at pseudo-random for our analysis. For the purposes of this document, only these ten variables "exist". Below, I will list the ten variables we are interested in studying as predictors of arithmetic performance score followed by the syntax for their object in R. All assessments were in German.

***Dependent Variable:***\
Arithmetic performance/sum_arith_perf., as measured by "the number of correctly solved problems in order as instructed" on a simple arithmetic speed test.

***Predictors:***\
Age/'age', as measured in years

Sex/'sex', where 1 = male, 2 = female, and 3=other. Participants who ignored this question were removed.

Neuroticism/`score_BFI_N`, as measured by the sum score of the 8 items of the Big Five Inventory (short version) pertaining to neuroticism.

Math anxiety/`score_AMAS_total`, as measured by the sum score on the Abbreviated Math Anxiety Scale.

General trait anxiety/`score_GAD`, as measured by sum score on the Generalized Anxiety Disorder Screener (GAD-7).

Math self-efficacy/`score_PISA_ME`, as measured in the PISA 2012 study using the sum score of six items.

General state anxiety/`score_STAI_state_short`, as assessed by the sum of the five-item scale STAI-SKD.

Test anxiety/`score_TAI_short`, as measured by the sum score of the 5 items on the short version of the Test Anxiety Inventory.

Math self-concept/`score_SDQ_M`, as measured by the sum score of the four math-related statements on the Self-Description Questionnaire III. Evaluates variables such as one's comfort/enjoyment/pride with math, whereas self-efficacy evaluates one's self-confidence in math abilities.

Language self-concept/`score_SDQ_L`, as measured by the sum score of the four language-related statements on the Self-Description Questionnaire III.

Arithmetic performance/`sum_arith_perf`, as measured by "the number of correctly solved problems in order as instructed" on a simple arithmetic speed test.

***Cleaning the data and a little EDA***

We'll start with importing and cleaning the data to make sure it fits our task.

```{r}
#| label: Load packages, clean data
#| warning: false
library(here)

library(tidyverse)
library(caret)
library(readxl)
library(readr)
library(ggplot2)
library(randomForest)
```

The `here()\` function is extremely convenient for creating paths and directories that won't break when files are moved or when you reproduce the work on another computer. "Here" on your computer will be whatever the top level folder of the current project folder is (which you set up when initializing a project). Looks like "here" for me is `r here()` You can also set it using the `set_here` function. You can specify subfolders using a comma, then quotation marks to define the name of each folder or file.You don't even need to worry about working directories if you use the here function.

Below, we'll again use the here function to tell the readr function where to go to get the dataset. Our data was stored in the "Main Script" folder, in a subfolder called "OSF archive", and named "AMATUS_dataset.csv". Use that info to read the code below. Notice how we use a `normalizePath` function to convert relative or incomplete path names to absolute path names. That way, we don't have to worry too much about Mac vs. Windows or cloud vs. local.

```{r}
here::i_am("Dr. Lai Feature Selection Project 9.13.24.rproj")
#If you'd like to, you can manually set your path with the command above. #I've set it to my rproj file
#Otherwise, just check with the following
#command:
here::here()

# adjust as needed.
file_path <- normalizePath(here::here("Main Script/OSF archive/AMATUS_dataset.csv"))
# adjust as needed.

amatus <- read_csv2(file_path, c("", "NA"), col_names = TRUE) 
# it's already in the "working directory" of the main script folder, 
# so we only need the subfolder OSF archive and the actual name of the dataset.

View(amatus)
# We can use this command to view our data.
table(amatus$sum_arith_perf)

amatusclean <- amatus[!is.na(amatus$sum_arith_perf), ] 
# removing the individuals who did not complete the performance test in order as instructed.
amatusclean <- amatusclean[!(amatusclean$sample %in% 
c("german_teachers", "belgian_teachers")), ] 
# removing the other two samples from the dataset

#Just some regular old data cleaning to start with.
amatusclean$sex <- as.factor(amatusclean$sex)
amatusclean$age_range <- as.factor(amatusclean$age_range)
amatusclean$breaks <- as.factor(amatusclean$breaks)
amatusclean$honesty <- as.factor(amatusclean$honesty)
amatusclean$native_speaker <- as.factor(amatusclean$native_speaker)
amatusclean$noise <- as.factor(amatusclean$noise)

amatusclean <- amatus[!is.na(amatus$sum_arith_perf), ] # removing the
# individuals who did not complete the performance test in order as instructed
amatusclean <- amatusclean[!(amatusclean$sample %in% 
c("german_teachers", "belgian_teachers")), ] 
# removing the other two samples from the dataset

View(amatusclean)
```

Now, we'll just check out a quick histogram of the dependent variable, arithmetic math performance.

```{r}
#| label: fig-arith-perf
#| fig-cap: "Distribution of outcome variable: Arithmetic Math Performance"
#| warning: false
ggplot(
  amatusclean,
  aes(x = sum_arith_perf)
) +
  geom_density(fill = "lightblue", alpha = 0.7) +
  theme_minimal() # I also set up the theme globally in hidden code at the top of this doc.
```

As you might expect from test scores, our data is clearly skewed right (@fig-arith-perf). There's no missing data for our outcome/Y variable of performance since we are not interested in participants who answered out of order against instructions. Let's also look at some predictors.

```{r}
#| label: fig-AMAS-BFI_N
#| fig-cap: "Distribution of math anxiety and neuroticism"
ggplot(
  amatusclean,
  aes(x = score_AMAS_total)
) +
  geom_density(fill = "lightblue", alpha = 0.7) 

ggplot(
  amatusclean,
  aes(x = score_BFI_N)
) +
  geom_density(fill = "lightblue", alpha = 0.7) 
```

One predictor's distribution is very normal, the other isn't. I'll save you the trouble and tell you that the rest of the features are a grab bag too. Normally we wouldn't care about standardization given that random forest is a tree-based model. However, because we are interested in variable importance metrics (since we are trying to focus on feature selection), we actually do need to scale our features. This is because variable importance metrics are skewed towards predictors that have a wider range of values. So a continuous variable may return a more biased estimate in the variable importance metric than a variable that can take 3 or 4 values. So we do want to scale our variables this time around.

We'll show code for an alternative way of scaling predictors, but the way we will choose to do it is use the `scale()` function to create a scaled training set. This will center (subtract mean from the value for a mean of 0) and scale (divide value by its standard deviation for an SD of 1) our continuous predictors. The training set will then only include the scaled variables. For our categorical variables, just remember not to worry about the scale function. We'll go ahead and get started by creating our first random forest model. Let's try using the `caret` package, a relatively general machine learning package.

A side note: There are several different validation methods you can use in caret, including bootstrapping, cross-validation, and repeated cross-validation. Repeated cross-validation should always be used when computationally appropriate. Bootstrapping may be compelling here due to the small dataset, but repeated cross-validation should work just as well as bootstrap in smaller samples and should do the job for us just fine. You can increase the `"number"` argument to increase the number of folds in the dataset and you can increase the `"repeat"` argument to increase the number of times the entire cross validation gets repeated, both at the expense of computational power.

## First Model Setup

***Setting up the train/test split***

```{r}
#| label: First model setup
library(caret)
set.seed(39) # I'm going to put this before every random generation just for
# clarity. Before data partition, please.
inTrain <- createDataPartition(amatusclean$sum_arith_perf,
  p = 0.7, list = FALSE
)
# 0.7 selected to have a decent number of N in the test set

# create training vs test data.
training <- amatusclean[inTrain, ]
test <- amatusclean[-inTrain, ]
View(training)
View(test)


##### There are two methods to include scaling of predictor variables: manual
# scaling of predictors (and predictions later on), and preprocessing, which
# means using an R function to "process" your data to put it in the model.
# I will show an example with preProcess from the caret package, however
# the `recipes` package may be even more powerful for preprocessing and less
# inconvenient, since preProcess is usually used for scaling all variables
# in a dataset at once (and therefore is harder to use with subsequent models,
# since you need to tune the application of your preprocessing.)
#####
# I'm going to pull out the dependent variable.
trainnodv <- training
preProc <- preProcess(training, method = c("center", "scale"))
trainscaled <- predict(preProc, training)
# Again, we will not be using this, but training data would be standardized in
# trainscaled.

# Let's try doing it in a more general way.
numeric_predictors <- c(
  "score_BFI_N", "score_AMAS_total", "age", "score_GAD",
  "score_PISA_ME", "score_STAI_state_short",
  "score_TAI_short", "score_SDQ_L", "score_SDQ_M"
)
categorical_variable <- "sex"
response_variable <- "sum_arith_perf"

# Create a new dataframe by scaling and centering numeric predictors.

scaled_training <- training %>%
  mutate(across(all_of(numeric_predictors), ~ scale(.) %>% as.vector())) %>%
  # Scale and convert back to vector just in case
  select(all_of(numeric_predictors), all_of(categorical_variable), all_of(response_variable)) # Select only the relevant columns

# View the scaled data
View(scaled_training)


```

***Hyperparameters and creating the Random Forest Model***

```{r}

# The argument `mtry` is a parameter of the number of randomly selected predictors to
# consider splitting at each node of the decision tree. 
# More on this after the code.
control <- trainControl(
  method = "repeatedcv", number = 10, repeats = 3,
  search = "random"
) 
# picks random `mtry` values in the range 1 to # of predictors. 
# Can pick the same value twice, so not ideal for me.

control <- trainControl(
  method = "repeatedcv", number = 10, repeats = 3,
  search = "grid"
)
# this one picks preselected values for `mtry`, which you would define
# on your own. We will be using this
tunegrid <- expand.grid(.mtry = c(2:8)) 
# This function creates a data frame from all
# combinations of the supplied vectors/factors. In other words, this
#creates the grid of values for the hyperparameter `mtry`. 
# This one says try every value including 
# and between 2 and 8. You can't try more `mtry` values than there are predictors.
# We're going to stick with this for the most part throughout.


# let's start with all values of `mtry` (1 to number of predictors, which is 10) for the very first model.
tunegrid <- expand.grid(.mtry = c(1:10)) 


############
# You can also try to use "default" values. 
x <- amatusclean[, 1:10]
mtry <- ncol(x) / 3 
# in random forest for regression, predictors/3 is
# often the "default" value of this parameter. 
# In classification, it is the square root of the # of predictors.
# Just including this here in case you want to try it.
# But make sure to fix your `tuneGrid` argument too!!!!!

# As you'll remember, the REAL parameters are below:
control <- trainControl(
  method = "repeatedcv", number = 10, repeats = 3,
  search = "grid"
)
tunegrid <- expand.grid(.mtry = c(1:10)) 

#Now, to run the random forest
set.seed(40)
# We will be setting the seed at each model.
# You can also just run this once and all random items, as long as they are generated in the same order, will be replicable.
rf_scaled <- train(
  sum_arith_perf ~ scale(score_AMAS_total) + sex + scale(age) + 
    scale(score_BFI_N) + scale(score_GAD) + scale(score_PISA_ME) +
    scale(score_STAI_state_short) + scale(score_TAI_short) +
    scale(score_SDQ_M) + scale(score_SDQ_L),
  data = training,
  importance = TRUE, method = "rf", tuneGrid=tunegrid, trControl = control
)
# Notice that we are using data = training, NOT the scaled
# training set, as well as the `scale()` function.
# Also, no `tuneLength` argument. Just `trControl`.

print(rf_scaled)
varImpPlot(rf_scaled$finalModel, type = 1)
```

As you can see in the first model (`rf_scaled`), If it was up to caret, the best predictive method would have us randomly sample 1 predictor to consider splitting at each node of the decision tree. That means we probably have very few predictors that actually provide value in predicting math performance. This is expected in this dataset and in much psychological research, as we stated early on. Although an `mtry=1` is too random, we clearly need to use a low mtry. The difference between the default (# of predictors/3) number for `mtry` and `mtry=2` isn't too big, so we can stick with either for now. We'll try to set 2 as the minimum and make sure our code test a range of mtries, just in case another value ends up being better than 2 (which I don't expect to be the case).

***Refining the First Model***

Let's exclude 1 and go again and try more options.

```{r}
#| label: Excluding value of 1

control <- trainControl(
  method = "repeatedcv", number = 10, repeats = 3,
  search = "grid"
)
# the term "grid" means it will picks preselected values for mtry, 
# which you would define on your own.
# Remeber to change the control variable back to search = "random" if
# you aren't using a grid search.

tunegrid <- expand.grid(.mtry = c(2:8)) 
# This one says try values between and including 2-8.
# You can't try more mtries than there are predictors. We're going to stick with this throughout. Since our optimal mtry at first was 1, the optimal
# mtry is probably going to be low (and will likely be 2).


# I'm going to put this before every random generation just for clarity.
set.seed(40)
rf_mtry2 <- train(
  sum_arith_perf ~ score_AMAS_total + sex + age + score_BFI_N +
    score_GAD + score_PISA_ME + score_STAI_state_short + score_TAI_short +
    score_SDQ_M + score_SDQ_L,
  data = scaled_training, importance = TRUE,
  method = "rf", tuneGrid = tunegrid, trControl = control
)
# Notice we keep using the importance=TRUE parameter, which calculates variable
# importance metrics. We need this to be set to TRUE to be able to see
# varimp plots later.

print(rf_mtry2)
varImpPlot(rf_mtry2$finalModel, type = 1)

```

R squared of around .14ish is obviously mildly low, but it's actually decent for psychological research. In published papers, you'll often happen to see r-squareds around .15-.25 or so, if that. It's very hard to build an extremely accurate model of human behavior namely because an incalculable number of factors influence our behavior in seen and unseen ways. Not to mention we care more about feature selection for than perfect prediction, which is why we are scaling variables in the first place. If we just wanted the best predictive model,

The `VarImpplot()` function assesses the variable importance for each of the variables in our model, measured by the increase in Mean square error should the variable be removed from the model. As you can see, the lowest variable is extremely irrelevant to the model, that variable being neuroticism. Neuroticism seems to be a bad predictor of arithmetic performance, but be careful --- however unlikely, it may instead be the case that all the variance neuroticism explains is better explained by other variables in the model (if correlation is high). It may also improve performance through stronger correlation with the IVs than the DV. However, it's probably a bad variable.

When you run a model, variables with a negative value for variable importance mean that the error was higher when using the original predictor variable values than when using the permuted ones. Obviously, this tends to mean the variable doesn't have a role in prediction. So, you can help identify unimportant variables in (usually low-dimension) models with variable importance. However, keep in mind that occasionally, low importance variables can become "important" in models with fewer dimensions (particularly if they correlate strongly with other independent variables or happen to explain the noise well). This can be an issue if it is overfitting random noise in the training data, but it may still generalize well to test data.

It also looks like the models we run think that math self-concept, sex, math self-efficacy and possibly math anxiety are the most important variables. Just looking at the graphs, there does indeed seem to be around 2-5 variables here that actually predict a significant amount of variance in our DV, so that's probably a great number for a parsimonious model.

Now, to fit our goal of parsimony, we want to cut out the variables that don't help us very much because we run the risk of creating a very nonparsimonious model, and of overfitting of course. From the model above, it seems that there's only one, maybe two predictors that are extremely irrelevant to the model. This is probably in part because the effect sizes of most variables are so weak that removing any one variable can significantly reduce the predictive power of the model. In other words, the best predictive model might be a 9 or 10 variable model, but the best parsimonious model (which contains only the most relevant predictors at the cost of retaining all variables with even minor predictive power) may be somewhere in the 2-5 variable range.

We'll start with a 9 variable model, which we'll create FIRST by cutting `BFI_N`.

***First Selection Model***

```{r}
#| label: First selection, cutting test anxiety
set.seed(41)
rf_firstselection <- train(
  sum_arith_perf ~ score_AMAS_total + sex + age +
    score_TAI_short + score_GAD + score_PISA_ME + score_STAI_state_short +
    score_SDQ_M + score_SDQ_L,
  data = scaled_training, method = "rf",
  importance = TRUE, tuneGrid = tunegrid, trControl = control
)
print(rf_firstselection)
# we'll look at variable importance again in a minute.
```

```{r}
varImpPlot(rf_firstselection$finalModel, type = 1)
# type 1 refers to estimates following perturbation.
```

We're seeing a similar pattern. Math self-efficacy and sex seem to be two of the most powerful variables in every model, while math self-concept and some math anxiety variables trail not too far behind (the rest seem to hang in the balance). It seems like of the ten we chose, those are some of the most important variables needed to predict math performance in German students. `RMSE` and R squared improved! But these numbers fluctuate anyways given the nature of random forest models. We'll take a look at how these models predict the test set later.

Since `AMAS` measures math anxiety, which conceptually seems to map on to math performance better than general anxiety, it makes sense that state anxiety and general anxiety are also relatively unimportant predictors. General anxiety doesn't seem to add much value to predicting math performance when math anxiety is already accounted for, and state anxiety isn't much better. Test anxiety (from earlier) being a weak predictor is not as intuitive, but it's possible that the most salient aspect here is that the test is on math, not that there's a test itself, so anxiety of tests in general doesn't really have predictive power of performance the same way math anxiety does. Regardless, they likely cover a lot of each other's correlation.

Now, we're going to try to run more random forest models with a reduced set of predictors since we are at least starting to identify the most important predictors. Let's see if we can find a more parsimonious model with still good predictive power. Language self-efficacy goes next.

***Second Selection Model***

```{r}
#| warning: false
#| label: Second selection

View(scaled_training)
set.seed(42)
rf_secondselection <- train(
  sum_arith_perf ~ score_AMAS_total + sex + age +
     score_TAI_short + score_STAI_state_short + score_PISA_ME + score_SDQ_M +
    score_GAD,
  data = scaled_training, method = "rf", importance = TRUE,
  tuneGrid = tunegrid, trControl = control
)
print(rf_secondselection)
```

```{r}
varImpPlot(rf_secondselection$finalModel, type = 1)
# type 1 refers to estimates following perturbation.
```

Slight reduction in `RMSE` and increase in (training data) R-squared! Importantly, the variable importance metrics are starting to look SLIGHTLY better --- the lowest `incMSE%` is around 3%. great news for our goal of parsimonious modeling. However, I'm not sure if this will continue. It's okay if while going through variable selection, your model doesn't necessarily have a massive reduction in `RMSE` or massive increase in r squared. Remember that we're looking to explain the most variance with as few variables as needed, so sometimes parsimony is worth slight decreases in predictive accuracy (though you should make sure you're not cutting a seemingly low-predictive power variable that has explanatory importance).

This may be a decent model. The fact that the `$incMSE` is starting to approach reasonable numbers means we're starting to run out of questionable predictors. This model is likely better than the full model, but also likely not the best model of sheer prediction. We may not need all those measures of anxiety, and like we said earlier, we likely are looking for a 2-5 variable model to cover almost all of the variance with the fewest variables possible. So we should keep going before we start prediction.

Let's cut the next least important variable - `score_GAD`.

***Third Selection Model***

```{r}
#| warning: false
#| label: Third selection
set.seed(43) #
rf_thirdselection <- train(
  sum_arith_perf ~ score_AMAS_total + sex + age +
    score_STAI_state_short + score_PISA_ME + score_TAI_short + score_SDQ_M,
  data = scaled_training, method = "rf", importance = TRUE,
  tuneGrid = tunegrid, trControl = control
)
print(rf_thirdselection)

# we'll look at variable importance again later.
```

Slight reduction in R squared seems odd, but it's definitely possible that the model got worse after removing GAD. It's also possible that this model fits the train data well but will not generalize well to the test data. We'll see.

```{r}
varImpPlot(rf_thirdselection$finalModel, type = 1)
# type 1 refers to estimates following perturbation.
```

The `RMSE` did go up. This doesn't seem better than the previous models at the moment. Let's keep going. By the way, at this point the highest mtry being tried (7) is greater than number of predictors (6). It still works without it but it produces a lot of annoying warnings, so we'll start to include a cap line at the beginning of the chunk of code for your convenience (before the set seed, importantly). We're cutting test anxiety next.

***Fourth Selection Model***

```{r}
#| label: Fourth selection
tunegrid <- expand.grid(.mtry = c(2:6))
set.seed(44)
rf_fourthselection <- train(
  sum_arith_perf ~ score_AMAS_total + sex + age +
    score_STAI_state_short + score_PISA_ME + score_SDQ_M,
  data = scaled_training,
  method = "rf", importance = TRUE, tuneGrid = tunegrid, trControl = control
)
print(rf_fourthselection)
# we'll look at variable importance again later.
```

```{r}
varImpPlot(rf_fourthselection$finalModel, type = 1)
# type 1 refers to estimates following perturbation.
```

Hmm, looks like the anxiety variables aren't really helping us all that much. However, it is a bit odd that these anxiety variables tend to flip flop with their importance in these graphs. I've also noticed that age, which was once near the bottom of the "importance" graph, continues to move upwards. At this point, I start to wonder if the math anxiety variables may add predictive power, despite seemingly being not the variables with the best zero-order correlations with the DV. Not to mention that the `%IncMSE` is relatively high, even enough that this sort of model starts to look pretty appealing. However, let's keep going, as this doesn't seem to be a great model either.

I suspect the MOST parsimonious model may either have only one anxiety variable (likely math anxiety) or no anxiety variables. Let's cut state anxiety, then.

***Fifth Selection Model***

```{r}
#| label: Fifth selection
tunegrid <- expand.grid(.mtry = c(2:5))
set.seed(45)
rf_fifthselection <- train(
  sum_arith_perf ~ sex + age + score_AMAS_total +
    score_PISA_ME + score_SDQ_M,
  data = scaled_training, method = "rf",
  importance = TRUE, tuneGrid = tunegrid, trControl = control
)
print(rf_fifthselection)

```

```{r}
varImpPlot(rf_fifthselection$finalModel, type = 1)
# type 1 refers to estimates following perturbation.
```

The models continue to worsen. It seems like some of these variables that we've removed for seeming middling may contribute to predictive accuracy, but we will check this with test data. However, it does seem that we have identified the key variables as being math self-efficacy and self-concept as well as sex, possibly with math (or state) anxiety and age. Also, age used to be one of the predictors with the lowest `incMSE%` in the full model, but has stuck in the models we are testing. That indicates to me that age may have value in some models, but it seems to generally have little relationship with the dependent variable. It likely should only be used in the model if it happens to be in the best predictive model or if you are convinced of its key relationship with another variable. If this were an actual research project, I would have conducted more exploratory data analysis to check 'age''s correlation with other variables as well as evaluated the part correlation in the full model to see if variables such as age were acting as suppressors of key variables.

***Sixth Selection Model***

```{r}
#| label: Sixth selection
# just for fun, we'll use the in-formula scale() method to scale data.
tunegrid <- expand.grid(.mtry = c(2:4))
set.seed(46)
rf_sixthselection <- train(
  sum_arith_perf ~ sex + score_PISA_ME + score_SDQ_M +
    score_AMAS_total,
  data = scaled_training, method = "rf", importance = TRUE,
  tuneGrid = tunegrid, trControl = control
)
print(rf_sixthselection)

```

```{r}
varImpPlot(rf_sixthselection$finalModel, type = 1)
# type 1 refers to estimates following perturbation.
```

The `RMSE` went up again, and by a decent amount. At this point, it seems safe to say that models are not getting any better --- in fact, they're getting worse. The four variables above do seem to be the most "important" variables in terms of identifying predictors that genuinely correlate with the dependent variable, and math self-efficacy and sex are likely the top two. Of course, this has nothing to do with accuracy of prediction. The 4 or 5 variable models seem to be pretty good parsimonious models if we want to collect more than only 2 variables.

In other words, we've identified 5 variables that can produce a good model that captures a good amount of the variability in scores. This model may be the best combination of parsimony and predictive power (in fact, it seems like it may be flat out the best predictive model), but this model is likely not the most parsimonious (that seems to be a model with only a couple predictors, as we've indicated before). There is also a chance it is not the most predictive (which we'll test in the Random Forest Test Predictions addendum).

In my addendum to this blog about automated feature selection methods, we 
will devise a "parsimonious" two-predictor model. In my second addendum to this
blog, entitled "Random Forest Test Prediction", we will see that its 
predictive accuracy rivals that of some bigger models we created above.

Let's look at some automated variable selection procedures in the next addendum, then we'll test all our predictions in a third addendum.

