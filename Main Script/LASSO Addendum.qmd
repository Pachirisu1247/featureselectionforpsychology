---
title: "LASSO Addendum "
author: Francisco N. Ramos
format: pdf
knitr:
  opts_chunk: 
    collapse: true
    echo: true
    results: "markup"
    message: true
    warning: true
    R.options:
      knitr.graphics.auto_pdf: true
---

## Addendum: Using LASSO as Feature Selection for Psychologists in R

As an addendum, I will walk through the use of LASSO for feature selection on a psychological dataset in R. LASSO is a regression method that performs variable selection and regularization. LASSO is essentially an extension of ordinary least squares where a penalty term is added to the loss function, resulting in "weightings" of coefficients. The penalty term in LASSO (L1 regularization) uses absolute value, whereas the penalty term in the very-similar ridge regression uses squared values (L2 regularization, which we will discuss in the adaptive LASSO section below). LASSO can reduce a predictor's weight to 0, removing it from the model, which makes it more suitable for feature selection than ridge regression. The latter is best for handling highly collinear predictors, which we will also check since LASSO can handle multicollinearity, but only to an extent.

LASSO uses a least-squares "shrinkage operator", so we should make sure we can fit a linear model with our AMATUS data first. We'll check assumptions and conduct exploratory data analysis. It's more okay if the data seems to be less fitting of a linear model after LASSO calculation (more on this later), but we definitely want to make sure we could fit a linear model with our 10 variables as is.

As part of the AMATUS dataset, the original researchers already calculated a correlation matrix for the variables we're interested in. To save you time, I'll tell you right now that the variables that are correlated with each other to a potentially problematic extent are math self-efficacy and self-concept positively (duh) and math self-concept as well as math self-efficacy with math anxiety, negatively. This shouldn't cause too many issues because LASSO and ridge regression can handle multicollinearity, but it's likely that in some models, one of these variables will be picked and the other will be ignored since they cover much of the same ground. So don't be surprised if one LASSO model picks for example SDQ_M and another picks PISA_ME, as they explain much of the same variance.

Real quick, let's load the data again. Same dataset as the random forest document, so use that code as reference.

```{r}
#load packages
library(here)
library(tidyverse)
library(caret)
library(readxl)
library(readr)
library(ggplot2)
library(randomForest)
library(stats)

here::i_am("Dr. Lai Feature Selection Project 9.13.24.rproj")
#If you'd like to, you can manually set your path with the command above. #I've set it to my rproj file
#Otherwise, just check with the following
#command:
here::here()

# adjust as needed.
file_path <- normalizePath(here::here("Main Script/OSF archive/AMATUS_dataset.csv"))
# adjust as needed.

amatus <- read_csv2(file_path, c("", "NA"), col_names = TRUE)
# it's already in the "working directory" of the main script folder,
# so we only need the subfolder OSF archive and the actual name of the dataset.

View(amatus)
# We can use this command to view our data.
table(amatus$sum_arith_perf)

#### Just some regular old data cleaning to start with.
amatus$sex <- as.factor(amatus$sex)
amatus$age_range <- as.factor(amatus$age_range)
amatus$breaks <- as.factor(amatus$breaks)
amatus$honesty <- as.factor(amatus$honesty)
amatus$native_speaker <- as.factor(amatus$native_speaker)
amatus$noise <- as.factor(amatus$noise)

amatusclean <- amatus[!is.na(amatus$sum_arith_perf), ] # removing the
# individuals who did not complete the performance test in order as instructed
amatusclean <- amatusclean[!(amatusclean$sample %in%
  c("german_teachers", "belgian_teachers")), ]

# removing the other two samples from the dataset
View(amatusclean)

```

```{r}
library(ggplot2)
library(olsrr) # nice tools for linear regression
library(caret)
library(MASS)
library(dplyr)
library(car)
library(glmnet)

AMASplot <- ggplot(data = amatusclean, aes(x = score_AMAS_total, y = sum_arith_perf))
AMASplot <- AMASplot + geom_point(fill = "lightskyblue1", color = "black")
AMASplot <- AMASplot + geom_count(show.legend = TRUE)
AMASplot <- AMASplot + scale_y_continuous(breaks = seq(0, 50, 5), limits = c(0, 50))
AMASplot <- AMASplot + geom_smooth(method = "lm", se = FALSE) +
  geom_smooth(method = "loess", se = FALSE)
AMASplot

# modestly linear, right tail deviates.

```

```{r}
lassoamatus <- amatusclean
###
ageplot <- ggplot(data = lassoamatus, aes(x = age, y = sum_arith_perf))
ageplot <- ageplot + geom_point(fill = "lightskyblue1", color = "black")
ageplot <- ageplot + geom_count(show.legend = TRUE)
ageplot <- ageplot + scale_y_continuous(breaks = seq(0, 50, 5), limits = c(0, 50))
ageplot <- ageplot + geom_smooth(method = "lm", se = FALSE)
ageplot <- ageplot + geom_smooth(method = "loess", se = FALSE)
ageplot
# notreallylinear. A log transformation makes it look a little better.
```

```{r}
bfiplot <- ggplot(data = lassoamatus, aes(x = score_BFI_N, y = sum_arith_perf))
bfiplot <- bfiplot + geom_point(fill = "lightskyblue1", color = "black")
bfiplot <- bfiplot + geom_count(show.legend = TRUE)
bfiplot <- bfiplot + scale_y_continuous(breaks = seq(0, 50, 5), limits = c(0, 50))
bfiplot <- bfiplot + geom_smooth(method = "lm", se = FALSE) +
  geom_smooth(method = "loess", se = FALSE)
bfiplot
# Pretty damn linear.
```

```{r}
gadplot <- ggplot(data = lassoamatus, aes(x = score_GAD, y = sum_arith_perf))
gadplot <- gadplot + geom_point(fill = "lightskyblue1", color = "black")
gadplot <- gadplot + geom_count(show.legend = TRUE)
gadplot <- gadplot + scale_y_continuous(breaks = seq(0, 50, 5), limits = c(0, 50))
gadplot <- gadplot + geom_smooth(method = "lm", se = FALSE) +
  geom_smooth(method = "loess", se = FALSE)
gadplot
# modestly linear?
```

```{r}
PISAplot <- ggplot(data = lassoamatus, aes(x = score_PISA_ME, y = sum_arith_perf))
PISAplot <- PISAplot + geom_point(fill = "lightskyblue1", color = "black")
PISAplot <- PISAplot + geom_count(show.legend = TRUE)
PISAplot <- PISAplot + scale_y_continuous(breaks = seq(0, 50, 5), limits = c(0, 50))
PISAplot <- PISAplot + geom_smooth(method = "lm", se = FALSE) +
  geom_smooth(method = "loess", se = FALSE)
PISAplot
# modestly linear.
```

```{r}
STAIplot <- ggplot(data = lassoamatus, aes(x = score_STAI_state_short, y = sum_arith_perf))
STAIplot <- STAIplot + geom_point(fill = "lightskyblue1", color = "black")
STAIplot <- STAIplot + geom_count(show.legend = TRUE)
STAIplot <- STAIplot + scale_y_continuous(breaks = seq(0, 50, 5), limits = c(0, 50))
STAIplot <- STAIplot + geom_smooth(method = "lm", se = FALSE) +
  geom_smooth(method = "loess", se = FALSE)
STAIplot
# modestly linear, big deviation at right tail.
```

```{r}
TAIplot <- ggplot(data = lassoamatus, aes(x = score_TAI_short, y = sum_arith_perf))
TAIplot <- TAIplot + geom_point(fill = "lightskyblue1", color = "black")
TAIplot <- TAIplot + geom_count(show.legend = TRUE)
TAIplot <- TAIplot + scale_y_continuous(breaks = seq(0, 50, 5), limits = c(0, 50))
TAIplot <- TAIplot + geom_smooth(method = "lm", se = FALSE) +
  geom_smooth(method = "loess", se = FALSE)
TAIplot
# Pretty linear, tiny deviation at right tail.
```

```{r}
SDQ_Mplot <- ggplot(data = lassoamatus, aes(x = score_SDQ_M, y = sum_arith_perf))
SDQ_Mplot <- SDQ_Mplot + geom_point(fill = "lightskyblue1", color = "black")
SDQ_Mplot <- SDQ_Mplot + geom_count(show.legend = TRUE)
SDQ_Mplot <- SDQ_Mplot + scale_y_continuous(breaks = seq(0, 50, 5), limits = c(0, 50))
SDQ_Mplot <- SDQ_Mplot + geom_smooth(method = "lm", se = FALSE) +
  geom_smooth(method = "loess", se = FALSE)
SDQ_Mplot
## Vert linear, small deviation at left tail.

```

```{r}
SDQ_Lplot <- ggplot(data = lassoamatus, aes(x = score_SDQ_L, y = sum_arith_perf))
SDQ_Lplot <- SDQ_Lplot + geom_point(fill = "lightskyblue1", color = "black")
SDQ_Lplot <- SDQ_Lplot + geom_count(show.legend = TRUE)
SDQ_Lplot <- SDQ_Lplot + scale_y_continuous(breaks = seq(0, 50, 5), limits = c(0, 50))
SDQ_Lplot <- SDQ_Lplot + geom_smooth(method = "lm", se = FALSE) +
  geom_smooth(method = "loess", se = FALSE)
SDQ_Lplot
# doesn't seem really linear, to be honest.
```

Honestly, most of these seem pretty linear, albeit with slight deviations at the ends for some. SDQ_L, however, has pretty clear nonlinearity going on. In a real analysis, we would want to use some nonlinear modeling techniques, something like spline regression or piecewise linear regression. But that is outside the scope of the paper, so we'll just keep SDQ_L as is for now.

#can maybe try splines. B-spline would have a penalty already. Is it used with LASSO or as a substitute for LASSO?

## Running the Model

Normally, you should naturally transform your variables to the desired distributions then run the model. In order to show you why you should do that, I will first run the model with no transformations.

```{r}
set.seed(150) # I'm going to put this before every random generation just for clarity.
inTrain <- createDataPartition(amatusclean$sum_arith_perf, p = 0.7, list = FALSE) # 0.7 selected to have a decent number of N in the test set

# create training vs test data.
training2 <- amatusclean[inTrain, ]
test2 <- amatusclean[-inTrain, ]
View(training2)
View(test2)


# Extracting the dataframe to keep things simple.
# Define the variable names
numeric_predictors <- as.character(c(
  "score_BFI_N", "score_AMAS_total", "age", "score_GAD",
  "score_PISA_ME", "score_STAI_state_short",
  "score_TAI_short", "score_SDQ_L", "score_SDQ_M"
))
categorical_variable <- as.character("sex")
response_variable <- as.character("sum_arith_perf")

# Create a new dataframe by scaling and centering numeric predictors
scaled_training2 <- training2 %>%
  # Select only specified columns using all_of
  dplyr::select(all_of(c(numeric_predictors, categorical_variable, response_variable))) %>%
  # Identify numeric columns
  dplyr::mutate(across( # notice we put dplyr:: in front so it doesn't think it is using MASS's version.
    # Specify numeric columns for scaling
    .cols = all_of(numeric_predictors), # Use all_of to ensure these are the numeric predictors
    .fns = ~ scale(.) %>% as.vector() # Scale and convert to vector
  ))


# View the scaled data
View(scaled_training2)


lambdamodel <- lm(sum_arith_perf ~ score_AMAS_total + sex + age + score_BFI_N + score_GAD + score_PISA_ME + score_STAI_state_short + score_TAI_short + score_SDQ_M + score_SDQ_L, data = scaled_training2) # we're calling it lambda for reasons to be made abundantly clear shortly
summary(lambdamodel)
## see residual/fitted plots, qq plots, and much more
ols_plot_diagnostics(lambdamodel)
## see residuals by variable, mostly for linearity.
car::residualPlots(lambdamodel,
  pch = 20, col = "gray",
  fitted = F,
  ask = F, layout = c(3, 2),
  tests = F
)
## see residuals by variable, mostly for heteroskedasticity, notice the fitted argument
car::residualPlots(lambdamodel,
  pch = 20, col = "gray",
  ask = F, layout = c(3, 2),
  fitted = T,
  tests = F
)

ggplot(lassoamatus, aes(x = sum_arith_perf)) +
  geom_histogram(binwidth = 0.5, fill = "blue", color = "black") +
  labs(title = "Histogram of Values", x = "Value", y = "Frequency")
```

Like we mentioned in the first document, the dependent variable was skewed right. So is our distribution of residuals, unsurprisingly. We're gonna need to try a transformation of the DV. Let's try a Box Cox transformation to center the varia- wait, we have two observations that are 0 and Box Cox doesn't work with those! We probably wouldn't lose variability if we just cut the two 0s. But that's lazy. Let's try an extremely similar transformation, the Yeo-Johnson transformation, that can indeed handle values of 0.

```{r}
library(bestNormalize)
library(car)
# optimal lambda for Yeo-Johnson transformation? Let's find it.
a <- boxCox(lambdamodel, family = "yjPower", plotit = TRUE)
optimal_lambda2 <- a$x[which.max(a$y)]
optimal_lambda2
# .58585856

# Let's visualize it too.
crPlots(lambdamodel,
  pch = 20, col = "gray",
  smooth = list(smoother = car::gamLine)
)


# Apply the Yeo-Johnson transformation manually
yj_transform <- yeojohnson(scaled_training2$sum_arith_perf, lambda = optimal_lambda)

scaled_training2$sum_arith_yj <- yj_transform$x.t
# View the transformed data
View(scaled_training2)

ggplot(scaled_training2, aes(x = sum_arith_yj)) +
  geom_histogram(binwidth = 0.3, fill = "blue", color = "black") +
  labs(title = "Histogram of Values", x = "Value", y = "Frequency")



```

```{r}
#| echo: false
transformedmodel <- lm(sum_arith_yj ~ score_AMAS_total + sex + age + score_BFI_N + score_GAD + score_PISA_ME + score_STAI_state_short + score_TAI_short + score_SDQ_M + score_SDQ_L, data = scaled_training2)
summary(transformedmodel)
ols_plot_diagnostics(transformedmodel)
vif(transformedmodel)
## see residuals by variable, mostly for linearity
car::residualPlots(transformedmodel,
  pch = 20, col = "gray",
  fitted = F,
  ask = F, layout = c(3, 2),
  tests = F
)
## see residuals by variable, mostly for heteroskedasticity, notice the fitted argument
car::residualPlots(transformedmodel,
  pch = 20, col = "gray",
  ask = F, layout = c(3, 2), fitted = T,
  tests = F
)
```

This model looks much much better. Much better resid/fitted plot, much better QQ plot (though with some issues we'll mention in a second), and of course a much more reasonable DV plot.VIF numbers seem reasonable, but we'll keep an eye on it. However, notice the residual histogram has a small issue - there is a tail of low residual values (which can be spotted near the bottom of the residual/fitted graph.) This indicates the model is systematically underfitting certain values around 0. The QQ plot shows that our distribution is pretty normal, but there are a couple causes for concern, namely the over/under estimation at theoretical quantile values around 2 and -2, respectively. They are not super concerning, but I suspect it indicates that the distribution of the SDQ_L variable is not ideal, as well as the existence of mild nonlinearity in some other predictors.

Just for fun, we'll also try a model just cutting the outliers and also run a model with no SDQ_L.

```{r}
# no outlier model.
scaled_training3 <- scaled_training2
scaled_training3 <- scaled_training2[-c(502, 10, 477), ]
transformedmodel2 <- lm(sum_arith_yj ~ score_AMAS_total + sex + age + score_BFI_N + score_GAD + score_PISA_ME + score_STAI_state_short + score_TAI_short + score_SDQ_M + score_SDQ_L, data = scaled_training3)
ols_plot_diagnostics(transformedmodel2)

##### No SDQ_L model.
transformedmodel3 <- lm(sum_arith_yj ~ score_AMAS_total + sex + age + score_BFI_N + score_GAD + score_PISA_ME + score_STAI_state_short + score_TAI_short + score_SDQ_M, data = scaled_training2)
ols_plot_diagnostics(transformedmodel3)



a <- boxCox(lambdamodel, family = "yjPower", plotit = TRUE)
optimal_lambda2 <- a$x[which.max(a$y)]
optimal_lambda2
```

Removing the outliers cleaned up the numbers a little but didn't address the issue of mild nonlinearity at certain points for the regression model. Cutting SDQ_L did the same thing, made the QQ plot a little nicer but not so much that we don't notice that weird overfitting at the higher end of the theoretical quantiles. It's not super concerning, but these are the sorts of things you should be thinking about before and while running an analysis like this.

Okay, LASSO. Let's just run it and see what we get.

```{r}

R.version.string
if (!requireNamespace("BiocManager", quietly = TRUE)) {
  install.packages("BiocManager")
}
library(glmnet)
## we need to turn the dataframe into a matrix
x2 <- model.matrix(~ score_AMAS_total + sex + age + score_BFI_N + score_GAD + score_PISA_ME + score_STAI_state_short + score_TAI_short + score_SDQ_M + score_SDQ_L - 1, data = scaled_training2)

x2 <- x2[, -3]
y2 <- scaled_training2$sum_arith_yj

set.seed(151)
cv_model <- cv.glmnet(x2, y2, alpha = 1)

plot(cv_model)

# The upper and lower bars are the MSE +- standard error.
set.seed(151)
best_lambda <- cv_model$lambda.min
print(best_lambda)
# after we set the lambda, we run the model!
set.seed(152)
lassomodel <- glmnet(x2, y2, alpha = 1, lambda = best_lambda)

lassomodel
lassomodel$dev.ratio
lassomodel # is the fraction of (null) deviance explained. Pretty good GOF estimator but we'll find another soon.

coef(lassomodel)


# SECOND TRY - NOW WITH LAMBDA that minimizes CV error plus one standard error.
set.seed(151)
best_lambda2 <- cv_model$lambda.1se
print(best_lambda2)
# after we set the lambda, we run the model!
set.seed(152)
lassomodel2 <- glmnet(x2, y2, alpha = 1, lambda = best_lambda2)

lassomodel2
print(coef(lassomodel2))

lassomodel2$dev.ratio # is the fraction of (null) deviance explained. Pretty good GOF estimator but we'll find another soon.
# This may be a better model, tbh. Let's test later.


## GOOD IDEA: CHECK FOR DIAGNOSTIC PLOTS (AND STATS) OF YOUR TWO LASSO MODELS. SEE IF THERE ARE MANUAL PLOT COMMANDS FOR LASSO (PARTICULARLY RESIDS), COULD BE SOMETHING YOU WRITE OR WRITE UP WELL.
## CHECK FOR OTHER THINGS LIKE ROBUST LASSO/ADAPTIVE LASSO ETC. SEE WHAT WOULD FIT YOUR DATA BEST. PROBABLY SOMETHING THAT GIVES ROBUST ERRORS DUE TO YOUR MILD HETEROSKEDASTICITY.
# THE ROBUSTIFICATION OF THE LASSO AND ELASTIC NET DISSERTATION BY (?MATT MULTACH)

```

Keep in mind that the cross-validation we used is attempting to estimate the value of lambda that leads to the minimal prediction error. In other words, the lambda is being selected to predict the best, NOT to select the "best" model. The lambda that is chosen using cross validation is chosen using MSE, which may lead to a "bigger" model than if we were trying to optimize the lambda to "produce the correct model"/"select the relevant predictors". This is a huge caveat, and something you need to consider when you are thinking about WHY you are using LASSO. You can read more at the following link:

(more here: https://stats.stackexchange.com/questions/353185/why-using-cross-validation-is-not-a-good-option-for-lasso-regression)

###might be better to just see what variables are being used in the lowest prediction error model? or which variables tend to have the lowest prediction error? Can always try this other perspective, or at LEAST discuss it further than what I currently have. Dr. Lai doesn't really believe in "true" models, for example.

```{r}
# Let's check out the residuals.
# Cross validation
set.seed(1)
lasso.cv <- cv.glmnet(x2, y2)

# Prediction
yhat <- predict(lasso.cv, newx = x2)

# residuals
res <- y2- yhat

plot(res ~ yhat)
```

As you can see, there is a slight pattern in the residuals, namely that lower predicted values tend to have lower residuals and higher predicted values tend to have higher residuals. However, LASSO does not call for any particular distribution of its residuals. Recall what LASSO does — it is not a model, with a linear model being denoted by y = XB + e. It is a coefficient estimation method/variable selection method that uses a shrinkage term to anticipate the decrease in model fit when used on test data, which introduces some bias but reduces variance. Because the LASSO function only takes y (the dependent variable) and X (the predictors) while producing B (the coefficients, there are no assumptions regarding the distribution of e, errors. Thus, this residual pattern is not concerning. In fact, it may even be expected due to the fact that LASSO will shift predicted values closer to the mean, meaning (get it?) residuals may display a pattern like the one we see above.

Let's keep looking at both models with the differing values of lambda. I think it's probably fair to say that the model with the lambda.min value will be better at prediction, but may not contain variables outside of the "true" subset of variables, whereas the lambda.1se value produces a more sparse model that is more likely to contain the true set of variables.

Just for fun, before we do prediction, let's try including the selected variables of both values of lambda in a random forest model and see the models' stats.

```{r}
library(caret)
#### TESTING ACCURACY OF MODEL ON TEST SET!
# First, we'll make sure we have a spare test set in case anything goes horribly wrong.

View(test2)
View(training2)
test2.0 <- test2
# We'll also save the actuals so we have them.
actuals <- test2$sum_arith_perf
View(actuals)
# let's do the test set
# Extracting the dataframe to keep things simple.
numeric_predictors2 <- c(
  "score_BFI_N", "score_AMAS_total", "age", "score_GAD",
  "score_PISA_ME", "score_STAI_state_short",
  "score_TAI_short", "score_SDQ_L", "score_SDQ_M"
)
categorical_variable2 <- "sex"
response_variable2 <- "sum_arith_perf"

# Create a new dataframe
library(dplyr)
scaled_test2 <- test2 %>%
  dplyr::select(
    all_of(numeric_predictors2),
    # specifying dplyr again.
    all_of(categorical_variable2), all_of(response_variable2)
  )



# View the raw test data before scaling
View(scaled_test2)



# first, find scaling parameters.

scale(training2$score_BFI_N) # mean 23.41085, 6.200986
scale(training2$score_AMAS_total) # 18.08333, 6.487242
scale(training2$age) # 23.37016, 4.241794
scale(training2$score_GAD) # 12.48256, 3.958239
scale(training2$score_PISA_ME) # 20.58915, 3.328284
scale(training2$score_STAI_state_short) # 8.372093, 2.966662
scale(training2$score_TAI_short) # 11.01163, 3.716646
scale(training2$score_SDQ_L) # 13.7655, 2.350222
scale(training2$score_SDQ_M) # 11.69186, 3.078269
# We then apply the exact same scaling to the test set. Since the test set is unseen data, we apply the TRAINING scaling parameters to the TEST set. We also do this so our predictions are not nonsensical.

# Define the desired means and SDs for each predictor
scaling_params <- list(
  score_BFI_N = list(mean = 23.41085, sd = 6.200986),
  score_AMAS_total = list(mean = 18.08333, sd = 6.487242),
  age = list(mean = 23.37016, sd = 4.241794),
  score_GAD = list(mean = 12.48256, sd = 3.958239),
  score_PISA_ME = list(mean = 20.58915, sd = 3.328284),
  score_STAI_state_short = list(mean = 8.372093, sd = 2.966662),
  score_TAI_short = list(mean = 11.01163, sd = 3.716646),
  score_SDQ_L = list(mean = 13.7655, sd = 2.350222),
  score_SDQ_M = list(mean = 11.69186, sd = 3.078269)
)

# Apply scaling to 'age' and 'score_gad' using 'predictor' as the loop variable
# Apply scaling to each variable in the test dataset
for (predictor in names(scaling_params)) {
  if (predictor %in% colnames(scaled_test2)) { # Check if the predictor exists in the dataset
    new_mean <- scaling_params[[predictor]]$mean
    new_sd <- scaling_params[[predictor]]$sd

    # C
    # Overwrite the original predictor with the scaled version
    scaled_test2[[predictor]] <- (scaled_test2[[predictor]] - new_mean) / new_sd
  } else {
    warning(paste("Variable", predictor, "not found in test dataset."))
  }
}

View(scaled_test2)
View(scaled_training2) # compare with train data, make sure all looks good.
library(caret)

###TEST LASSO PREDICTION WITH FULL MODEL!
predictors <- c(
  "score_BFI_N", "score_AMAS_total", "age", "score_GAD",
  "score_PISA_ME", "score_STAI_state_short",
  "score_TAI_short", "score_SDQ_L", "score_SDQ_M", "sum_arith_perf")
scaled_matrix2 <- model.matrix(~ . + 0, data = scaled_training2[, predictors, drop = FALSE])
head(scaled_matrix2)
predictionslasso <- predict(lassomodel, scaled_matrix2)
validationlasso <- data.frame(
  R2 = R2(predictionslasso, scaled_matrix2[, "sum_arith_perf"]),
  RMSE = RMSE(predictionslasso, scaled_matrix2[, "sum_arith_perf"]),
  MAE = MAE(predictionslasso, scaled_matrix2[, "sum_arith_perf"])
) # these statistical functions are in caret technically under trainControl(), which passes this argument through defaultSummary() I believe.
print(validationlasso)

###First LASSO model:
predictors <- c(
  "score_BFI_N", "score_AMAS_total", "age", "score_GAD",
  "score_PISA_ME", "score_STAI_state_short",
  "score_TAI_short", "score_SDQ_L", "score_SDQ_M", "sum_arith_perf")
scaled_matrix3 <- model.matrix(~ . + 0, data = scaled_training2[, predictors, drop = FALSE])
head(scaled_matrix2)
predictionslasso2 <- predict(lassomodel2, scaled_matrix3)
validationlasso2 <- data.frame(
  R2 = R2(predictionslasso2, scaled_matrix3[, "sum_arith_perf"]),
  RMSE = RMSE(predictionslasso2, scaled_matrix3[, "sum_arith_perf"]),
  MAE = MAE(predictionslasso2, scaled_matrix3[, "sum_arith_perf"])
) # these statistical functions are in caret technically under trainControl(), which passes this argument through defaultSummary() I believe.
print(validationlasso2)





control <- trainControl(method = "repeatedcv", number = 10, repeats = 3, search = "grid")
tunegrid <- expand.grid(.mtry = c(2:8))
set.seed(100)
rf_lasso <- train(sum_arith_perf ~ sex + score_TAI_short + score_STAI_state_short + score_PISA_ME + score_SDQ_M + score_SDQ_L, data = scaled_training2, method = "rf", importance = TRUE, tuneLength = 10, trControl = control)

print(rf_lasso)

predictionslassomodel <- predict(rf_lasso, scaled_test2)
validationlassomodel <- data.frame(
  R2 = R2(predictionslassomodel, scaled_test2$ sum_arith_perf),
  RMSE = RMSE(predictionslassomodel, scaled_test2$ sum_arith_perf),
  MAE = MAE(predictionslassomodel, scaled_test2$ sum_arith_perf)
) # these statistical functions are in caret technically under trainControl(), which passes this argument through defaultSummary() I believe.
print(validationlassomodel)
# pretty weak prediction it would seem. I'm sure the model may be a bit undertuned.
## put it back in the linear model for fun
linearlassomodel2 <- lm(sum_arith_yj ~ +sex + score_PISA_ME + score_STAI_state_short + score_SDQ_M, data = scaled_training2)
summary(linearlassomodel2)
ols_plot_diagnostics(linearlassomodel2)

# 1se version
set.seed(101)
rf_lasso_1se <- train(sum_arith_perf ~ sex + score_PISA_ME + score_SDQ_M, data = scaled_training2, method = "rf", importance = TRUE, tuneLength = 10, trControl = control)
print(rf_lasso_1se)


predictionslasso1semodel <- predict(rf_lasso_1se, scaled_test2)
validationlasso1semodel <- data.frame(
  R2 = R2(predictionslasso1semodel, scaled_test2$ sum_arith_perf),
  RMSE = RMSE(predictionslasso1semodel, scaled_test2$ sum_arith_perf),
  MAE = MAE(predictionslasso1semodel, scaled_test2$ sum_arith_perf)
) # these statistical functions are in caret technically under trainControl(), which passes this argument through defaultSummary() I believe.
print(validationlasso1semodel)

```

Wow, looks like our LASSO model fitted with the lambda.min actually had pretty great prediction, almost in line with our comprehensive model. The lambda.1se model had pretty middling prediction, but that model is really just aimed at identifying the key variables, which it seemed to do alright at (despite obviously missing PISA_ME, likely because PISA_ME and SDQ_M cover a lot of the same ground). In case you didn't notice, the lambda.min model was our Boruta model, meaning the top 3 predictors alongside math and state anxiety, with the inclusion of SDQ_L. The inclusion of SDQ_L is curious, particularly given how tiny its coefficient is compared to the similarly scaled SDQ_M. One of two things is likely going on. It is possible that SDQ_L is a suppressor variable, meaning that SDQ_L better explains residuals than it does the DV. This means the other predictors lose error in their predictions, and though it doesn't always happen, the r squared may go up. The other idea is that SDQ_L seems to explain patterns in the noise left after all the other variables are taken into account, and as such, it seems to improve model prediction when it is really just predicting noise. I sort of lean towards the latter explanation because SDQ_L seems to be generally a very poor predictor and not selected in the automated methods except RFE, which did not select any variables.

(A quick example of assessing the correlations and part correlations to identify suppressor variables.

```{r}
library(glmnet)
library(ggm)


# Extract coefficients at best lambda
coefs <- coef(lassomodel, s = best_lambda)
# Convert to a vector and get the non-zero coefficients (ignoring intercept)
coefs_vector <- as.vector(coefs)
non_zero_indices <- which(coefs_vector != 0)  # Get indices of non-zero coefficients
non_zero_indices <- non_zero_indices[-1]
# Ensure non_zero_indices are valid for subsetting X
# If X is a matrix and does not have column names, use column numbers; otherwise, use column names.
# If using a data frame, this step ensures correct subsetting.
if (is.data.frame(x2)) {
  selected_columns <- colnames(x2)[non_zero_indices]
  X_selected <- x2[, selected_columns, drop = FALSE]
} else {
  # If X is a matrix, we use column indices directly
  X_selected <- x2[, non_zero_indices, drop = FALSE]
}

# Compute the covariance matrix of the predictors
cov_matrix <- cov(X_selected)
cor_matrix <- cor(X_selected)

# Display the correlation matrix
print(cor_matrix)

precision_matrix <- solve(cov_matrix)

# Calculate the partial correlation matrix
partial_corr_matrix <- -cov2cor(precision_matrix)

# Display the partial correlation matrix
print(partial_corr_matrix)


```

As we have determined across the two analysis methods we have used, age may be a useful variable to include in a strictly predictive model, but it does not seem to be a "key" variable in the sense that it has very little relationship with the dependent variable. Age may help clear up the picture of the dependent variable, but without sex, math self efficacy and math self-concept, it is essentially worthless. However, given the relative weakness of most of our predictors, it is not unlikely that the full 10-predictor model (or possibly 9-predictor, sans SDQ_L) may have the best predictive power on a larger test set, even if it includes several relatively unimportant variables.

It appears clear that the second LASSO model only reports the necessary variables to explain the DV's variance, whereas the first model includes covariates to produce an "optimal" model without too many covariates, but enough to have better predictive power than say, a 5 variable model. In fact, such a model is likely better at prediction than the fourth-selection model, which despite having the highest R squared of any model so far, may not generalize predictive ability as well as the first LASSO model outside of the relatively small test sample size. Across models, the inclusion of the age variable does not seem to contribute highly to the R squared of our models. The other seemingly key covariates of state anxiety and possibly text anxiety seem to add greater benefit in predictive modeling after we have naturally included the three main variables of sex, math self-efficacy, and math self-concept.

I also want to check out another LASSO model, one that popped up when I ran a different cv.glm for the lambda value. This one includes the seemingly key measures of anxiety.

```{r}
library(caret)


tunegrid <- expand.grid(.mtry = c(2:5))
control <- trainControl(method = "repeatedcv", number = 10, repeats = 3, search = "grid")

set.seed(101)
rf_lasso2 <- train(sum_arith_perf ~ sex + score_AMAS_total + score_STAI_state_short + score_PISA_ME + score_SDQ_M, data = scaled_training2, method = "rf", importance = TRUE, tuneGrid = tunegrid, trControl = control)
print(rf_lasso2)


predictionslassomodel2 <- predict(rf_lasso2, scaled_test2)
validationlassomodel2 <- data.frame(
  R2 = R2(predictionslassomodel2, scaled_test2$ sum_arith_perf),
  RMSE = RMSE(predictionslassomodel2, scaled_test2$ sum_arith_perf),
  MAE = MAE(predictionslassomodel2, scaled_test2$ sum_arith_perf)
) # these statistical functions are in caret technically under trainControl(), which passes this argument through defaultSummary() I believe.
print(validationlassomodel2)
# Mid prediction again.
```

Probably should've assumed this model would be poor, but I wanted to test it anyways just to see if LASSO had missed something.

Remember that with the way we selected the lambda value, LASSO is attempting to optimize for prediction. LASSO is NOT attempting to select only non-noise predictors given the "optimal" lambda. Therefore, if you were looking to use a smaller subset of predictors but retain great predictive accuracy, LASSO has you covered. If you're instead primarily hoping to identify the "important" variables, we will soon look at another LASSO-adjacent method that purports to do just that with the optimal lambda.

Before we try that final method however, let's do a little post-selection inference and try and see how our actual LASSO models look.

```{r}
library(selectiveInference)
n <- nobs(lassomodel)
fixedLassoInf(x2, y2, beta = coef(lassomodel, s = best_lambda / n)[-1], lambda = best_lambda)
##

n2 <- nobs(lassomodel2)
fixedLassoInf(x2, y2, beta = coef(lassomodel2, s = best_lambda2 / n2)[-1], lambda = best_lambda2)

n3 <- nobs(lassomodel2)
fixedLassoInf(x2, y2, beta = coef(lassomodel2, s = best_lambda2 / n2)[-1], lambda = best_lambda2)

```

Lastly, we will take a look at one more final method for regularization and variable selection in regression: Adaptive Lasso. Adaptive LASSO is an "evolution" of the LASSO (it's not really built on the LASSO though, as the LASSO is itself built on elastic net procedures) that uses a weighted (by coefficient) L1 penalty rather than a simple L1 penalty. Unlike LASSO, Adaptive LASSO appears to have "oracle" properties when the "correct" value of lambda, which means that it has the following properties - it will identify the "correct" subset model (won't include noise variables) and has an optimal estimation rate, which essentially just means that coefficients are estimated as accurately as can be from the given data. **This is extremely exciting, particularly because we estimated our LASSO** **model using the best lambda for prediction, rather than for to have the "correct" model.**

As you'll recall, LASSO and ridge regression introduce bias to counteract high variance, so adaptive LASSO can work around this by weighting penalties on coefficients.

Adaptive LASSO requires a two-step procedure, where coefficient estimates are estimated from a previously estimated model. The original author suggests using the OLS Beta estimates unless collinearity is a concern, in which case we will use ridge regression coefficients, which are more stable in the presence of multicollinearity. We'll do both of course.

To briefly summarize ridge vs. LASSO, ridge regression uses an L2 penalty which simply means that the penalty term squares the vector of Beta values whereas LASSO uses absolute deviation. Furthermore, ridge regression will retain all features in the model, some with very low coefficients whereas LASSO will reduce unimportant features to 0. Lastly, ridge regression will handle multicollinearity and outliers better than LASSO, but LASSO's reduce-coefficient-to-0 capabilities make it extremely attractive for feature selection, particularly in situations where you are not concerned about severe multicollinearity or severe P\>\>N (where ridge is often better).

Let's work out the OLS and ridge coefficients. We'll start with ridge since we've already done LASSO, not to mention some of our predictors actually are relatively correlated (probably not enough that we'd actually want to start with ridge if this were a real analysis, but still).

```{r}
set.seed(200)
ridge1 <- cv.glmnet(
  x = x2, y = y2,
  ## measure to optimize for cross validation
  type.measure = "mse",
  ## K = 10 is the default.
  nfold = 10,
  ## ‘alpha = 1’ is the lasso elastic net penalty, and ‘alpha = 0’ the ridge elastic net penalty penalty.
  alpha = 0
)


## Penalty vs CV MSE plot
plot(ridge1)

## 1se and 1min
ridge1$lambda.min
ridge1$lambda.1se

# coefficients
coef(ridge1, s = ridge1$lambda.min)
coef(ridge1, s = ridge1$lambda.1se)
## Oops, gotta drop intercept estimate, we don't need that right now.
best_ridge_coef <- as.numeric(coef(ridge1, s = ridge1$lambda.min))[-1]

best_ridge_coef2 <- as.numeric(coef(ridge1, s = ridge1$lambda.1se))[-1]

## okay guess we'll try adaptive lasso.
set.seed(201)
alasso1 <- cv.glmnet(
  x = x2, y = y2,
  type.measure = "mse",
  nfold = 10,
  alpha = 1,
  # penalty factors can be different for each coefficient. This number is multiplied by lambda to allow differing levels of shrinkage. Default is 1 for all variables. Also apparently, penalty factors are internally rescaled to sum to the number of variables, so the lambda sequence will reflect this change. As you can see, we are using the inverse of the absolute values of the ridge coefficients.
  penalty.factor = 1 / abs(best_ridge_coef),
  ## prevalidated array is returned
  keep = TRUE
)
## Penalty vs CV MSE plot
plot(alasso1)

## Extract coefficients at the two lambda values we like
alasso1$lambda.min
alasso1$lambda.1se

alassocoef1 <- coef(alasso1, s = alasso1$lambda.min)
alassocoef1
alassocoef2 <- coef(alasso1, s = alasso1$lambda.1se)
alassocoef2

```

The less stringent lambda produced almost the same model as the regular LASSO, but without GAD. We are probably happy with that - we know GAD hasn't really helped our models in the past. Seems like that adaptive lasso created a somewhat more parsimonious model, but still with some possible noise variables (indicating we did not use the "optimal" level of lambda, which is understandable of course).

Seems that the more stringent lambda value actually may have selected a model that contains the crucial variables in terms of explaining the data. - WRITE MORE HERE ITS INCOMPLETEEEEEE This could be because we're using ridge regression coefficients, and it may not be the best true fit for our predictors given that our predictors are not extremely correlated.

Let's do OLS now.

```{r}
lm1 <- lm(y2 ~ x2) # simple linear model
summary(lm1) # kind of interesting that SDQ_L keeps popping up as like, mildly relevant? But as we know, SDQ_L is kind of nonlinear so I bet that is influencing things significantly.

ols_coef <- coef(lm1)[-1] # Exclude the intercept
best_ols_coef <- 1 / abs(ols_coef)

# We'll skip cross validation because it would just be bootstrap, which is written about elsewhere.

# Fit lasso w ols weights
set.seed(300)
alasso_ols <- cv.glmnet(x2, y2,
  type.measure = "mse",
  nfold = 10,
  alpha = 1,
  # penalty factors can be different for each coefficient. This
  penalty.factor = best_ols_coef,
  ## prevalidated array is returned
  keep = TRUE
)



## Extract coefficients at the two lambda values we like
alasso_ols$lambda.min
alasso_ols$lambda.1se

alassocoef3 <- coef(alasso_ols, s = alasso_ols$lambda.min)
alassocoef3
alassocoef4 <- coef(alasso_ols, s = alasso_ols$lambda.1se)
alassocoef4

```

The more stringent lambda reported the same coefficients as the ridge adaptive lasso, which means including SDQ_M but not PISA_ME. The model with the less stringent lambda simply included PISA_ME as the third predictor. Like we mentioned way back at the beginning of the worksheet, PISA_ME and SDQ_M naturally correlate quite strongly. Thus, it's possible that the stringent adaptive LASSO selected SDQ_M and then saw little further value in including PISA_ME due to their correlation. The less stringent adaptive LASSO did seem to find unique value in including PISA_ME regardless of its correlation with SDQ_M.

Overall, it's clear the LASSO seems to have done a worse job selecting "important" features than

YOU WANNA ADD LASSO-SPECIFIC PREDICTIONS INSTEAD OF PUTTING THEM IN A RF AND LETTING THEM GO WILD. COMPARE AND CONTRAST THE VARIABLES LASSO SELECTED (HINT: THEY'RE THE REAL VARIABLES BUT WORSE PREDICTION!) CHECK WHAT YOU DID ON CHATGPT

YOU ALSO WANTED TO COMPARE PREDICTIVE POWER OF ALASSO VS LASSO, IF POSSIBLE (AKA JUST USING PREDICT FUNCTION ON EACH ONE BRO)

COMPARE AND CONTRAST THE LAMBDA VALUES, TOO.

MAKE SURE EVERY RANDOM OPERATION (CV.GLMNET, LITERALLY ANYTHING ELSE) HAS SET SEED RIGHT BEFORE IT!!!!!!!

**References**

Allaire, J.J., Teague, C., Scheidegger, C., Xie, Y., & Dervieux C. (2024). Quarto version 1.4. https://github.com/quarto-dev/quarto-cli

Brownlee, J. (2017, December 12th). *Project-oriented Workflow*. Tidyverse.org. https://machinelearningmastery.com/tune-machine-learning-algorithms-in-r/ .

Bryan, J. (2020, July 30). *Tune machine learning algorithms in R (random forest case study)*. MachineLearningMastery.com.https://www.tidyverse.org/blog/2017/12/workflow-vs-script/

Cipora, K., Lunardon, M., Masson, N., Georges, C., Nuerk, H.-C., & Artemenko, C. (2024). The AMATUS Dataset: Arithmetic Performance, Mathematics Anxiety and Attitudes in Primary School Teachers and University Students. *Journal of Open Psychology Data*, *12*. https://doi.org/10.5334/jopd.115

Chatterjee, T., & Chowdhury, R. (2017). Chapter 11 - Improved Sparse Approximation Models for Stochastic Computations. In *Handbook of Neural Computation* (pp. 201--223). essay, Academic Press.

Friedman J., Tibshirani R., Hastie T. (2010). "Regularization Paths for Generalized Linear Models via Coordinate Descent." *Journal of Statistical Software*, *33*(1), 1--22. [doi:10.18637/jss.v033.i01](https://doi.org/10.18637/jss.v033.i01).

Genuer, R., Poggi, J. M., & Tuleau-Malot, C. (2015). VSURF: An R package for variable selection using random forests. *The R Journal*, *7*(2), 19--33. https://doi.org/10.32614/rj-2015-018

Ilma, H. (2019, December 18). Ridge and LASSO Regression. https://algotech.netlify.app/blog/ridge-lasso/

Kuhn, M. (2008). "Building Predictive Models in R Using the caret Package." *Journal of Statistical Software*, *28*(5), 1--26. [doi:10.18637/jss.v028.i05](https://doi.org/10.18637/jss.v028.i05), <https://www.jstatsoft.org/index.php/jss/article/view/v028i05>.

Kursa MB, Rudnicki WR (2010). "Feature Selection with the Boruta Package." Journal of Statistical Software, 36(11), 1--13. https://doi.org/10.18637/jss.v036.i11.

Nahhas, R.W. (2024). *Introduction to Regression Methods for Public Health Using R.* CRC Press. https://bookdown.org/rwnahhas/RMPH/

Speiser, J. L., Miller, M. E., Tooze, J., & Ip, E. (2019). A Comparison of Random Forest Variable Selection Methods for Classification Prediction Modeling. *Expert systems with applications*, *134*, 93--101. https://doi.org/10.1016/j.eswa.2019.05.028

Tay, J.K. (2021, February 19). *The box-cox and Yeo-Johnson transformations for continuous variables*. Statistical Odds and Ends. https://statisticaloddsandends.wordpress.com/2021/02/19/the-box-cox-and-yeo-johnson-transformations-for-continuous-variables/

Tay J.K, Narasimhan B., Hastie T. (2023). "Elastic Net Regularization Paths for All Generalized Linear Models." *Journal of Statistical Software*, *106*(1), 1--31. [doi:10.18637/jss.v106.i01](https://doi.org/10.18637/jss.v106.i01).

Tibshirani, R.J., Taylor, J., Lockhart, R., & Tibshirani, R. (2016). Exact Post-Selection Inference for Sequential Regression Procedures. *Journal of the American Statistical Association*, *111*(514), 600--620. https://doi.org/10.1080/01621459.2015.1108848

Yoshida, K. (2017). Adaptive Lasso. *Rpubs by RStudio. https://rpubs.com/kaz_yos/alasso*

Zou, H. (2006). The Adaptive Lasso and Its Oracle Properties. *Journal of the American Statistical Association*, *101*(476), 1418--1429. https://doi.org/10.1198/016214506000000735

And a host of Stack Exchange questions as references:

https://stats.stackexchange.com/questions/174976/why-does-the-intercept-column-in-model-matrix-replace-the-first-factor

https://datascience.stackexchange.com/questions/39932/feature-scaling-both-training-and-test-data

https://stats.stackexchange.com/questions/350484/why-is-r-squared-not-a-good-measure-for-regressions-fit-using-lasso

https://stats.stackexchange.com/questions/522265/residual-analysis-for-the-lasso-estimator

https://stats.stackexchange.com/questions/6502/lasso-assumptions

https://stats.stackexchange.com/questions/291409/inference-after-using-lasso-for-variable-selection

https://math.stackexchange.com/questions/2162932/big-picture-behind-how-to-use-kkt-conditions-for-constrained-optimization

https://stats.stackexchange.com/questions/156098/cross-validating-lasso-regression-in-r

https://stats.stackexchange.com/questions/403310/per-cent-increase-in-mse-incmse-random-forests-importance-measure-why-is-mea

https://stackoverflow.com/questions/71900366/randomforest-error-this-function-only-works-for-objects-of-class-rfsrc-grow

https://stats.stackexchange.com/questions/485471/when-in-adaptive-lasso-process-does-it-make-sense-to-constrain-control-variable

https://stats.stackexchange.com/questions/487412/is-this-the-correct-way-to-run-an-adaptive-lasso
