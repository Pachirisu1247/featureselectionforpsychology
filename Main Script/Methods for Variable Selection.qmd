---
title: "Methods for Variable Selection - Random Forest"
format: html
editor: visual
---

## Manual and Automated Feature Selection Methods: VSURF, Boruta, RFE

```{r}
#| label: fig-preds_DV_2
#| warning: false
#| fig-cap: "Distribution of predictors against the DV (Page 2)"
#| fig-width: 12
#| fig-height: 10
#| fig-pos: "H"

facet_plot_1 <- ggplot(
  long_data, aes(x = Value, y = .data[[response_variable]])) +  
    geom_point(alpha = 0.7, color = "lightblue") +  
    geom_smooth(method = "lm", color = "red", se = FALSE) +  
    geom_smooth(method = "loess", color = "green", se = FALSE) +  
    facet_wrap_paginate(~Predictor, ncol = 3, nrow = 3, scales = "free", 
                      page = 1) +  
  theme(axis.text = element_text(size = 8))

facet_plot_1




# Create paginated facet plot (Page 2)
facet_plot_2 <- ggplot(
  long_data, aes(x = Value, y = .data[[response_variable]])) +  
    geom_point(alpha = 0.7, color = "lightblue") +  
    geom_smooth(method = "lm", color = "red", se = FALSE) +  
    geom_smooth(method = "loess", color = "blue", se = FALSE) +  
    facet_wrap_paginate(~Predictor, ncol = 3, nrow = 3, scales = "free",
                        page = 2) +  
  theme(axis.text = element_text(size = 8))

facet_plot_2
```

```{r}

# Write a function to generate the plots
plot_scatter_smooth <- function(data, x_var, y_var) {
  ggplot(data, aes(x = .data[[x_var]], y = .data[[y_var]])) +  
    # Using .data[[var]] for tidy evaluation. Actual objects passed in 
    # the `lapply()` function later on
    geom_point(fill = "lightblue", alpha = 0.7) + # fill of the plot
    geom_smooth(method = "lm", color = "red", se = FALSE) +  
    # for linear trendline
    geom_smooth(method = "loess", color = "blue", se = FALSE)
    # for nonlinear trend line
}

# Generate plots using the predictor variables
plots <- lapply(numeric_predictors, function(var) plot_scatter_smooth(amatusclean, var, response_variable))

# Display plots
plots[1:2]

library(ggplot2)


















library(rpart)
library(rpart.plot)
library(ggplot2
        )



# Extract variable importance scores
var_imp <- varImp(rf_mtry2)$importance

# Normalize the importance scores (scale to 0-100)
var_imp$Normalized <- (var_imp$Overall / max(var_imp$Overall)) * 100

# Sort the scores in descending order for curve fitting
var_imp <- var_imp[order(-var_imp$Normalized), ]

# Add variable rank for plotting
var_imp$Rank <- seq_along(var_imp$Normalized)

# Calculate the differences between consecutive importance scores
var_imp$Difference <- c(NA, diff(var_imp$Normalized))

# Find the elbow point using a heuristic (largest drop in importance)
elbow_index <- which.max(-var_imp$Difference[-1]) + 1  # Exclude first NA and adjust index
threshold <- var_imp$Normalized[elbow_index]

# Select variables with importance above the threshold
selected_variables <- rownames(var_imp)[var_imp$Normalized >= threshold]

# Print results
cat("Elbow Threshold (VI):", threshold, "\n")
cat("Selected Variables:", paste(selected_variables, collapse = ", "), "\n")

# Fix the issue with assigning the "Selected" column
var_imp$Selected <- ifelse(var_imp$Rank == elbow_index, "Elbow", "Other")

# Plot the VI curve with ggplot
vicurve<-ggplot(var_imp, aes(x = Rank, y = Normalized)) +
  geom_line(color = "blue") +
  geom_point(aes(color = Selected), size = 3) +
  scale_color_manual(values = c("Elbow" = "red", "Other" = "black")) +
  labs(
    title = "Variable Importance Curve with Elbow Point",
    x = "Variable Rank",
    y = "Normalized Variable Importance"
  ) +
  geom_text(
    data = subset(var_imp, Selected == "Elbow"),
    aes(label = "Elbow"),
    hjust = -0.2, vjust = -0.5,
    color = "red"
  ) +
  theme_minimal()

vicurve


# Replace "sexm" with "sex" in selected_variables

selected_variables<-c("sex","age","score_PISA_ME", "score_SDQ_M", "score_AMAS_total", "score_GAD", "score_STAI_state_short", "score_SDQ_L")

selected_variables <- gsub("sexm", "sex", selected_variables)

# Step 2: Variable selection
# Create nested models with the selected variables

oob_errors <- numeric(length(selected_variables))
for (k in seq_along(selected_variables)) {
  # Select the top k variables
  selected_vars_k <- selected_variables[1:k]
  
  # Fit a Random Forest model with the selected variables using caret
  formula <- as.formula(paste("sum_arith_perf ~", paste(selected_vars_k, collapse = " + ")))
  
  # Subset the dataset for the selected variables
  subset_data <- scaled_training[, c("sum_arith_perf", selected_vars_k), drop = FALSE]
  
  # Define a valid range for mtry (1 to the number of predictors)
  max_mtry <- length(selected_vars_k)
  tune_grid <- expand.grid(mtry = seq(1, max_mtry))  # mtry must be between 1 and the number of predictors
  
  # Train the model using caret
  rf_temp <- train(
    formula,
    data = subset_data,
    method = "rf",
    trControl = control,
    tuneGrid = tune_grid
  )
  
  # Record the OOB error (use RMSE for regression)
  oob_errors[k] <- min(rf_temp$results$RMSE)
}

# Find the optimal number of variables (smallest OOB error)
optimal_k <- which.min(oob_errors)
optimal_variables <- selected_variables[1:optimal_k]

# Print results
cat("Optimal number of variables:", optimal_k, "\n")
cat("Selected variables:", paste(optimal_variables, collapse = ", "), "\n")

# Plot OOB error for visualization
plot(oob_errors, type = "b", xlab = "Number of Variables", ylab = "OOB Error (RMSE)",
     main = "OOB Error vs. Number of Variables")
```

```{r}
# LM models.

linearrfmodel<-lm(sum_arith_perf~., data=training_subset)
plot(linearrfmodel)
summary(linearrfmodel)


linearrfmodel2<-lm(log(sum_arith_perf)~score_PISA_ME+sex+total_time_minutes+score_SDQ_M, data=na.omit(subset(training_subset, sum_arith_perf > 0)))
summary(linearrfmodel2)
plot(linearrfmodel2)


```

As an addendum to `Introducing Machine Learning Models for Psychologists - Random Forest`, I will walk through a manual example of a variable selection procedure described in the literature for random forest as well as provide several examples of explicit automated methods for variable selection. VSURF, the first method I will describe, essentially uses backward stepwise selection followed by forward selection based on variable importance (using a basic definition as we used above being, depending on the task, the difference between MSE/misclassification rate after permuting each predictor). There have been several methods for variable selection described in the literature, however, a recent simulation study demonstrated that VSURF often demonstrates some of the best performance for classification tasks (Speiser et al.), which may extend to regression. However, it is worth noting that VSURF is prediction oriented and as such, accepts the risk of producing false negatives.

(We will demonstrate an unbiased advanced method of obtaining variable importance measures using conditional inference forests at the end of this document.)

An aside on scaling variables: We will choose to standardize our variables using the `scale()` function, which you may recognize. The `scale()` function will center (subtract mean from the value for a mean of 0) and scale (divide value by its standard deviation for an SD of 1) our continuous predictors. We don't include categorical variables. We'll also go ahead and get started by creating our first random forest model. Let's try using the `caret` package, a relatively general machine learning package.

(Another function that can be used to standardize is the `preProcess()` function in the `caret` package, and it may be better or easier to use for your purposes. I just like `scale()`.)

***Importing the Data*** Below, I will reprint the data import and cleaning code from the original blog post regarding variable selection in Random Forest models. We are again using the AMATUS dataset.

```{r}
#| label: Load packages, clean data
#| warning: false
library(here)

library(tidyverse)
library(caret)
library(readxl)
library(readr)
library(ggplot2)
library(randomForest)

here::i_am("Dr. Lai Feature Selection Project 9.13.24.rproj")
#If you'd like to, you can manually set your path with the command above. #I've set it to my rproj file
#Otherwise, just check with the following
#command:
here::here()

# adjust as needed.
file_path <- normalizePath(here::here("Main Script/OSF archive/AMATUS_dataset.csv"))


amatus <- read_csv2(file_path, c("", "NA"), col_names = TRUE) 

View(amatus)
table(amatus$sum_arith_perf)

amatusclean <- amatus[!is.na(amatus$sum_arith_perf), ] 

amatusclean <- amatusclean[!(amatusclean$sample %in% 
c("german_teachers", "belgian_teachers")), ] 

amatusclean$sex <- as.factor(amatusclean$sex)
amatusclean$age_range <- as.factor(amatusclean$age_range)
amatusclean$breaks <- as.factor(amatusclean$breaks)
amatusclean$honesty <- as.factor(amatusclean$honesty)
amatusclean$native_speaker <- as.factor(amatusclean$native_speaker)
amatusclean$noise <- as.factor(amatusclean$noise)

amatusclean <- amatus[!is.na(amatus$sum_arith_perf), ] 
amatusclean <- amatusclean[!(amatusclean$sample %in% 
c("german_teachers", "belgian_teachers")), ] 


library(caret)
set.seed(39) 
inTrain <- createDataPartition(amatusclean$sum_arith_perf,
  p = 0.7, list = FALSE
)

# create training vs test data.
training <- amatusclean[inTrain, ]
test <- amatusclean[-inTrain, ]
View(training)
View(test)

numeric_predictors <- c(
  "score_BFI_N", "score_AMAS_total", "age", "score_GAD",
  "score_PISA_ME", "score_STAI_state_short",
  "score_TAI_short", "score_SDQ_L", "score_SDQ_M"
)
categorical_variable <- "sex"
response_variable <- "sum_arith_perf"


scaled_training <- training %>%
  mutate(across(all_of(numeric_predictors), ~ scale(.) %>% as.vector())) %>%
  select(all_of(numeric_predictors), all_of(categorical_variable), all_of(response_variable)) 

```

***First VSURF Run***

```{r}
#| label: Automated variable selection example
if (!require(VSURF)) {
  install.packages("VSURF",
    repos = "http://cran.us.r-project.org"
  )
}

library(VSURF)
rm(x)
#we don't really care about only testing a few mtry values at this point 
#since the best has been mtry=2 every time, so.
set.seed(46)
vamatus <- VSURF(sum_arith_perf ~ score_AMAS_total + sex +
  age + score_BFI_N +
  score_GAD + score_PISA_ME +
  score_STAI_state_short + score_TAI_short +
  score_SDQ_M +
  score_SDQ_L, data = scaled_training, na.action = na.omit)

View(training)

print(vamatus$varselect.pred) # predictors vsurf selected at final stage.
```

VSURF suggests that the best model would contain two variables, `PISA_ME` and `sex`. Keep in mind this is the most powerful predictive model with the fewest variables. Thus, sex may explains a lot more of the variance than a variable like self-concept when self-efficacy is included in the model (and in fact, including all three may again lead to a less parsimonious model, or worse predictions).

Let's work it out in a random forest paradigm for fun. This should hopefully show us the predictive benefit of using such an automated system to create a parsimonious model.

***VSURF Selection Model***

```{r}
#| label: Building random forests out of the VSURF-selected variables

control <- trainControl(
  method = "repeatedcv", number = 10, repeats = 3,
  search = "grid"
)

tunegrid <- expand.grid(.mtry = c(2:3)) 
set.seed(47)
rf_strongestvariables <- train(
  sum_arith_perf ~ sex + score_PISA_ME +
    score_SDQ_M,
  data = scaled_training, method = "rf", importance = TRUE,
  tuneGrid = tunegrid, trControl = control
)
print(rf_strongestvariables)
varImpPlot(rf_strongestvariables$finalModel, type = 1)
# type 1 refers to estimates following perturbation.



tunegrid <- expand.grid(.mtry = c(2:2)) 
set.seed(48)
rf_strongestvariables2 <- train(sum_arith_perf ~ sex + score_SDQ_M,
  data = scaled_training, method = "rf", importance = TRUE,
  tuneGrid = tunegrid, trControl = control
)
print(rf_strongestvariables2)

varImpPlot(rf_strongestvariables2$finalModel, type = 1)
# type 1 refers to estimates following perturbation.


set.seed(49)
rf_strongestvariables3 <- train(sum_arith_perf ~ score_PISA_ME + score_SDQ_M,
  data = scaled_training, method = "rf", importance = TRUE,
  tuneGrid = tunegrid, trControl = control
)
print(rf_strongestvariables3)

varImpPlot(rf_strongestvariables3$finalModel, type = 1)
# type 1 refers to estimates following perturbation.


# this will be the VSURF selected model.
set.seed(50)
rf_strongestvariables4 <- train(sum_arith_perf ~ score_PISA_ME + sex,
  data =
    scaled_training, method = "rf", importance = TRUE, tuneGrid = tunegrid,
  trControl = control
)
print(rf_strongestvariables4)

varImpPlot(rf_strongestvariables4$finalModel, type = 1)
# type 1 refers to estimates following perturbation.

```

As you can see, the model with sex and self-efficacy has by far the lowest `MSE` and the best R-squared (though the model with `sex` and `SDQ_M` seemed to do fine, probably because `PISA_ME` and `SDQ_M` cover a lot of the same ground). Its r-squared is comparable to models with many more variables, and its `MSE` isn't too bad either. It's safe to say that this is definitely the most parsimonious model in the sense that it explains a ton of the variance with only two variables. Adding a couple more variables should likely increase raw predictive power, but if you're looking to identify the model with the absolute best predictive power in the fewest variables, you likely need to look no further.

## Boruta

Boruta is a package in R that conducts feature selection using its own algorithm. In a nutshell, Boruta iteratively cuts features that appear to provide less importance to the model than random probes. It uses a similar permutation-based method, however, to determine prediction error increase when a variable's values are permuted.

```{r}
#| label: Running Boruta package
# Load necessary libraries
library(Boruta)
library(randomForest)


# Set the seed for reproducibility
set.seed(123)
boruta_output <- Boruta(
  sum_arith_perf ~ score_AMAS_total + sex + age + score_BFI_N +
    score_GAD + score_PISA_ME + score_STAI_state_short + score_TAI_short +
    score_SDQ_M + score_SDQ_L,
  data = scaled_training, doTrace = 2,  # to set up the cross validation
  maxRuns=150, #max number of times it tries to run the importance.
  ) 
 # 2 specifies most detail.

# Print the results
print(boruta_output)
# View final decision on variable importance
print(boruta_output$finalDecision)

# Plotting the importance
plot(boruta_output, las = 2, cex.axis = 0.7)

```

As you can see, Boruta reports pretty much what we expected with regards to unimportant and important variables. Boruta reports `SDQ_L`, `BFI_N`, and `TAI_short` as the least important variables, which aligns with our pseudo-feature selection process we underwent earlier. Even more interestingly, we had originally determined that sex, math self-efficacy, math self-concept, and at least one of the anxiety variables (except test anxiety) seemed to be crucial to predicting arithmetic performance. We can see that that lines up with Boruta's output: the three important variables are above all others while `AMAS_total` and test anxiety are also confirmed, though AMAS seems to be a bit more powerful. Thus, our backwards selection procedure did seem to be good for intuitively identifying the most important variables. However, as mentioned earlier, backwards selection is dangerous for individual variable interpretation, and automated methods are almost always more statistically valid.

Alright just for fun: one more Boruta run after removing the three "worst" variables.

***Second Boruta Run***

```{r}
#| label: Second run of Boruta
# Load necessary libraries
library(Boruta)
library(randomForest)


# Set the seed for reproducibility
set.seed(123)
boruta_output2 <- Boruta(
  sum_arith_perf ~ score_AMAS_total + sex + age +
    score_GAD + score_PISA_ME + score_STAI_state_short +
    score_SDQ_M,
  data = scaled_training, doTrace = 2, #2 specifies most detail.
  #default maxRun is 100, btw
  ) 

# Print the results
print(boruta_output2)

# View final decision on variable importance
print(boruta_output2$finalDecision)

# Plotting the importance
plot(boruta_output2, las = 2, cex.axis = 0.7)
```

One more piece of the puzzle --- Boruta does indeed seem to find that age is tentatively a useful predictor. This somewhat aligns with our earlier findings that age continued to pop up in our recursively smaller models, but eventually became the least important predictor. It probably isn't included in the best prediction models or even the "true subset of variables" models, but still nice to know

## RFE for Prediction

Lastly, I'm going to provide code for a recursive feature selection (RFE) process through the caret package. It starts by fitting a model to all predictors. It then removes the worst predictor from each model, then it refits the model and reevaluates the model fit and the descriptive statistics for each predictor in the model, now sans the removed variable. This process repeats until the model fit is not improved by removing a variable. RFE is really just backwards selection, so it is quite simple. However, any kind of stepwise selection procedure (even ) are far from the best choice to do feature selection for most models. Instead, use a better automated method, like the ones detailed above or best subsets regression (where the algorithm runs every single model that could be created from X predictor variables and selects the one with the best model fit), or use your knowledge of the field and to select the variables you think will have an effect on the DV a priori.

The RFE takes several hours to complete, maybe more if your machine isn't too powerful. Keep that in mind when you're running it.

```{r}
#| label: RFE for Prediction
#| warning: false
# Load necessary libraries
library(caret)

# Define control for RFE
control <- rfeControl(
  method = "repeatedcv", # Use repeated cross-validation
  number = 10, # Number of folds
  repeats = 3, # Number of repetitions
  returnResamp = "final" # Return the final results
)

# Set seed for reproducibility
set.seed(123)
# Run RFE
rfe_output <- rfe(
  x = scaled_training[, c(
    "score_AMAS_total", "sex", "age", "score_BFI_N",
    "score_GAD", "score_PISA_ME", "score_STAI_state_short",
    "score_TAI_short", "score_SDQ_M", "score_SDQ_L"
  )],
  y = scaled_training$sum_arith_perf,
  sizes = c(1:10), # Specify the number of predictors to evaluate
  rfeControl = control,
  method = "rf" # Use random forest as the model
)

# Print results
print(rfe_output)

# Looks like the best model was with all 10 variables.
rfe_output$fit

rfe_output$optVariables

# mean importance by variable
# Extract the variable importance information from the RFE object
rfe_output$variables # this is importance across all subsets
caret::varImp(rfe_output) # this is importance in final model
```

Spoilers --- the best predictive performance seems to be with all ten variables, like we sort of anticipated. It seems that due to low effect sizes for most variables and the fact that some variables, like certain anxiety variables or age, may not have a strong correlation with the DV (or even that much importance in random forest models), but they seem to be very useful for prediction, hopefully not by overfitting on the random noise and creating false patterns, but by hopefully acting as suppressors that improve the performance of other variables in the set (which we will again test using test data).

Now, in a second addendum entitled "Random Forest Test Prediction", we will test the predictive accuracy of all models created for the original "Intermediate Models...Random Forest and LASSO" document as well as all models created in this addendum above. We will do so by evaluating the model fit (using r-squared) on the test set of data.
