<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Francisco N. Ramos">
<meta name="dcterms.date" content="2025-02-25">

<title>Introducing Machine Learning Models for Psychologists - Random Forest</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="RandomForest_files/libs/clipboard/clipboard.min.js"></script>
<script src="RandomForest_files/libs/quarto-html/quarto.js"></script>
<script src="RandomForest_files/libs/quarto-html/popper.min.js"></script>
<script src="RandomForest_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="RandomForest_files/libs/quarto-html/anchor.min.js"></script>
<link href="RandomForest_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="RandomForest_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="RandomForest_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="RandomForest_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="RandomForest_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introducing-machine-learning-models-for-psychologists---random-forest" id="toc-introducing-machine-learning-models-for-psychologists---random-forest" class="nav-link active" data-scroll-target="#introducing-machine-learning-models-for-psychologists---random-forest"><span class="header-section-number">1</span> Introducing Machine Learning Models for Psychologists - Random Forest</a>
  <ul class="collapse">
  <li><a href="#the-task-at-hand" id="toc-the-task-at-hand" class="nav-link" data-scroll-target="#the-task-at-hand"><span class="header-section-number">1.1</span> The Task at Hand</a></li>
  <li><a href="#the-data" id="toc-the-data" class="nav-link" data-scroll-target="#the-data"><span class="header-section-number">1.2</span> The Data</a>
  <ul class="collapse">
  <li><a href="#dependent-variable-definition" id="toc-dependent-variable-definition" class="nav-link" data-scroll-target="#dependent-variable-definition"><span class="header-section-number">1.2.1</span> Dependent Variable Definition</a></li>
  <li><a href="#predictors-definitions" id="toc-predictors-definitions" class="nav-link" data-scroll-target="#predictors-definitions"><span class="header-section-number">1.2.2</span> Predictors Definitions</a></li>
  </ul></li>
  <li><a href="#cleaning-the-data-and-a-little-exploratory-data-analysis" id="toc-cleaning-the-data-and-a-little-exploratory-data-analysis" class="nav-link" data-scroll-target="#cleaning-the-data-and-a-little-exploratory-data-analysis"><span class="header-section-number">1.3</span> Cleaning the Data and a Little Exploratory Data Analysis</a>
  <ul class="collapse">
  <li><a href="#dependent-variable-distribution" id="toc-dependent-variable-distribution" class="nav-link" data-scroll-target="#dependent-variable-distribution"><span class="header-section-number">1.3.1</span> Dependent Variable Distribution</a></li>
  <li><a href="#predictor-distributions" id="toc-predictor-distributions" class="nav-link" data-scroll-target="#predictor-distributions"><span class="header-section-number">1.3.2</span> Predictor Distributions</a></li>
  </ul></li>
  <li><a href="#standardization-and-trainingtest-sets" id="toc-standardization-and-trainingtest-sets" class="nav-link" data-scroll-target="#standardization-and-trainingtest-sets"><span class="header-section-number">1.4</span> Standardization and Training/Test Sets</a>
  <ul class="collapse">
  <li><a href="#setting-up-the-traintest-split" id="toc-setting-up-the-traintest-split" class="nav-link" data-scroll-target="#setting-up-the-traintest-split"><span class="header-section-number">1.4.1</span> Setting up the Train/Test Split</a></li>
  <li><a href="#a-note-on-featurevariable-selection" id="toc-a-note-on-featurevariable-selection" class="nav-link" data-scroll-target="#a-note-on-featurevariable-selection"><span class="header-section-number">1.4.2</span> A Note on Feature/Variable Selection</a></li>
  </ul></li>
  <li><a href="#tuning-your-hyperparameters-for-your-random-forest-model" id="toc-tuning-your-hyperparameters-for-your-random-forest-model" class="nav-link" data-scroll-target="#tuning-your-hyperparameters-for-your-random-forest-model"><span class="header-section-number">1.5</span> Tuning your Hyperparameters for your Random Forest Model</a></li>
  <li><a href="#tuning-packages-in-r" id="toc-tuning-packages-in-r" class="nav-link" data-scroll-target="#tuning-packages-in-r"><span class="header-section-number">1.6</span> Tuning Packages in R</a></li>
  <li><a href="#training-the-models" id="toc-training-the-models" class="nav-link" data-scroll-target="#training-the-models"><span class="header-section-number">1.7</span> Training the Models</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">1.8</span> References</a></li>
  </ul></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Introducing Machine Learning Models for Psychologists - Random Forest</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Francisco N. Ramos </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 25, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="introducing-machine-learning-models-for-psychologists---random-forest" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introducing Machine Learning Models for Psychologists - Random Forest</h1>
<p>In this document, I will walk through a simple tutorial for fitting and analyzing a random forest model of typical psychological data.</p>
<p>Although the workings of random forest models are explained in depth elsewhere (<a href="https://einsteinmed.edu/uploadedfiles/centers/ictr/new/intro-to-random-forest.pdf">click here for a more detailed but digestible explanation</a>), to be brief, random forest models are built from single decision trees, which predict a response variable/dependent variable (DV) based on binary answers to other questions, which are selected in order to create as homologous of groups as possible. For example, a tree might first decide to check “Is the social anxiety score over or under 10.5?”, and divide the data based on each record’s “answer”. Then, it will continuously repeat the process with new questions designed to improve the decisions, like “Is sex male or female?”, until it has finalized its prediction and new questions do not improve its output. A random forest model is built from averaging the observations of thousands or more trees, all of which ask different “questions” and may assess different variables <span class="citation" data-cites="breiman_random_2001">(<a href="#ref-breiman_random_2001" role="doc-biblioref">Breiman, 2001</a>)</span>.</p>
<p>Random forest models usually do well in describing nonlinear/complex relationships between predictor and response variables, are robust (though can technically overfit), and can handle missing and categorical data. However, they are more difficult to interpret than other models like linear regression. Random forests are popular in fields such as biostatistics because they are excellent for problems where the number of predictors <em>p</em> exceeds the number of samples/participants <em>n</em>. Such problems are not as common in psychology, but random forests still model <em>n</em>&gt;<em>p</em> problems well, as should be expected.</p>
<p>This is the first part of a set of documents walking through building simple random forest models for psychological data, including the forthcoming addenda <code>Methods of Variable Selection</code>, <code>LASSO (Least absolute shrinkage and selection operator)</code>, and finally <code>Predictions</code>.</p>
<p>All references (including the dataset) are at the end of the <code>LASSO</code> addendum to this document.</p>
<section id="the-task-at-hand" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="the-task-at-hand"><span class="header-section-number">1.1</span> The Task at Hand</h2>
<p>Importantly, I am conducting a <strong>regression</strong> task (predicting a continuous variable), not a <strong>classification</strong> task (predicting a dichotomous or polytomous variable).</p>
<p>The <strong>first</strong> main objective I’ll be attempting to address, and the <strong>only one addressed in this specific document</strong> (the other tasks are addressed in addendum posts, which I link to at the top and bottom of this document as well as throughout), is that I would like to build a suitable predictive model that generalizes well to unseen data of arithmetic performance score in German students from a set of predictor variables including age, sex, self-reported math anxiety, and others. Testing on unseen data is specifically performed in my <code>Predictions</code> addendum.</p>
<p>My <strong>second</strong> main objective is to find a “parsimonious” model, which uses a small number of variables to make “good” or “sufficient” prediction. This may be useful insofar as, for example, future researchers do not have to waste resources collecting unimportant variables for no or minimal improvement in prediction. This can be found in my forthcoming <code>Methods for Variable Selection</code> and <code>LASSO</code> addenda.</p>
<p>A related <strong>subgoal</strong> is another common objective, that being I would like to have a good sense of which predictor variables appear useful in predicting arithmetic performance and which ones do not. In other words, I would like to identify which variables are important to predicting the response, regardless of their correlation with other predictors. This will also be addressed in my forthcoming <code>Methods for Variable Selection</code> and <code>LASSO</code> addenda.</p>
<p>I have not seen the data ahead of time. The analyses described hereafter are entirely novel.</p>
</section>
<section id="the-data" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="the-data"><span class="header-section-number">1.2</span> The Data</h2>
<p>The dataset I will investigate in this document is from Cipora and colleagues <span class="citation" data-cites="cipora_amatus_2024">(<a href="#ref-cipora_amatus_2024" role="doc-biblioref">2024</a>)</span>. It is characteristic of many psychology datasets — it has a solid but not too large sample size (735 N before being split into test/training sets), a good but not excessive number of potential predictors (20+ collected as part of the survey, many of which are likely unhelpful in prediction), and what is likely middling to low effect sizes for predictors.</p>
<p>For what it’s worth, this dataset does not scream “use random forest”. The appeal of random forests is often their proficiency in modeling <em>p</em> <span class="math inline">\(\ggg\)</span> <em>n</em> situations, particularly because random forest models often do best on sizeable datasets. However, random forest models are able to model many situations, including <em>n</em>&gt;<em>p</em>, well and often do so competitively with other models, such as linear regression or other machine learning models. I will find out if this holds for our dataset in my <code>Prediction</code> addendum.</p>
<p>For simplicity and didactic purposes, I have pulled twenty variables from the full dataset for my analysis. Some will have a relationship with the DV, others will not. <em>A priori</em> variable selection according to expert knowledge of what variables are likely to have predictive relevance would likely be the best variable selection method, though I have retained all suitable variables for this demonstration. Variable selection for this dataset is the focus of my forthcoming <code>Methods for Variable Selection</code> addendum.</p>
<p>Below, I define the single DV as well as the twenty candidate predictor variables followed by the syntax for their object in R.</p>
<section id="dependent-variable-definition" class="level3" data-number="1.2.1">
<h3 data-number="1.2.1" class="anchored" data-anchor-id="dependent-variable-definition"><span class="header-section-number">1.2.1</span> Dependent Variable Definition</h3>
<ol type="1">
<li>Arithmetic performance/<code>sum_arith_perf</code>, as measured by “the number of correctly solved problems in order as instructed” on a simple arithmetic speed test.</li>
</ol>
</section>
<section id="predictors-definitions" class="level3" data-number="1.2.2">
<h3 data-number="1.2.2" class="anchored" data-anchor-id="predictors-definitions"><span class="header-section-number">1.2.2</span> Predictors Definitions</h3>
<ol type="1">
<li><p>Age/<code>age</code>, as measured in years.</p></li>
<li><p>Sex/<code>sex</code>, where 1 = male, 2 = female, and 3=other. Participants who ignored this question were removed.</p></li>
<li><p>Neuroticism/<code>score_BFI_N</code>, as measured by the sum score of the 8 items of the Big Five Inventory (short version) pertaining to neuroticism.</p></li>
<li><p>Math anxiety/<code>score_AMAS_total</code>, as measured by the sum score on the Abbreviated Math Anxiety Scale.</p></li>
<li><p>General trait anxiety/<code>score_GAD</code>, as measured by sum score on the Generalized Anxiety Disorder Screener (GAD-7).</p></li>
<li><p>Math self-efficacy/<code>score_PISA_ME</code>, as measured by using the sum score of six items designed by the OECD for a previous study.</p></li>
<li><p>General state anxiety/<code>score_STAI_state_short</code>, as assessed by the sum of the five-item scale STAI-SKD.</p></li>
<li><p>Test anxiety/<code>score_TAI_short</code>, as measured by the sum score of the 5 items on the short version of the Test Anxiety Inventory.</p></li>
<li><p>Math self-concept/<code>score_SDQ_M</code>, as measured by the sum score of the four math-related statements on the Self-Description Questionnaire III. Evaluates variables such as one’s comfort/enjoyment/pride with math, whereas self-efficacy evaluates one’s self-confidence in math abilities.</p></li>
<li><p>Language self-concept/<code>score_SDQ_L</code>, as measured by the sum score of the four language-related statements on the Self-Description Questionnaire III.</p></li>
<li><p>Total duration of survey/<code>total_time_minutes</code>, as measured by the number of full minutes a participant took to complete the survey (excluding the timed arithmetic performance section).</p></li>
<li><p>Last math grade at school/<code>math_grade</code>, as measured on a scale ranging from 1-6, where 1 is the best grade and 6 is the worst. (I will reverse this in data cleaning)</p></li>
<li><p>Participant subject’s relation to math/<code>math_load</code>, where a “1” means a participant’s subject of study has low relation to math, a “2” means a participant’s subject has medium relation to math, and a “3” means a participant’s subject has high relation to math.</p></li>
<li><p>Influence of math load on participant’s subject choice/<code>math_inf_program_choice</code>, as measured by a 9-point Likert scale where a “1” means “chosen because I wanted to avoid math subjects”, a “5” means “no role” and a “9” means “chosen because I wanted to take math subjects”.</p></li>
<li><p>Level of liking math/<code>liking_math</code>, as measured by a 5-point Likert scale where a “1” means “not at all” and a 5 means “absolutely”.</p></li>
<li><p>Level of liking science/<code>liking_science</code>, as measured by a 5-point Likert scale where a “1” means “not at all” and a 5 means “absolutely”.</p></li>
<li><p>Level of liking humanities/<code>liking_humanities</code>, as measured by a 5-point Likert scale where a “1” means “not at all” and a 5 means “absolutely”.</p></li>
<li><p>Persistence in solving tasks in mathematics/<code>persistence_math</code>, as measured by a 5-point Likert scale where a “1” means “I get discouraged very fast” and a “5” means “I am very persistent”.</p></li>
<li><p>Persistence in solving tasks in science/<code>persistence_science</code>, as measured by a 5-point Likert scale where a “1” means “I get discouraged very fast” and a “5” means “I am very persistent”.</p></li>
<li><p>Persistence in solving tasks in humanities/<code>persistence_humanities</code>, as measured by a 5-point Likert scale where a “1” means “I get discouraged very fast” and a “5” means “I am very persistent”.</p></li>
</ol>
</section>
</section>
<section id="cleaning-the-data-and-a-little-exploratory-data-analysis" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="cleaning-the-data-and-a-little-exploratory-data-analysis"><span class="header-section-number">1.3</span> Cleaning the Data and a Little Exploratory Data Analysis</h2>
<p>I’ll start with importing and cleaning the data to make sure it fits the task. I load the most basic libraries - I will load other libraries at the start of later code chunks when needed. The package required for each function in this document is specified in the function call, unless it is available in base R.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(here)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(readr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <code>here()</code> function is convenient for creating paths and directories that won’t break when files are moved or when you reproduce the work on another computer <span class="citation" data-cites="muller_here_2020">(<a href="#ref-muller_here_2020" role="doc-biblioref">Müller, 2020</a>)</span>. More information is available at the following URL: https://here.r-lib.org/</p>
<p>Below, I’ll use the <code>here()</code> function to tell the <code>readr()</code> function where to go to get the dataset. The data was stored in my “Main Script” folder, in a subfolder called “OSF archive”, and named “AMATUS_dataset.csv”. Notice how I use a <code>normalizePath()</code> function to convert relative path names to absolute path names, which helps when switching OSes or from cloud to local storage.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>here<span class="sc">::</span><span class="fu">i_am</span>(<span class="st">"Dr. Lai Feature Selection Project 9.13.24.rproj"</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="do">## here() starts at /Users/frankie/Desktop/Dr. Lai Feature Selection Project 9.13.24</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># I manually set "here" to the folder containing my rproj file.</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>file_path <span class="ot">&lt;-</span> <span class="fu">normalizePath</span>(here<span class="sc">::</span><span class="fu">here</span>(</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>  <span class="st">"Main Script/OSF archive/AMATUS_dataset.csv"</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>))</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Also adjust this as needed. Function from base R.</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing dataset</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>amatus <span class="ot">&lt;-</span> readr<span class="sc">::</span><span class="fu">read_csv2</span>(file_path, <span class="fu">c</span>(<span class="st">""</span>, <span class="st">"NA"</span>), <span class="at">col_names =</span> <span class="cn">TRUE</span>)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="do">## ℹ Using "','" as decimal and "'.'" as grouping mark. Use `read_delim()` for more control.</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Note that the `c()` function is defining</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co"># what NA values look like for the import function.</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="fu">View</span>(amatus)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="co"># View the data.</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>amatusclean <span class="ot">&lt;-</span> amatus[<span class="sc">!</span><span class="fu">is.na</span>(amatus<span class="sc">$</span>sum_arith_perf), ]</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Removing the individuals who did not complete the performance test in order</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="co"># as instructed. Using simple subsetting.</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>amatusclean <span class="ot">&lt;-</span> amatusclean[<span class="sc">!</span>(amatusclean<span class="sc">$</span>sample <span class="sc">%in%</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">c</span>(<span class="st">"german_teachers"</span>, <span class="st">"belgian_teachers"</span>)), ]</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Removing the 2 teacher samples from the dataset, as I am only # interested in</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="co"># the student sample.</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a><span class="fu">View</span>(amatusclean)</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Data cleaning (using tidyverse syntax this time around):</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>amatusclean <span class="ot">&lt;-</span> amatusclean <span class="sc">%&gt;%</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">filter</span>(<span class="sc">!</span><span class="fu">is.na</span>(sum_arith_perf)) <span class="sc">%&gt;%</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">mutate</span>(<span class="fu">across</span>(<span class="fu">c</span>(</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>    sex, age_range, breaks, honesty, native_speaker, noise</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>  ), as.factor))</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a><span class="co"># `filter()` removes the individuals who did not complete the performance test</span></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a><span class="co"># in order as instructed. `mutate()` applies the `as.factor()` function to all</span></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a><span class="co"># categorical/factor variables.</span></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a><span class="co"># I also want to reverse the order of the math grade variable so a 1 is the</span></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a><span class="co"># worst grade for simplicity to match every other variable, where lower</span></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a><span class="co"># values correspond to having "less" of or being "worse" at the concept.</span></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>amatusclean <span class="ot">&lt;-</span> amatusclean <span class="sc">%&gt;%</span></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">mutate</span>(<span class="at">math_grade =</span> <span class="fu">recode</span>(math_grade,</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>    <span class="st">`</span><span class="at">1</span><span class="st">`</span> <span class="ot">=</span> <span class="dv">6</span>,</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>    <span class="st">`</span><span class="at">2</span><span class="st">`</span> <span class="ot">=</span> <span class="dv">5</span>,</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>    <span class="st">`</span><span class="at">3</span><span class="st">`</span> <span class="ot">=</span> <span class="dv">4</span>,</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>    <span class="st">`</span><span class="at">4</span><span class="st">`</span> <span class="ot">=</span> <span class="dv">3</span>,</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>    <span class="st">`</span><span class="at">5</span><span class="st">`</span> <span class="ot">=</span> <span class="dv">2</span>,</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>    <span class="st">`</span><span class="at">6</span><span class="st">`</span> <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>  ))</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a><span class="fu">View</span>(amatusclean)</span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a><span class="fu">invisible</span>(<span class="fu">colSums</span>(<span class="fu">is.na</span>(amatusclean)))</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a><span class="co"># It appears there is only one NA value in the variables of interest from the</span></span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a><span class="co"># dataset - in the math load variable. Remove the `invisible()` function from</span></span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a><span class="co"># the line above to print the results and see for yourself. For simplicity,</span></span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a><span class="co"># I will just drop this value. There are 734 data points now.</span></span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a>amatusclean <span class="ot">&lt;-</span> <span class="fu">subset</span>(amatusclean, <span class="sc">!</span><span class="fu">is.na</span>(math_load))</span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a><span class="fu">nrow</span>(amatusclean)</span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 734</span></span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's also isolate the variables in objects now for easier use later on.</span></span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>numeric_predictors <span class="ot">&lt;-</span> <span class="fu">c</span>(</span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a>  <span class="st">"score_BFI_N"</span>, <span class="st">"score_AMAS_total"</span>, <span class="st">"age"</span>, <span class="st">"score_GAD"</span>,</span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a>  <span class="st">"score_PISA_ME"</span>, <span class="st">"score_STAI_state_short"</span>,</span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a>  <span class="st">"score_TAI_short"</span>, <span class="st">"score_SDQ_L"</span>, <span class="st">"score_SDQ_M"</span>,</span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a>  <span class="st">"total_time_minutes"</span>, <span class="st">"math_grade"</span>, <span class="st">"math_load"</span>,</span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a>  <span class="st">"math_inf_program_choice"</span>, <span class="st">"liking_math"</span>,</span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a>  <span class="st">"liking_science"</span>, <span class="st">"liking_humanities"</span>,</span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a>  <span class="st">"persistence_math"</span>, <span class="st">"persistence_science"</span>,</span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a>  <span class="st">"persistence_humanities"</span></span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a>categorical_variable <span class="ot">&lt;-</span> <span class="st">"sex"</span></span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a>response_variable <span class="ot">&lt;-</span> <span class="st">"sum_arith_perf"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, I’ll just take a look at a simple histogram of the distribution of the dependent variable, arithmetic math performance.</p>
<section id="dependent-variable-distribution" class="level3" data-number="1.3.1">
<h3 data-number="1.3.1" class="anchored" data-anchor-id="dependent-variable-distribution"><span class="header-section-number">1.3.1</span> Dependent Variable Distribution</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>  amatusclean,</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">x =</span> sum_arith_perf)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>) <span class="sc">+</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">fill =</span> <span class="st">"lightblue"</span>, <span class="at">alpha =</span> <span class="fl">0.7</span>) <span class="sc">+</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co"># I set up the theme for all plots in hidden code, so I</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co"># won't include it again after this.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-arith-perf" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-arith-perf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="RandomForest_files/figure-html/fig-arith-perf-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-arith-perf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Distribution of response variable: Arithmetic Math Performance
</figcaption>
</figure>
</div>
</div>
</div>
<p>This distribution is clearly skewed right (<a href="#fig-arith-perf" class="quarto-xref">Figure&nbsp;1</a>). This indicates that most people scored around the “low” end of all possible values.</p>
<p>Let’s look at the distributions of our predictor variables.</p>
</section>
<section id="predictor-distributions" class="level3" data-number="1.3.2">
<h3 data-number="1.3.2" class="anchored" data-anchor-id="predictor-distributions"><span class="header-section-number">1.3.2</span> Predictor Distributions</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># The `facet_wrap()` function from `ggplot2` will create a single-image for the</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co"># multiple graphs I create below. However, it only works with "long" data.</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Thus, we use the `pivot_longer()` function from the `dplyr` package to switch</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># the data from wide to long format first below.</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>long_data <span class="ot">&lt;-</span> amatusclean <span class="sc">%&gt;%</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>  tidyr<span class="sc">::</span><span class="fu">pivot_longer</span>(</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">cols =</span> dplyr<span class="sc">::</span><span class="fu">all_of</span>(</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>      numeric_predictors</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    ), <span class="at">names_to =</span> <span class="st">"Predictor"</span>, <span class="at">values_to =</span> <span class="st">"Value"</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Now, we create all 19 graphs using one function.</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>facet_distribution_plots <span class="ot">&lt;-</span> ggplot2<span class="sc">::</span><span class="fu">ggplot</span>(</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>  <span class="co"># All subsequent functions here are from `ggplot2`, as well.</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>  long_data, <span class="fu">aes</span>(<span class="at">x =</span> Value)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>) <span class="sc">+</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">fill =</span> <span class="st">"lightblue"</span>, <span class="at">alpha =</span> <span class="fl">0.7</span>) <span class="sc">+</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>Predictor,</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">ncol =</span> <span class="dv">4</span>, <span class="at">nrow =</span> <span class="dv">5</span>, <span class="at">scales =</span> <span class="st">"free"</span>,</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># I specify rows and columns for 19-20 graphs. Scales vary per predictor.</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    <span class="at">strip.position =</span> <span class="st">"bottom"</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span> <span class="co"># Facet labels on bottom of graphs.</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>    <span class="at">strip.text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">12</span>, <span class="at">face =</span> <span class="st">"bold"</span>), <span class="co"># text settings.</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>    <span class="at">strip.placement =</span> <span class="st">"outside"</span>, <span class="co"># moves labels below x-axes.</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>    <span class="at">strip.background =</span> <span class="fu">element_blank</span>() <span class="co"># removes ugly gray background</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>facet_distribution_plots</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-predictors" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="h">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-predictors-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="RandomForest_files/figure-html/fig-predictors-1.png" class="img-fluid figure-img" data-fig-pos="h" width="1344">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-predictors-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Distribution of predictors
</figcaption>
</figure>
</div>
</div>
</div>
<p>Some predictors are approximately normal, others clearly aren’t. Luckily, distribution of predictors doesn’t matter that much for random forest models. For fun, we’ll also look at some continuous predictors graphed against our DV.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Again, we create all 19 graphs using one function.</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>facet_bivariate_plots <span class="ot">&lt;-</span> ggplot2<span class="sc">::</span><span class="fu">ggplot</span>(</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># All subsequent functions here are from `ggplot2`, as well.</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>  long_data, <span class="fu">aes</span>(<span class="at">x =</span> Value, <span class="at">y =</span> .data[[response_variable]])</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>) <span class="sc">+</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># This time, we graph predictors against the outcome variable.</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.7</span>, <span class="at">color =</span> <span class="st">"lightblue"</span>) <span class="sc">+</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># This creates a linear trend line for visualization.</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"loess"</span>, <span class="at">color =</span> <span class="st">"blue"</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># This creates a nonlinear trend line for visualization.</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    <span class="sc">~</span>Predictor,</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    <span class="at">ncol =</span> <span class="dv">4</span>, <span class="at">nrow =</span> <span class="dv">5</span>, <span class="at">scales =</span> <span class="st">"free"</span>,</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    <span class="at">strip.position =</span> <span class="st">"bottom"</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">strip.text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">12</span>, <span class="at">face =</span> <span class="st">"bold"</span>),</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">strip.placement =</span> <span class="st">"outside"</span>,</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>    <span class="at">strip.background =</span> <span class="fu">element_blank</span>()</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>facet_bivariate_plots</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-preds_DV" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="p">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-preds_DV-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="RandomForest_files/figure-html/fig-preds_DV-1.png" class="img-fluid figure-img" data-fig-pos="p" width="1344">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-preds_DV-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Distribution of predictors against the DV (sum_arith_perf)
</figcaption>
</figure>
</div>
</div>
</div>
<p>The graphs created in the code above are shown on the next page, Page 11, in <a href="#fig-preds_DV" class="quarto-xref">Figure&nbsp;3</a>.</p>
<p>Like much of psychological data, most of our predictors (and our DV) are not truly continuous. Our DV for example, <code>sum_arith_perf</code>, takes a fairly wide range of values, but it is clear from its distribution that is truly ordinal because it is measured on a scale. Furthermore, we are currently treating a few ordinal variables with few levels, such as the <code>liking</code> and <code>persistence</code> variables, as continuous, assuming there is a linear relationship between each level. You would need to check this in a real data project before proceeding. Otherwise, you may turn the ordinal variables into categorical variables (or use different models), though you would lose some information.</p>
<p><a href="https://imgur.com/a/yX6rj7t">Click here for a link to an image of the correlation table</a> provided by the original authors of the dataset, which contains the DV and most of the predictors I am evaluating. Many of the continuous predictors depicted are decently intercorrelated, which is fine because random forest handles this better than many other models (and there aren’t any correlations over .8 or .9, which would sound alarm bells). Furthermore, correlations go in the “expected” directions - e.g., arithmetic performance correlates positively with liking math and math self-efficacy correlates negatively with all anxiety measures.</p>
<p>This exploratory data analysis is light, but that is only because I want to focus on the model building and tuning. For a real project, graph all of your variables, transform predictor or outcome variables if needed, and use alternatives to omitting missing data in order to keep statistical power as high as possible (such as multiple imputation).</p>
</section>
</section>
<section id="standardization-and-trainingtest-sets" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="standardization-and-trainingtest-sets"><span class="header-section-number">1.4</span> Standardization and Training/Test Sets</h2>
<p>Because they simply partition data to make predictions at thresholds, random forest models do not <em>require</em> you to standardize your variables. However, it has been shown by Strobl and colleagues <span class="citation" data-cites="strobl_bias_2007">(<a href="#ref-strobl_bias_2007" role="doc-biblioref">2007</a>)</span> that random forest variable importance measures are biased by the range of values a variable can take, tending to produce higher importance scores for variables with a greater range of possible values, even when both are uninformative.</p>
<p>Permutation importance is unbiased with regard to variable importance regarding the average value, although permutation importance can display variance of variable importance for variables with many categories such that this measure can be unreliable in single trials. Importance measures are still commonly utilized in random forest variable selection with solid results <span class="citation" data-cites="speiser_comparison_2019">(<a href="#ref-speiser_comparison_2019" role="doc-biblioref">Speiser et al., 2019</a>)</span> - we will review automated methods as well as alternative variable importance metrics in my forthcoming <code>Methods for Variable Selection</code> addendum. Furthermore, the data contains categorical and ordinal data that I do not want to standardize if I do not need to, as this could bias the underlying variance of the noncontinuous data.</p>
<p>I will now create a training set and a test set for model validation. Although validation methods like repeated cross-validation account for the need of a “validation set”, I want to have train/test data for the simple purpose of testing built models on “unseen data” to evaluate how well it generalizes to new data. This also comes with concerns (such as the fact that the dataset the data is trained on gets smaller), but this should still help me work out which of the models I construct are truly the best at prediction. (See the <code>Predictions</code> addendum for the results).</p>
<section id="setting-up-the-traintest-split" class="level3" data-number="1.4.1">
<h3 data-number="1.4.1" class="anchored" data-anchor-id="setting-up-the-traintest-split"><span class="header-section-number">1.4.1</span> Setting up the Train/Test Split</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Loading required package: lattice</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Attaching package: 'caret'</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="do">## The following object is masked from 'package:purrr':</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="do">##     lift</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co"># needed first for the inTrain function below.</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">39</span>)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="co"># I'm going to set the random seed BEFORE every</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="co"># random generation, including model fitting, for clarity.</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>inTrain <span class="ot">&lt;-</span> caret<span class="sc">::</span><span class="fu">createDataPartition</span>(amatusclean<span class="sc">$</span>sum_arith_perf,</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">p =</span> <span class="fl">0.7</span>, <span class="at">list =</span> <span class="cn">FALSE</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="co"># 0.7 will sort 70% of the records into training set, 30% into the test</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="co"># set. I selected 0.7 to have a decent number of N in the test set.</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Create training vs test data using subsetting.</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>training <span class="ot">&lt;-</span> amatusclean[inTrain, ]</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>test <span class="ot">&lt;-</span> amatusclean[<span class="sc">-</span>inTrain, ]</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a><span class="fu">View</span>(training)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a><span class="fu">View</span>(test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="a-note-on-featurevariable-selection" class="level3" data-number="1.4.2">
<h3 data-number="1.4.2" class="anchored" data-anchor-id="a-note-on-featurevariable-selection"><span class="header-section-number">1.4.2</span> A Note on Feature/Variable Selection</h3>
<p>For the purposes of this particular document, I retain all 20 predictor variables to build models and pretend all of them are theorized to have some level of predictive power. However, this is simply for demonstrative purposes and to compare the models I build with all variables later on with better, more parsimonious models. In reality, <em>this</em> is the point where you would want to perform variable selection and remove highly correlated/irrelevant predictors from the model to improve performance. I perform this process in the forthcoming <code>Methods for Variable Selection</code> addendum. If you suspect your dataset contains irrelevant/highly correlated features, I recommend waiting for the <code>Methods for Variable Selection</code> addendum. If you would simply like to build and tune a random forest model, proceed.</p>
</section>
</section>
<section id="tuning-your-hyperparameters-for-your-random-forest-model" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="tuning-your-hyperparameters-for-your-random-forest-model"><span class="header-section-number">1.5</span> Tuning your Hyperparameters for your Random Forest Model</h2>
<p>Hyperparameters are simply model parameters that, rather than being calculated by the model, you set prior to building the model. In most cases, random forest works pretty well with the “default values” in software packages <span class="citation" data-cites="probst_hyperparameters_2019">(<a href="#ref-probst_hyperparameters_2019" role="doc-biblioref">Probst et al., 2019</a>)</span>, but tuning can improve performance above and beyond that.</p>
<p>Below, I describe four hyperparameters - <code>mtry</code>, node size, number of trees and sample size. Most of what I summarize below is explained in greater detail in Probst, Wright and Boulesteix <span class="citation" data-cites="probst_hyperparameters_2019">(<a href="#ref-probst_hyperparameters_2019" role="doc-biblioref">2019</a>)</span>, which is an incredibly helpful resource for tuning random forest hyperparameters.</p>
<ol type="1">
<li><p>The hyperparameter <code>mtry</code> defines the number of randomly selected predictors to consider splitting at each node of the decision tree, ranging from 1 to <em>p</em>, the number of predictor variables. If you expect many relevant predictors with varying effect sizes, mtry should likely be set low since less influential variables will still be chosen and could provide performance gains. If you expect only a few relevant variables out of many (possibly hundreds), mtry should likely be set high so the algorithm can better find relevant variables <span class="citation" data-cites="probst_hyperparameters_2019">(<a href="#ref-probst_hyperparameters_2019" role="doc-biblioref">Probst et al., 2019</a>)</span>.</p>
<ol type="i">
<li>The default value for regression in many packages is <em>p</em>/3, which has been shown to perform well enough. However, Genuer and colleagues <span class="citation" data-cites="genuer_random_2008">(<a href="#ref-genuer_random_2008" role="doc-biblioref">2008</a>)</span> have shown that in low dimensional regression problems (such as the one at hand), the square root of <em>p</em> performs better, which would be about 4.472 for my analysis. I will tune the <code>mtry</code> value, but will also create models with “default” values for hyperparameters for comparison.</li>
</ol></li>
<li><p>The hyperparameter <code>nodesize</code> describes the minimum number of data points in a leaf/terminal node. It sets the “depth” of trees, meaning that a lower setting leads to more splits being performed until reaching the terminal nodes, which contain fewer observations. The default value is 5 for regression in most statistical packages. Increasing the number of noise variables has been shown to lead to higher optimal node size <span class="citation" data-cites="segal_machine_2003">(<a href="#ref-segal_machine_2003" role="doc-biblioref">Segal, 2003</a>)</span> and a higher value is computationally preferable. It has also been shown that higher <code>nodesize</code> values lead to overfitting on probability estimation <span class="citation" data-cites="barrenada_understanding_2024">(<a href="#ref-barrenada_understanding_2024" role="doc-biblioref">Barreñada et al., 2024</a>)</span>, though this only applies to probability estimation. Again, though I will tune these hyperparameters, I will build other models with default values.</p></li>
<li><p>The hyperparameter “number of trees” (or <code>ntree</code>) refers to the number of individual trees needed to optimize the results of the random forest. The default value is often 500 in statistical packages. This parameter is not really “tunable”, as more trees is always better for unbiased error terms. However, more trees directly leads to longer computation time. I will use large values for ntree for the purposes of this document, such as 8000. A number even around a few thousand is likely sufficient for my purposes, though a greater <code>ntree</code> can only improve performance (though minimally after a given point).</p></li>
<li><p>The hyperparameter “sample size” decides how many observations are sampled when training each decision tree. The default value is usually <em>n</em>. If you have a lower value than that, whether you are sampling with or without replacement also becomes a key matter (discussed later). Martinez-Munoz and Suarez <span class="citation" data-cites="martinez-munoz_outbag_2010">(<a href="#ref-martinez-munoz_outbag_2010" role="doc-biblioref">2010</a>)</span> observed that in most datasets, a sample size lower than the standard option tended to produce better performance.</p></li>
</ol>
<p>A side note: There are several different validation strategies for a given algorithm that you can use in <code>caret</code> or <code>ranger</code> or whichever package you use, including bootstrapping, cross-validation, and repeated cross-validation. For random forest, you can also use out-of-bag predictions, which evaluates model performance on the data not selected when the data was bootstrapped as part of the bootstrap/aggregation process. Repeated cross-validation is often optimal when computationally appropriate. However, I will use out-of-bag error to save time and because it is appropriate and reliable for many datasets, as results appear to only be biased in extreme situations (e.g., <em>n</em> &lt; 20 with hundreds/thousands of predictors; see <span class="citation" data-cites="janitza_overestimation_2018">(<a href="#ref-janitza_overestimation_2018" role="doc-biblioref">Janitza &amp; Hornung, 2018</a>)</span>.</p>
</section>
<section id="tuning-packages-in-r" class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="tuning-packages-in-r"><span class="header-section-number">1.6</span> Tuning Packages in R</h2>
<p>Machine learning packages, such as <code>caret</code>, contain plenty of functions to help tune models like random forest. For example, the <code>trainControl()</code> function is used to define the strategy used, such as repeated cross-validation. You also define if you’d like to search from a grid of predetermined values (<code>search = "grid"</code>) or from a random set (<code>search = "random"</code>) However, these methods of search can be quite time-consuming, not to mention rather basic (though I recommend reading more about this <a href="https://www.rdocumentation.org/packages/caret/versions/6.0-92/topics/trainControl">at the link here</a>.</p>
<p>Instead, I am going to use a slightly more advanced method (sequential model-based optimization, or SMBO) implemented in a simple package, <code>tuneRanger</code>, for tuning. Probst and colleagues <span class="citation" data-cites="probst_hyperparameters_2019">(<a href="#ref-probst_hyperparameters_2019" role="doc-biblioref">2019</a>)</span>, in their introduction to the package, state that SMBO is “a very successful tuning strategy that iteratively tries to find the best hyperparameter settings based on evaluations of hyperparameters that were done beforehand”. More detail on the process can also be found in papers by Bischl and colleagues <span class="citation" data-cites="bischl_mlrmbo_2018">(<a href="#ref-bischl_mlrmbo_2018" role="doc-biblioref">2018</a>)</span> and Hutter and colleagues <span class="citation" data-cites="coello_sequential_2011">(<a href="#ref-coello_sequential_2011" role="doc-biblioref">2011</a>)</span>. The implementation of SMBO in tuneRanger is computationally quick, simultaneously tunes 3 “tunable” parameters, and requires relatively few lines of code.</p>
<p>Please note that tuneRanger uses bootstrap without replacement in the aggregation of the trees. Choosing to sample with or without replacement is an argument in most functions that build a random forest model. This only matters if your sample size hyperparameter is lower than <em>n</em>. Sampling with vs.&nbsp;without replacement does not change the model much particularly when the sample size parameter is “optimal” <span class="citation" data-cites="martinez-munoz_outbag_2010">(<a href="#ref-martinez-munoz_outbag_2010" role="doc-biblioref">Martínez-Muñoz &amp; Suárez, 2010</a>)</span>, but in situations with categorical variables with varying number of categories, a small variable selection bias may be present. I will stick to without replacement because <code>tuneRanger</code> does.</p>
<p>(I am optimizing hyperparameters to have the model with the least mean squared error, given that I am conducting regression.)</p>
<p>Let’s go through the code to tune these hyperparameters now. Notice that you may need to install and load the <code>mlr</code> package, or “Machine Learning in R”, for initial setup of the model (or the “regression task” in <code>mlr</code> notation). Note that <code>mlr</code> is depreciated and <code>mlr3</code> is the new and improved package, but I will stick to <code>mlr</code> in this demonstration since the tuneRanger package was written for the use of <code>mlr</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ranger)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tuneRanger)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mlr)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Here's where the helpful defined objects I set earlier come in handy.</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Subset the original training data using dplyr</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>training_subset <span class="ot">&lt;-</span> training <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="fu">all_of</span>(</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">c</span>(response_variable, numeric_predictors, categorical_variable)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>))</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure it's a dataframe</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>training_subset <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(training_subset)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="co"># use the `makeRegrTask()` function in mlr to create the "task"</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>tuning_task1 <span class="ot">&lt;-</span> mlr<span class="sc">::</span><span class="fu">makeRegrTask</span>(</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> training_subset,</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>  <span class="at">target =</span> <span class="st">"sum_arith_perf"</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate how long tuning will take with default parameters</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>tuneRanger<span class="sc">::</span><span class="fu">estimateTimeTuneRanger</span>(tuning_task1)</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a><span class="do">## Approximated time for tuning: 1M 18S</span></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a><span class="co"># This first run will involve a large number of trees and more iterations</span></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a><span class="co"># than default since I have leftover computing power. Adjust the values</span></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a><span class="co"># as needed.</span></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">40</span>)</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>rf_firsttuneranger <span class="ot">&lt;-</span> tuneRanger<span class="sc">::</span><span class="fu">tuneRanger</span>(</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>  tuning_task1,</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>  <span class="at">num.trees =</span> <span class="dv">8000</span>, <span class="at">num.threads =</span> <span class="dv">8</span>, <span class="at">iters =</span> <span class="dv">100</span>,</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>  <span class="at">iters.warmup =</span> <span class="dv">50</span></span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a><span class="co"># The `measure` argument defines the measures to be optimized. Since we have</span></span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a><span class="co"># not specified a value, it will use the default for regression, mean squared</span></span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a><span class="co"># error. `num.trees` means number of trees, `num.threads` is for parallel</span></span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a><span class="co"># processing (optional), `iters` is the number of iterations the SMBO will go</span></span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a><span class="co"># through (default value is 70) and `iters.warmup` is the number of warmup</span></span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a><span class="co"># steps for the initial design (default value of 30). Lastly, `tune.parameters`</span></span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a><span class="co"># can manually specify the list of tuned parameters (`mtry`, node size,</span></span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a><span class="co"># and sample size by default).</span></span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>rf_firsttuneranger<span class="sc">$</span>model</span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a><span class="do">## Model for learner.id=regr.ranger; learner.class=regr.ranger</span></span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a><span class="do">## Trained on: task.id = training_subset; obs = 515; features = 20</span></span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a><span class="do">## Hyperparameters: num.threads=8,verbose=FALSE,respect.unordered.factors=order,mtry=20,min.node.size=38,sample.fraction=0.267,num.trees=8e+03,replace=FALSE</span></span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a><span class="co"># This second run has fewer trees.  Perhaps realistic in terms of computing</span></span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a><span class="co"># power for most and will be slightly different than the previous run.</span></span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">41</span>)</span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a>rf_secondtuneranger <span class="ot">&lt;-</span> tuneRanger<span class="sc">::</span><span class="fu">tuneRanger</span>(</span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a>  tuning_task1,</span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a>  <span class="at">num.trees =</span> <span class="dv">6000</span>, <span class="at">num.threads =</span> <span class="dv">8</span>, <span class="at">iters =</span> <span class="dv">70</span>,</span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a>  <span class="at">iters.warmup =</span> <span class="dv">30</span></span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a>rf_secondtuneranger<span class="sc">$</span>model</span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a><span class="do">## Model for learner.id=regr.ranger; learner.class=regr.ranger</span></span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a><span class="do">## Trained on: task.id = training_subset; obs = 515; features = 20</span></span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a><span class="do">## Hyperparameters: num.threads=8,verbose=FALSE,respect.unordered.factors=order,mtry=16,min.node.size=38,sample.fraction=0.311,num.trees=6e+03,replace=FALSE</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="training-the-models" class="level2" data-number="1.7">
<h2 data-number="1.7" class="anchored" data-anchor-id="training-the-models"><span class="header-section-number">1.7</span> Training the Models</h2>
<p>Now that I have pulled some hopefully optimized settings for the parameters, I demonstrate building two random forest models with these parameters. I will use the <code>ranger</code> package <span class="citation" data-cites="wright_ranger_2017">(<a href="#ref-wright_ranger_2017" role="doc-biblioref">Wright &amp; Ziegler, 2017</a>)</span>, which is essentially a speedier implementation of the older <code>randomForest</code> package, which is what <code>caret</code> uses to create RF models. It is also a bit simpler to specify hyperparameters like minimum node size and sample fraction. However, if you have used <code>caret</code> or <code>mlr</code> to build machine learning models in the past, those are excellent options too.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the actual Random Forest model using ranger</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>rf_rangertunedmodel <span class="ot">&lt;-</span> ranger<span class="sc">::</span><span class="fu">ranger</span>(</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">formula =</span> sum_arith_perf <span class="sc">~</span> ., <span class="co"># Target variable, all features</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> training_subset, <span class="co"># Subsetted Training dataset</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">num.trees =</span> <span class="dv">8000</span>, <span class="co"># Number of trees. 8000 for strong performance.</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">mtry =</span> <span class="dv">20</span>, <span class="co"># Number of variables sampled at each split</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">min.node.size =</span> <span class="dv">38</span>, <span class="co"># Minimum node size.</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">sample.fraction =</span> <span class="fl">0.267</span>, <span class="co"># Fraction of n used for training model</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">importance =</span> <span class="st">"permutation"</span>, <span class="co"># Permutation feature importance.</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">replace =</span> <span class="cn">FALSE</span>, <span class="co"># Bootstrapping without replacement.</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">splitrule =</span> <span class="st">"variance"</span>, <span class="co"># standard splitting rule for random forest.</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>  <span class="at">write.forest =</span> <span class="cn">TRUE</span>, <span class="co"># saves individual trees just in case you want them.</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>  <span class="at">verbose =</span> <span class="cn">TRUE</span> <span class="co"># Show training in a bit more detail</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(rf_rangertunedmodel)</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a><span class="do">## Ranger result</span></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a><span class="do">##  ranger::ranger(formula = sum_arith_perf ~ ., data = training_subset,      num.trees = 8000, mtry = 20, min.node.size = 38, sample.fraction = 0.267,      importance = "permutation", replace = FALSE, splitrule = "variance",      write.forest = TRUE, verbose = TRUE) </span></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a><span class="do">## Type:                             Regression </span></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a><span class="do">## Number of trees:                  8000 </span></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a><span class="do">## Sample size:                      515 </span></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a><span class="do">## Number of independent variables:  20 </span></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a><span class="do">## Mtry:                             20 </span></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a><span class="do">## Target node size:                 38 </span></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a><span class="do">## Variable importance mode:         permutation </span></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a><span class="do">## Splitrule:                        variance </span></span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a><span class="do">## OOB prediction error (MSE):       37.27925 </span></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a><span class="do">## R squared (OOB):                  0.1441785</span></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>rf_rangertunedmodel2 <span class="ot">&lt;-</span> ranger<span class="sc">::</span><span class="fu">ranger</span>(</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>  <span class="at">formula =</span> sum_arith_perf <span class="sc">~</span> .,</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> training_subset,</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>  <span class="at">num.trees =</span> <span class="dv">8000</span>,</span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>  <span class="at">mtry =</span> <span class="dv">16</span>,</span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>  <span class="at">min.node.size =</span> <span class="dv">38</span>,</span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>  <span class="at">sample.fraction =</span> .<span class="dv">311</span>,</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>  <span class="at">importance =</span> <span class="st">"permutation"</span>,</span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>  <span class="at">replace =</span> <span class="cn">FALSE</span>,</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>  <span class="at">splitrule =</span> <span class="st">"variance"</span>,</span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>  <span class="at">verbose =</span> <span class="cn">TRUE</span></span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(rf_rangertunedmodel2)</span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a><span class="do">## Ranger result</span></span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a><span class="do">##  ranger::ranger(formula = sum_arith_perf ~ ., data = training_subset,      num.trees = 8000, mtry = 16, min.node.size = 38, sample.fraction = 0.311,      importance = "permutation", replace = FALSE, splitrule = "variance",      verbose = TRUE) </span></span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a><span class="do">## Type:                             Regression </span></span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a><span class="do">## Number of trees:                  8000 </span></span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a><span class="do">## Sample size:                      515 </span></span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a><span class="do">## Number of independent variables:  20 </span></span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a><span class="do">## Mtry:                             16 </span></span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a><span class="do">## Target node size:                 38 </span></span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true" tabindex="-1"></a><span class="do">## Variable importance mode:         permutation </span></span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true" tabindex="-1"></a><span class="do">## Splitrule:                        variance </span></span>
<span id="cb8-63"><a href="#cb8-63" aria-hidden="true" tabindex="-1"></a><span class="do">## OOB prediction error (MSE):       37.24817 </span></span>
<span id="cb8-64"><a href="#cb8-64" aria-hidden="true" tabindex="-1"></a><span class="do">## R squared (OOB):                  0.144892</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Fairly similar results from the two models, which is expected given that one is likely just a bit more refined given extra iterations and trees. The tuned value for node size is pretty high, which means the trees are not fitting too deeply. Similarly, the <code>mtry</code> values are relatively high, indicating that my models seem to fit better when more variables are being sampled at each split. High values of <code>mtry</code> appear to be better for datasets containing a few relevant variables out of many <span class="citation" data-cites="goldstein_random_2011">(<a href="#ref-goldstein_random_2011" role="doc-biblioref">Goldstein et al., 2011</a>)</span>, which may be the case here. Lastly, a lower sample size means the individual trees are more diverse and the out of the bag predictions should be solid in terms of estimating the optimal parameter.</p>
<p>You will also notice that the out-of-bag r-squared for each model is not great - 0.146 and 0.145, respectively. This is likely for a few reasons - first, there are undoubtedly some noise variables in the predictors (possibly even constituting a majority). This means the model will train in part on random noise and a more parsimonious model would likely produce more accurate predictions, especially with tuned parameters (more on that in my <code>Methods for Variable Selection</code> addendum). Furthermore, this actually probably isn’t actually too far from a usable R-squared in psychology research. Models of human behavior are often far from exact - for example, Wallert and colleagues <span class="citation" data-cites="wallert_predicting_2018">(<a href="#ref-wallert_predicting_2018" role="doc-biblioref">2018</a>)</span> used a random forest classifier model to predict adherence to internet cognitive behavioral therapy treatments for patients who suffered myocardial infarction. Their finalized model (after variable selection with the LASSO algorithm, which I go over in my <code>LASSO</code> addendum) reported an accuracy of 0.64. Such a low accuracy value might be surprising, but numbers in this range are usually decent for psychological research, given that human behavior is far more difficult to predict accurately than say, the genes that predict future occurrence of a disease.</p>
<p>Next, I’ll take a look at random forest variable importance scores, which are useful for identifying important predictors in a given model. Recall that I am using permutation importance. However, keep in mind that such scores are relative to the model and can be wildly different from other measures of “importance” to predicting a DV (like zero-order correlation, for example). The scores I evaluate are unscaled.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Pulling importance scores for model 1, to be called `importance_df`</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>importance_scores <span class="ot">&lt;-</span> rf_rangertunedmodel<span class="sc">$</span>variable.importance</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>importance_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">Variable =</span> <span class="fu">names</span>(importance_scores), <span class="at">Importance =</span> importance_scores</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>importance_df <span class="ot">&lt;-</span> importance_df[<span class="fu">order</span>(<span class="sc">-</span>importance_df<span class="sc">$</span>Importance), ]</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(importance_df)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="do">##                                        Variable   Importance</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="do">## score_PISA_ME                     score_PISA_ME  3.589547753</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="do">## score_SDQ_M                         score_SDQ_M  1.941928038</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="do">## sex                                         sex  1.475072763</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="do">## liking_math                         liking_math  1.444666776</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="do">## score_AMAS_total               score_AMAS_total  1.010055063</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="do">## persistence_math               persistence_math  0.541036820</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="do">## total_time_minutes           total_time_minutes  0.501312651</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="do">## math_grade                           math_grade  0.491543991</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="do">## math_inf_program_choice math_inf_program_choice  0.221200686</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="do">## liking_science                   liking_science  0.174240678</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a><span class="do">## score_STAI_state_short   score_STAI_state_short  0.152486418</span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a><span class="do">## persistence_science         persistence_science  0.092768977</span></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a><span class="do">## score_BFI_N                         score_BFI_N  0.065190642</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a><span class="do">## math_load                             math_load  0.063423876</span></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a><span class="do">## score_TAI_short                 score_TAI_short  0.051604622</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a><span class="do">## score_GAD                             score_GAD  0.032831793</span></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a><span class="do">## age                                         age  0.024770114</span></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a><span class="do">## persistence_humanities   persistence_humanities  0.022385539</span></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a><span class="do">## liking_humanities             liking_humanities  0.021394592</span></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a><span class="do">## score_SDQ_L                         score_SDQ_L -0.002609613</span></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Pulling importance scores for model 2, to be called `importance_df2`</span></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>importance_scores2 <span class="ot">&lt;-</span> rf_rangertunedmodel2<span class="sc">$</span>variable.importance</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>importance_df2 <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>  <span class="at">Variable =</span> <span class="fu">names</span>(importance_scores2), <span class="at">Importance =</span> importance_scores2</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>importance_df2 <span class="ot">&lt;-</span> importance_df2[<span class="fu">order</span>(<span class="sc">-</span>importance_df2<span class="sc">$</span>Importance), ]</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(importance_df2)</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a><span class="do">##                                        Variable  Importance</span></span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a><span class="do">## score_PISA_ME                     score_PISA_ME  3.48700624</span></span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a><span class="do">## score_SDQ_M                         score_SDQ_M  1.92734268</span></span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a><span class="do">## sex                                         sex  1.57027956</span></span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a><span class="do">## liking_math                         liking_math  1.53459657</span></span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a><span class="do">## score_AMAS_total               score_AMAS_total  0.94575458</span></span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a><span class="do">## persistence_math               persistence_math  0.56207837</span></span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a><span class="do">## math_grade                           math_grade  0.51245712</span></span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a><span class="do">## total_time_minutes           total_time_minutes  0.50277400</span></span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a><span class="do">## math_inf_program_choice math_inf_program_choice  0.21250082</span></span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a><span class="do">## score_STAI_state_short   score_STAI_state_short  0.18198110</span></span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a><span class="do">## liking_science                   liking_science  0.13511296</span></span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a><span class="do">## score_TAI_short                 score_TAI_short  0.09819293</span></span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a><span class="do">## score_BFI_N                         score_BFI_N  0.08915444</span></span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a><span class="do">## math_load                             math_load  0.07695278</span></span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a><span class="do">## persistence_science         persistence_science  0.04826767</span></span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a><span class="do">## score_GAD                             score_GAD  0.04481476</span></span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a><span class="do">## liking_humanities             liking_humanities  0.02784193</span></span>
<span id="cb9-55"><a href="#cb9-55" aria-hidden="true" tabindex="-1"></a><span class="do">## persistence_humanities   persistence_humanities  0.01774066</span></span>
<span id="cb9-56"><a href="#cb9-56" aria-hidden="true" tabindex="-1"></a><span class="do">## age                                         age  0.00722506</span></span>
<span id="cb9-57"><a href="#cb9-57" aria-hidden="true" tabindex="-1"></a><span class="do">## score_SDQ_L                         score_SDQ_L -0.01572412</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>It appears that <code>score_PISA_ME</code> is far and away the most important variable in each model. Besides the dropoff after <code>score_PISA_me</code>, there is also a major dropoff around the 5th/4th variable in the models respectively. Recall that a lot of the predictors are intercorrelated - I may be inclined to think here, for example, “is math persistence giving me a lot of unique information that something like math self-efficacy is not?”</p>
<p>The variable importance metrics suggest to me that the dataset likely has somewhere around 4-5 variables that actually truly contribute to prediction and are not mostly noise variables, perhaps even fewer being truly strong predictors of the dependent variable. However, this measure of variable importance is relative, and simply selecting the top X many predictors is a simplistic and suboptimal method of variable selection. This is why I will conduct and compare several methods of feature selection on this dataset in my <code>Methods for Variable Selection</code> addendum, then retune the hyperparameters and refit random forest models using the selected feature list to test their predictions in the <code>Predictions</code> addendum.</p>
<p>Lastly, as additional comparison models, I will create two 20-variable random forest models with “default” values for hyperparameters. In other words, minimum node size will be 5 (default for regression), <code>mtry</code> will equal <em>p</em>/3 as default for regression (as well as the square root of <em>p</em>), and sample size will be <em>n</em>. These models will just serve as a point of comparison for showing how tuning model parameters can lead to better prediction.</p>
<p>I hope this document has been helpful with regards to better understanding the concept and implementation of building random forest models and tuning their hyperparameters. Please read my <code>Methods for Variable Selection</code> and <code>Predictions</code> addenda to get the “full story” of this data analysis, as I will be selecting a more refined set of features to model and testing the predictions of all of the models I have and will build on unseen test data, respectively.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train model using default hyperparameters</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">43</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>rf_default_model <span class="ot">&lt;-</span> ranger<span class="sc">::</span><span class="fu">ranger</span>(</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">formula =</span> sum_arith_perf <span class="sc">~</span> ., <span class="co"># Target variable and features</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> training_subset, <span class="co"># Training dataset</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">num.trees =</span> <span class="dv">8000</span>, <span class="co"># Number of trees</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">mtry =</span> <span class="fl">4.47</span>, <span class="co"># Number of variables randomly sampled at each split</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">importance =</span> <span class="st">"permutation"</span>, <span class="co"># Computes feature importance</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">splitrule =</span> <span class="st">"variance"</span>, <span class="co"># For regression (use "gini" for classification)</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">verbose =</span> <span class="cn">TRUE</span> <span class="co"># Show training details</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(rf_default_model)</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="do">## Ranger result</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a><span class="do">##  ranger::ranger(formula = sum_arith_perf ~ ., data = training_subset,      num.trees = 8000, mtry = 4.47, importance = "permutation",      splitrule = "variance", verbose = TRUE) </span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a><span class="do">## Type:                             Regression </span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a><span class="do">## Number of trees:                  8000 </span></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a><span class="do">## Sample size:                      515 </span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a><span class="do">## Number of independent variables:  20 </span></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a><span class="do">## Mtry:                             4 </span></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a><span class="do">## Target node size:                 5 </span></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a><span class="do">## Variable importance mode:         permutation </span></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a><span class="do">## Splitrule:                        variance </span></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a><span class="do">## OOB prediction error (MSE):       37.84268 </span></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a><span class="do">## R squared (OOB):                  0.1312439</span></span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>default_importance_scores <span class="ot">&lt;-</span> rf_default_model<span class="sc">$</span>variable.importance</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>importance_scores3 <span class="ot">&lt;-</span> rf_default_model<span class="sc">$</span>variable.importance</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>default_importance_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>  <span class="at">Variable =</span> <span class="fu">names</span>(importance_scores3), <span class="at">Importance =</span> importance_scores3</span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>default_importance_df <span class="ot">&lt;-</span> default_importance_df[</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a>  <span class="fu">order</span>(<span class="sc">-</span>default_importance_df<span class="sc">$</span>Importance),</span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(default_importance_df)</span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a><span class="do">##                                        Variable  Importance</span></span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a><span class="do">## score_PISA_ME                     score_PISA_ME  2.49654711</span></span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a><span class="do">## liking_math                         liking_math  2.16155753</span></span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a><span class="do">## score_SDQ_M                         score_SDQ_M  1.88547592</span></span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a><span class="do">## score_AMAS_total               score_AMAS_total  1.58219510</span></span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a><span class="do">## persistence_math               persistence_math  1.54847319</span></span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a><span class="do">## math_grade                           math_grade  1.38496868</span></span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a><span class="do">## sex                                         sex  1.32869094</span></span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a><span class="do">## total_time_minutes           total_time_minutes  0.81925869</span></span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a><span class="do">## liking_humanities             liking_humanities  0.76350819</span></span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a><span class="do">## liking_science                   liking_science  0.65735862</span></span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a><span class="do">## score_STAI_state_short   score_STAI_state_short  0.61467080</span></span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a><span class="do">## persistence_science         persistence_science  0.53049509</span></span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a><span class="do">## math_load                             math_load  0.51647010</span></span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a><span class="do">## persistence_humanities   persistence_humanities  0.51088914</span></span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a><span class="do">## score_GAD                             score_GAD  0.51076088</span></span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a><span class="do">## score_BFI_N                         score_BFI_N  0.34438627</span></span>
<span id="cb10-56"><a href="#cb10-56" aria-hidden="true" tabindex="-1"></a><span class="do">## score_TAI_short                 score_TAI_short  0.31063735</span></span>
<span id="cb10-57"><a href="#cb10-57" aria-hidden="true" tabindex="-1"></a><span class="do">## age                                         age  0.15123593</span></span>
<span id="cb10-58"><a href="#cb10-58" aria-hidden="true" tabindex="-1"></a><span class="do">## score_SDQ_L                         score_SDQ_L  0.10091545</span></span>
<span id="cb10-59"><a href="#cb10-59" aria-hidden="true" tabindex="-1"></a><span class="do">## math_inf_program_choice math_inf_program_choice -0.00304766</span></span>
<span id="cb10-60"><a href="#cb10-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-61"><a href="#cb10-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-62"><a href="#cb10-62" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">44</span>)</span>
<span id="cb10-63"><a href="#cb10-63" aria-hidden="true" tabindex="-1"></a>rf_default_model2 <span class="ot">&lt;-</span> ranger<span class="sc">::</span><span class="fu">ranger</span>(</span>
<span id="cb10-64"><a href="#cb10-64" aria-hidden="true" tabindex="-1"></a>  <span class="at">formula =</span> sum_arith_perf <span class="sc">~</span> ., <span class="co"># Target variable and features</span></span>
<span id="cb10-65"><a href="#cb10-65" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> training_subset, <span class="co"># Training dataset</span></span>
<span id="cb10-66"><a href="#cb10-66" aria-hidden="true" tabindex="-1"></a>  <span class="at">num.trees =</span> <span class="dv">8000</span>, <span class="co"># Number of trees</span></span>
<span id="cb10-67"><a href="#cb10-67" aria-hidden="true" tabindex="-1"></a>  <span class="at">mtry =</span> <span class="fl">6.666</span>, <span class="co"># Number of variables randomly sampled at each split</span></span>
<span id="cb10-68"><a href="#cb10-68" aria-hidden="true" tabindex="-1"></a>  <span class="at">importance =</span> <span class="st">"permutation"</span>, <span class="co"># Computes feature importance</span></span>
<span id="cb10-69"><a href="#cb10-69" aria-hidden="true" tabindex="-1"></a>  <span class="at">splitrule =</span> <span class="st">"variance"</span>, <span class="co"># For regression (use "gini" for classification)</span></span>
<span id="cb10-70"><a href="#cb10-70" aria-hidden="true" tabindex="-1"></a>  <span class="at">verbose =</span> <span class="cn">TRUE</span> <span class="co"># Show training details</span></span>
<span id="cb10-71"><a href="#cb10-71" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-72"><a href="#cb10-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-73"><a href="#cb10-73" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(rf_default_model2)</span>
<span id="cb10-74"><a href="#cb10-74" aria-hidden="true" tabindex="-1"></a><span class="do">## Ranger result</span></span>
<span id="cb10-75"><a href="#cb10-75" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb10-76"><a href="#cb10-76" aria-hidden="true" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb10-77"><a href="#cb10-77" aria-hidden="true" tabindex="-1"></a><span class="do">##  ranger::ranger(formula = sum_arith_perf ~ ., data = training_subset,      num.trees = 8000, mtry = 6.666, importance = "permutation",      splitrule = "variance", verbose = TRUE) </span></span>
<span id="cb10-78"><a href="#cb10-78" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb10-79"><a href="#cb10-79" aria-hidden="true" tabindex="-1"></a><span class="do">## Type:                             Regression </span></span>
<span id="cb10-80"><a href="#cb10-80" aria-hidden="true" tabindex="-1"></a><span class="do">## Number of trees:                  8000 </span></span>
<span id="cb10-81"><a href="#cb10-81" aria-hidden="true" tabindex="-1"></a><span class="do">## Sample size:                      515 </span></span>
<span id="cb10-82"><a href="#cb10-82" aria-hidden="true" tabindex="-1"></a><span class="do">## Number of independent variables:  20 </span></span>
<span id="cb10-83"><a href="#cb10-83" aria-hidden="true" tabindex="-1"></a><span class="do">## Mtry:                             6 </span></span>
<span id="cb10-84"><a href="#cb10-84" aria-hidden="true" tabindex="-1"></a><span class="do">## Target node size:                 5 </span></span>
<span id="cb10-85"><a href="#cb10-85" aria-hidden="true" tabindex="-1"></a><span class="do">## Variable importance mode:         permutation </span></span>
<span id="cb10-86"><a href="#cb10-86" aria-hidden="true" tabindex="-1"></a><span class="do">## Splitrule:                        variance </span></span>
<span id="cb10-87"><a href="#cb10-87" aria-hidden="true" tabindex="-1"></a><span class="do">## OOB prediction error (MSE):       38.13992 </span></span>
<span id="cb10-88"><a href="#cb10-88" aria-hidden="true" tabindex="-1"></a><span class="do">## R squared (OOB):                  0.1244199</span></span>
<span id="cb10-89"><a href="#cb10-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-90"><a href="#cb10-90" aria-hidden="true" tabindex="-1"></a>importance_scores4 <span class="ot">&lt;-</span> rf_default_model2<span class="sc">$</span>variable.importance</span>
<span id="cb10-91"><a href="#cb10-91" aria-hidden="true" tabindex="-1"></a>default_importance_df2 <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb10-92"><a href="#cb10-92" aria-hidden="true" tabindex="-1"></a>  <span class="at">Variable =</span> <span class="fu">names</span>(importance_scores4), <span class="at">Importance =</span> importance_scores4</span>
<span id="cb10-93"><a href="#cb10-93" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-94"><a href="#cb10-94" aria-hidden="true" tabindex="-1"></a>default_importance_df2 <span class="ot">&lt;-</span> default_importance_df2[</span>
<span id="cb10-95"><a href="#cb10-95" aria-hidden="true" tabindex="-1"></a>  <span class="fu">order</span>(<span class="sc">-</span>default_importance_df2<span class="sc">$</span>Importance),</span>
<span id="cb10-96"><a href="#cb10-96" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb10-97"><a href="#cb10-97" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(default_importance_df2)</span>
<span id="cb10-98"><a href="#cb10-98" aria-hidden="true" tabindex="-1"></a><span class="do">##                                        Variable  Importance</span></span>
<span id="cb10-99"><a href="#cb10-99" aria-hidden="true" tabindex="-1"></a><span class="do">## score_PISA_ME                     score_PISA_ME  3.04865606</span></span>
<span id="cb10-100"><a href="#cb10-100" aria-hidden="true" tabindex="-1"></a><span class="do">## liking_math                         liking_math  2.12716398</span></span>
<span id="cb10-101"><a href="#cb10-101" aria-hidden="true" tabindex="-1"></a><span class="do">## score_SDQ_M                         score_SDQ_M  1.87608742</span></span>
<span id="cb10-102"><a href="#cb10-102" aria-hidden="true" tabindex="-1"></a><span class="do">## score_AMAS_total               score_AMAS_total  1.60184531</span></span>
<span id="cb10-103"><a href="#cb10-103" aria-hidden="true" tabindex="-1"></a><span class="do">## persistence_math               persistence_math  1.55521674</span></span>
<span id="cb10-104"><a href="#cb10-104" aria-hidden="true" tabindex="-1"></a><span class="do">## sex                                         sex  1.35839263</span></span>
<span id="cb10-105"><a href="#cb10-105" aria-hidden="true" tabindex="-1"></a><span class="do">## math_grade                           math_grade  1.27416425</span></span>
<span id="cb10-106"><a href="#cb10-106" aria-hidden="true" tabindex="-1"></a><span class="do">## total_time_minutes           total_time_minutes  0.95407078</span></span>
<span id="cb10-107"><a href="#cb10-107" aria-hidden="true" tabindex="-1"></a><span class="do">## score_STAI_state_short   score_STAI_state_short  0.61972107</span></span>
<span id="cb10-108"><a href="#cb10-108" aria-hidden="true" tabindex="-1"></a><span class="do">## liking_humanities             liking_humanities  0.58438631</span></span>
<span id="cb10-109"><a href="#cb10-109" aria-hidden="true" tabindex="-1"></a><span class="do">## liking_science                   liking_science  0.53773137</span></span>
<span id="cb10-110"><a href="#cb10-110" aria-hidden="true" tabindex="-1"></a><span class="do">## score_GAD                             score_GAD  0.50994630</span></span>
<span id="cb10-111"><a href="#cb10-111" aria-hidden="true" tabindex="-1"></a><span class="do">## persistence_science         persistence_science  0.45964434</span></span>
<span id="cb10-112"><a href="#cb10-112" aria-hidden="true" tabindex="-1"></a><span class="do">## math_load                             math_load  0.45875730</span></span>
<span id="cb10-113"><a href="#cb10-113" aria-hidden="true" tabindex="-1"></a><span class="do">## persistence_humanities   persistence_humanities  0.39978225</span></span>
<span id="cb10-114"><a href="#cb10-114" aria-hidden="true" tabindex="-1"></a><span class="do">## score_TAI_short                 score_TAI_short  0.39717869</span></span>
<span id="cb10-115"><a href="#cb10-115" aria-hidden="true" tabindex="-1"></a><span class="do">## score_BFI_N                         score_BFI_N  0.34839674</span></span>
<span id="cb10-116"><a href="#cb10-116" aria-hidden="true" tabindex="-1"></a><span class="do">## age                                         age  0.15914036</span></span>
<span id="cb10-117"><a href="#cb10-117" aria-hidden="true" tabindex="-1"></a><span class="do">## score_SDQ_L                         score_SDQ_L  0.01485572</span></span>
<span id="cb10-118"><a href="#cb10-118" aria-hidden="true" tabindex="-1"></a><span class="do">## math_inf_program_choice math_inf_program_choice -0.29177447</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The End Links to other Documents: <code>Methods for Variable Selection</code> <code>LASSO Addendum</code> <code>Predictions</code></p>
</section>
<section id="references" class="level2" data-number="1.8">
<h2 data-number="1.8" class="anchored" data-anchor-id="references"><span class="header-section-number">1.8</span> References</h2>
<p><em>Note</em>: This references list contains additional works not cited in the article above, which I used to conduct my literature review when beginning to compose this document.</p>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-allaire_quarto_2024" class="csl-entry" role="listitem">
Allaire, J. J., Teague, C., Scheidegger, C., Xie, Y., Dervieux, C., &amp; Woodhull, G. (2024). <em>Quarto</em>. <a href="https://doi.org/10.5281/zenodo.5960048">https://doi.org/10.5281/zenodo.5960048</a>
</div>
<div id="ref-barrenada_understanding_2024" class="csl-entry" role="listitem">
Barreñada, L., Dhiman, P., Timmerman, D., Boulesteix, A.-L., &amp; Calster, B. V. (2024). Understanding overfitting in random forest for probability estimation: A visualization and simulation study. <em>Diagnostic and Prognostic Research</em>, <em>8</em>(1), 14. <a href="https://doi.org/10.1186/s41512-024-00177-1">https://doi.org/10.1186/s41512-024-00177-1</a>
</div>
<div id="ref-bischl_mlrmbo_2018" class="csl-entry" role="listitem">
Bischl, B., Richter, J., Bossek, J., Horn, D., Thomas, J., &amp; Lang, M. (2018). <em><span class="nocase">mlrMBO</span>: <span>A</span> <span>Modular</span> <span>Framework</span> for <span>Model</span>-<span>Based</span> <span>Optimization</span> of <span>Expensive</span> <span>Black</span>-<span>Box</span> <span>Functions</span></em>. arXiv. <a href="https://doi.org/10.48550/arXiv.1703.03373">https://doi.org/10.48550/arXiv.1703.03373</a>
</div>
<div id="ref-breiman_random_2001" class="csl-entry" role="listitem">
Breiman, L. (2001). Random <span>Forests</span>. <em>Machine Learning</em>, <em>45</em>(1), 5–32. <a href="https://doi.org/10.1023/A:1010933404324">https://doi.org/10.1023/A:1010933404324</a>
</div>
<div id="ref-cipora_amatus_2024" class="csl-entry" role="listitem">
Cipora, K., Lunardon, M., Masson, N., Georges, C., Nuerk, H.-C., &amp; Artemenko, C. (2024). The <span>AMATUS</span> <span>Dataset</span>: <span>Arithmetic</span> <span>Performance</span>, <span>Mathematics</span> <span>Anxiety</span> and <span>Attitudes</span> in <span>Primary</span> <span>School</span> <span>Teachers</span> and <span>University</span> <span>Students</span>. <em>Journal of Open Psychology Data</em>, <em>12</em>(1). <a href="https://doi.org/10.5334/jopd.115">https://doi.org/10.5334/jopd.115</a>
</div>
<div id="ref-degenhardt_evaluation_2019" class="csl-entry" role="listitem">
Degenhardt, F., Seifert, S., &amp; Szymczak, S. (2019). Evaluation of variable selection methods for random forests and omics data sets. <em>Briefings in Bioinformatics</em>, <em>20</em>(2), 492–503. <a href="https://doi.org/10.1093/bib/bbx124">https://doi.org/10.1093/bib/bbx124</a>
</div>
<div id="ref-genuer_random_2008" class="csl-entry" role="listitem">
Genuer, R., Poggi, J.-M., &amp; Tuleau, C. (2008). <em>Random <span>Forests</span>: Some methodological insights</em>. arXiv. <a href="https://doi.org/10.48550/arXiv.0811.3619">https://doi.org/10.48550/arXiv.0811.3619</a>
</div>
<div id="ref-genuer_variable_2010" class="csl-entry" role="listitem">
Genuer, R., Poggi, J.-M., &amp; Tuleau-Malot, C. (2010). Variable selection using random forests. <em>Pattern Recognition Letters</em>, <em>31</em>(14), 2225–2236. <a href="https://doi.org/10.1016/j.patrec.2010.03.014">https://doi.org/10.1016/j.patrec.2010.03.014</a>
</div>
<div id="ref-goldstein_random_2011" class="csl-entry" role="listitem">
Goldstein, B. A., Polley, E. C., &amp; Briggs, F. B. S. (2011). Random <span>Forests</span> for <span>Genetic</span> <span>Association</span> <span>Studies</span>. <em>Statistical Applications in Genetics and Molecular Biology</em>, <em>10</em>(1). <a href="https://doi.org/10.2202/1544-6115.1691">https://doi.org/10.2202/1544-6115.1691</a>
</div>
<div id="ref-gromping_variable_2009" class="csl-entry" role="listitem">
Grömping, U. (2009). Variable <span>Importance</span> <span>Assessment</span> in <span>Regression</span>: <span>Linear</span> <span>Regression</span> versus <span>Random</span> <span>Forest</span>. <em>The American Statistician</em>, <em>63</em>(4), 308–319. <a href="https://doi.org/10.1198/tast.2009.08199">https://doi.org/10.1198/tast.2009.08199</a>
</div>
<div id="ref-coello_sequential_2011" class="csl-entry" role="listitem">
Hutter, F., Hoos, H. H., &amp; Leyton-Brown, K. (2011). Sequential <span>Model</span>-<span>Based</span> <span>Optimization</span> for <span>General</span> <span>Algorithm</span> <span>Configuration</span>. In C. A. C. Coello (Ed.), <em>Learning and <span>Intelligent</span> <span>Optimization</span></em> (Vol. 6683, pp. 507–523). Springer Berlin Heidelberg. <a href="https://doi.org/10.1007/978-3-642-25566-3_40">https://doi.org/10.1007/978-3-642-25566-3_40</a>
</div>
<div id="ref-ishwaran_standard_2019" class="csl-entry" role="listitem">
Ishwaran, H., &amp; Lu, M. (2019). Standard errors and confidence intervals for variable importance in random forest regression, classification, and survival. <em>Statistics in Medicine</em>, <em>38</em>(4), 558–582. <a href="https://doi.org/10.1002/sim.7803">https://doi.org/10.1002/sim.7803</a>
</div>
<div id="ref-janitza_overestimation_2018" class="csl-entry" role="listitem">
Janitza, S., &amp; Hornung, R. (2018). On the overestimation of random forest’s out-of-bag error. <em>PLoS One</em>, <em>13</em>(8), e0201904. <a href="https://doi.org/10.1371/journal.pone.0201904">https://doi.org/10.1371/journal.pone.0201904</a>
</div>
<div id="ref-martinez-munoz_outbag_2010" class="csl-entry" role="listitem">
Martínez-Muñoz, G., &amp; Suárez, A. (2010). Out-of-bag estimation of the optimal sample size in bagging. <em>Pattern Recognition</em>, <em>43</em>(1), 143–152. <a href="https://doi.org/10.1016/j.patcog.2009.05.010">https://doi.org/10.1016/j.patcog.2009.05.010</a>
</div>
<div id="ref-muller_here_2020" class="csl-entry" role="listitem">
Müller, K. (2020). <em>Here: <span>A</span> <span>Simpler</span> <span>Way</span> to <span>Find</span> <span>Your</span> <span>Files</span></em>. <a href="https://CRAN.R-project.org/package=here">https://CRAN.R-project.org/package=here</a>
</div>
<div id="ref-organisation_for_economic_co-operation_and_development_internationaler_2012" class="csl-entry" role="listitem">
Organisation for Economic Co-operation and Development. (2012). <em>Internationaler <span>Schülerfragebogen</span> <span>PISA</span> 2012 [<span>International</span> student questionnaire <span>PISA</span> 2012]</em>. <a href="https://www.iqs.gv.at/downloads/internationale-studien/pisa/pisa-2012">https://www.iqs.gv.at/downloads/internationale-studien/pisa/pisa-2012</a>
</div>
<div id="ref-van_der_ploeg_modern_2014" class="csl-entry" role="listitem">
Ploeg, T. van der, Austin, P. C., &amp; Steyerberg, E. W. (2014). Modern modelling techniques are data hungry: A simulation study for predicting dichotomous endpoints. <em>BMC Medical Research Methodology</em>, <em>14</em>(1), 137. <a href="https://doi.org/10.1186/1471-2288-14-137">https://doi.org/10.1186/1471-2288-14-137</a>
</div>
<div id="ref-probst_hyperparameters_2019" class="csl-entry" role="listitem">
Probst, P., Wright, M., &amp; Boulesteix, A.-L. (2019). Hyperparameters and <span>Tuning</span> <span>Strategies</span> for <span>Random</span> <span>Forest</span>. <em>WIREs Data Mining and Knowledge Discovery</em>, <em>9</em>(3), e1301. <a href="https://doi.org/10.1002/widm.1301">https://doi.org/10.1002/widm.1301</a>
</div>
<div id="ref-segal_machine_2003" class="csl-entry" role="listitem">
Segal, M. (2003). Machine <span>Learning</span> <span>Benchmarks</span> and <span>Random</span> <span>Forest</span> <span>Regression</span>. <em>Technical Report, Center for Bioinformatics &amp; Molecular Biostatistics, University of California, San Francisco</em>.
</div>
<div id="ref-shmueli_explain_2010" class="csl-entry" role="listitem">
Shmueli, G. (2010). To <span>Explain</span> or to <span>Predict</span>? <em>Statistical Science</em>, <em>25</em>(3). <a href="https://doi.org/10.1214/10-STS330">https://doi.org/10.1214/10-STS330</a>
</div>
<div id="ref-speiser_comparison_2019" class="csl-entry" role="listitem">
Speiser, J. L., Miller, M. E., Tooze, J., &amp; Ip, E. (2019). A comparison of random forest variable selection methods for classification prediction modeling. <em>Expert Systems with Applications</em>, <em>134</em>, 93–101. <a href="https://doi.org/10.1016/j.eswa.2019.05.028">https://doi.org/10.1016/j.eswa.2019.05.028</a>
</div>
<div id="ref-strobl_bias_2007" class="csl-entry" role="listitem">
Strobl, C., Boulesteix, A.-L., Zeileis, A., &amp; Hothorn, T. (2007). Bias in random forest variable importance measures: <span>Illustrations</span>, sources and a solution. <em>BMC Bioinformatics</em>, <em>8</em>(1), 25. <a href="https://doi.org/10.1186/1471-2105-8-25">https://doi.org/10.1186/1471-2105-8-25</a>
</div>
<div id="ref-strobl_danger_2008" class="csl-entry" role="listitem">
Strobl, C., &amp; Zeileis, A. (2008). <em>Danger: <span>High</span> <span>Power</span>! – <span>Exploring</span> the <span>Statistical</span> <span>Properties</span> of a <span>Test</span> for <span>Random</span> <span>Forest</span> <span>Variable</span> <span>Importance</span></em>. <a href="https://doi.org/10.5282/ubm/epub.2111">https://doi.org/10.5282/ubm/epub.2111</a>
</div>
<div id="ref-wallace_use_2023" class="csl-entry" role="listitem">
Wallace, M. L., Mentch, L., Wheeler, B. J., Tapia, A. L., Richards, M., Zhou, S., Yi, L., Redline, S., &amp; Buysse, D. J. (2023). Use and misuse of random forest variable importance metrics in medicine: Demonstrations through incident stroke prediction. <em>BMC Medical Research Methodology</em>, <em>23</em>(1), 144. <a href="https://doi.org/10.1186/s12874-023-01965-x">https://doi.org/10.1186/s12874-023-01965-x</a>
</div>
<div id="ref-wallert_predicting_2018" class="csl-entry" role="listitem">
Wallert, J., Gustafson, E., Held, C., Madison, G., Norlund, F., Essen, L. von, &amp; Olsson, E. M. G. (2018). Predicting <span>Adherence</span> to <span>Internet</span>-<span>Delivered</span> <span>Psychotherapy</span> for <span>Symptoms</span> of <span>Depression</span> and <span>Anxiety</span> <span>After</span> <span>Myocardial</span> <span>Infarction</span>: <span>Machine</span> <span>Learning</span> <span>Insights</span> <span>From</span> the <span>U</span>-<span>CARE</span> <span>Heart</span> <span>Randomized</span> <span>Controlled</span> <span>Trial</span>. <em>Journal of Medical Internet Research</em>, <em>20</em>(10), e10754. <a href="https://doi.org/10.2196/10754">https://doi.org/10.2196/10754</a>
</div>
<div id="ref-wright_ranger_2017" class="csl-entry" role="listitem">
Wright, M. N., &amp; Ziegler, A. (2017). Ranger: <span>A</span> <span>Fast</span> <span>Implementation</span> of <span>Random</span> <span>Forests</span> for <span>High</span> <span>Dimensional</span> <span>Data</span> in <span>C</span>++ and <span>R</span>. <em>Journal of Statistical Software</em>, <em>77</em>(1), 1–17. <a href="https://doi.org/10.18637/jss.v077.i01">https://doi.org/10.18637/jss.v077.i01</a>
</div>
</div>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>