## Predictions on Test Set

Prediction time. Let's see what we get.

```{r}
#| label: Prediction.
#| warning: false
#### TESTING ACCURACY OF MODEL ON TEST SET!
# First, we'll make sure we have a spare test set in case anything goes wrong.

View(training)
View(test)
# We'll also save the actuals so we have them.
actuals <- test$sum_arith_perf
View(actuals)
# first, find scaling parameters.

scale(training$score_BFI_N) # mean 23.63953, scale 6.21257
scale(training$score_AMAS_total) # 18.43798, 6.76958
scale(training$age) # 23.68023, 4.479996
scale(training$score_GAD) # 12.60271, 4.112863
scale(training$score_PISA_ME) # 20.55426, 3.327287
scale(training$score_STAI_state_short) # 8.224806, 2.92094
scale(training$score_TAI_short) # 10.96318, 3.732903
scale(training$score_SDQ_L) # 13.86822, 2.370137
scale(training$score_SDQ_M) # 11.65504, 3.13223
# We then apply the exact same scaling to the test set.
# Since the test set is unseen data, we apply the TRAINING scaling parameters
# to the TEST set. We also do this so our predictions are not nonsensical.

# Define the desired means and SDs for each predictor
scaling_params <- list(
  score_BFI_N = list(mean = 23.63953, sd = 6.21257),
  score_AMAS_total = list(mean = 18.43798, sd = 6.730569),
  age = list(mean = 23.68023, sd = 4.479996),
  score_GAD = list(mean = 12.60271, sd = 4.112863),
  score_PISA_ME = list(mean = 20.55426, sd = 3.327287),
  score_STAI_state_short = list(mean = 8.224806, sd = 2.92094),
  score_TAI_short = list(mean = 10.96318, sd = 3.732903),
  score_SDQ_L = list(mean = 13.86822, sd = 2.370137),
  score_SDQ_M = list(mean = 11.65504, sd = 3.13223)
)

# Apply scaling to 'age' and 'score_gad' using 'predictor' as the loop variable
# Apply scaling to each variable in the test dataset
for (predictor in names(scaling_params)) {
  if (predictor %in% colnames(test)) { # Check if  predictor exists in dataset
    new_mean <- scaling_params[[predictor]]$mean
    new_sd <- scaling_params[[predictor]]$sd

    # C
    # Overwrite the original predictor with the scaled version
    test[[predictor]] <- (test[[predictor]] - new_mean) / new_sd
  } else {
    warning(paste("Variable", predictor, "not found in test dataset."))
  }
}



# View the dataset with the overwritten variables
View(test)
```

```{r}
# Then, find predictions.
# 10 variable model
predictionsmtry2 <- predict(rf_mtry2, newdata = test)
testmtry2 <- data.frame(
  R2 = R2(predictionsmtry2, test$ sum_arith_perf),
  ## this calculates some kind of pseudo R squared. The manual formula-derived
  # R squared is just below under "standard R2 formula".
  RMSE = RMSE(predictionsmtry2, test$ sum_arith_perf),
  MAE = MAE(predictionsmtry2, test$ sum_arith_perf)
)
print(testmtry2)
# Standard R2 formula
actuals <- test$sum_arith_perf
SSE <- sum((predictionsmtry2 - actuals)^2) # Sum of Squared Errors for mtry2
SST <- sum((actuals - mean(actuals))^2) # Total Sum of Squares for mtry2
R_squared <- 1 - (SSE / SST)
R_squared # You can try this if you'll like


###
# first selection
predictionsfirstselection <- predict(rf_firstselection, newdata = test)
# you're predicting test data from the model built on training data.
validationfirstselection <- data.frame(
  R2 = R2(predictionsfirstselection, test$ sum_arith_perf),
  RMSE = RMSE(predictionsfirstselection, test$ sum_arith_perf),
  MAE = MAE(predictionsfirstselection, test$ sum_arith_perf)
)
print(validationfirstselection)

actuals <- test$sum_arith_perf
SSE <- sum((predictionsfirstselection - actuals)^2) # Sum of Squared Errors
SST <- sum((actuals - mean(actuals))^2) # Total Sum of Squares
R_squared <- 1 - (SSE / SST)
R_squared



###
### Second selection
predictionssecondselection <- predict(rf_secondselection, test)
validationsecondselection <- data.frame(
  R2 = R2(predictionssecondselection, test$ sum_arith_perf),
  RMSE = RMSE(predictionssecondselection, test$ sum_arith_perf),
  MAE = MAE(predictionssecondselection, test$ sum_arith_perf)
)
# these statistical functions are in caret technically under trainControl(),
# which passes this argument through defaultSummary() I believe.
print(validationsecondselection)

actuals <- test$sum_arith_perf
SSE <- sum((predictionssecondselection - actuals)^2) # Sum of Squared Errors
SST <- sum((actuals - mean(actuals))^2) # Total Sum of Squares
R_squared <- 1 - (SSE / SST)
R_squared


#### Third selection
predictionsthirdselection <- predict(rf_thirdselection, test)
validationthirdselection <- data.frame(
  R2 = R2(predictionsthirdselection, test$ sum_arith_perf),
  RMSE = RMSE(predictionsthirdselection, test$ sum_arith_perf),
  MAE = MAE(predictionsthirdselection, test$ sum_arith_perf)
)
# these statistical functions are in caret technically under trainControl(),
# which passes this argument through defaultSummary() I believe.
print(validationthirdselection)

actuals <- test$sum_arith_perf
SSE <- sum((predictionsthirdselection - actuals)^2) # Sum of Squared Errors
SST <- sum((actuals - mean(actuals))^2) # Total Sum of Squares
R_squared <- 1 - (SSE / SST)
R_squared


###### fourth selection
predictionsfourthselection <- predict(rf_fourthselection, test)
validationfourthselection <- data.frame(
  R2 = R2(predictionsfourthselection, test$ sum_arith_perf),
  RMSE = RMSE(predictionsfourthselection, test$ sum_arith_perf),
  MAE = MAE(predictionsfourthselection, test$ sum_arith_perf)
)
print(validationfourthselection)

actuals <- test$sum_arith_perf
SSE <- sum((predictionsfourthselection - actuals)^2) # Sum of Squared Errors
SST <- sum((actuals - mean(actuals))^2) # Total Sum of Squares
R_squared <- 1 - (SSE / SST)
R_squared
## BEST PREDICTIVE MODEL!

##### fifth selection
predictionsfifthselection <- predict(rf_fifthselection, test)
validationfifthselection <- data.frame(
  R2 = R2(predictionsfifthselection, test$ sum_arith_perf),
  RMSE = RMSE(predictionsfifthselection, test$ sum_arith_perf),
  MAE = MAE(predictionsfifthselection, test$ sum_arith_perf)
)
print(validationfifthselection)

actuals <- test$sum_arith_perf
SSE <- sum((predictionsfifthselection - actuals)^2) # Sum of Squared Errors
SST <- sum((actuals - mean(actuals))^2) # Total Sum of Squares
R_squared <- 1 - (SSE / SST)
R_squared


#### sixth selection
predictionssixthselection <- predict(rf_sixthselection, test)
validationsixthselection <- data.frame(
  R2 = R2(predictionssixthselection, test$ sum_arith_perf),
  RMSE = RMSE(predictionssixthselection, test$ sum_arith_perf),
  MAE = MAE(predictionssixthselection, test$ sum_arith_perf)
)
print(validationsixthselection)

actuals <- test$sum_arith_perf
SSE <- sum((predictionssixthselection - actuals)^2) # Sum of Squared Errors
SST <- sum((actuals - mean(actuals))^2) # Total Sum of Squares
R_squared <- 1 - (SSE / SST)
R_squared


### 2/3 variable models
predictionsstrongest1 <- predict(rf_strongestvariables, test)
validationstrongest1 <- data.frame(
  R2 = R2(predictionsstrongest1, test$ sum_arith_perf),
  RMSE = RMSE(predictionsstrongest1, test$ sum_arith_perf),
  MAE = MAE(predictionsstrongest1, test$ sum_arith_perf)
)
print(validationstrongest1)

predictionsstrongest2 <- predict(rf_strongestvariables2, test)
validationstrongest2 <- data.frame(
  R2 = R2(predictionsstrongest2, test$ sum_arith_perf),
  RMSE = RMSE(predictionsstrongest2, test$ sum_arith_perf),
  MAE = MAE(predictionsstrongest2, test$ sum_arith_perf)
)
print(validationstrongest2)


predictionsstrongest3 <- predict(rf_strongestvariables3, test)
validationstrongest3 <- data.frame(
  R2 = R2(predictionsstrongest2, test$ sum_arith_perf),
  RMSE = RMSE(predictionsstrongest2, test$ sum_arith_perf),
  MAE = MAE(predictionsstrongest2, test$ sum_arith_perf)
)
print(validationstrongest3)



predictionstwovariablemodel <- predict(rf_strongestvariables4, test)
validationtwovariablemodel <- data.frame(
  R2 = R2(predictionstwovariablemodel, test$ sum_arith_perf),
  RMSE = RMSE(predictionstwovariablemodel, test$ sum_arith_perf),
  MAE = MAE(predictionstwovariablemodel, test$ sum_arith_perf)
)
print(validationtwovariablemodel)

actuals <- test$sum_arith_perf
SSE <- sum((predictionstwovariablemodel - actuals)^2) # Sum of Squared Errors
SST <- sum((actuals - mean(actuals))^2) # Total Sum of Squares
R_squared <- 1 - (SSE / SST)
R_squared


#### Just to double check our code above.
```

Like we posited earlier, the full model did indeed turn out to be the best model for prediction. Our parsimonious model turned out to be one of the best models for prediction (better than all except our first two models) and many of our in-between models with the 4/5 most key variables, despite predicting the train data the best, predicted the test set middlingly at best. Thus, feature selection in this case turned out to be wholly unnecessary for prediction, and was only useful for identifying a parsimonious model and the "true" predictors from the list of ten, which appear to have non-zero correlation with the DV but may not comprise the best predictive model. This may happen in psychology data because of the relatively small effect sizes of even some of the best predictors - this is why train/test splitting (or train, test, validation splitting) is important. Therefore, if we were only looking to predict arithmetic performance in German students, it would be best to use the full model or use an algorithmic variable selection method to search for pure predictive power, such as VSURF or LASSO (which we will use in an addendum shortly). If we were more interested in describing the most parsimonious model of arithmetic performance, the two-variable model would be a good choice. A 4-5 variable model (sex, math self-efficacy, math self-confidence, math anxiety, and lastly the slightly less valuable state anxiety) might also be a good choice to explain a few additional predictors that do indeed seem to be useful in predicting arithmetic performance, but given the poor fit of the model on test data, I would not be quite convinced. If you were strictly interested in prediction, algorithmic variable selection methods would reveal if there was a better fitting model in that range, even though it appears that given our weak predictors, the best model seems to be the full 10-variable model.
