---
title: "LASSO Addendum"
format: html
editor: visual
---

## Addendum: Using LASSO as Feature Selection for Psychologists in R

As an addendum, I will walk through the use of LASSO for feature selection on a psychological dataset in R. LASSO is a regression method that performs variable selection and regularization. LASSO is essentially an extension of ordinary least squares where a penalty term is added to the loss function, resulting in "weightings" of coefficients. The penalty term in LASSO (L1 regularization) uses absolute value, whereas the penalty term in ridge regression uses squared values (L2 regularization). LASSO can reduce a predictor's weight to 0, removing it from the model, which makes it more suitable for feature selection than ridge regression. The latter is best for handling highly collinear predictors, which we will also check soon since LASSO can handle multicollinearity, but only to an extent.

LASSO uses a least-squares "shrinkage operator", so we should make sure we can fit a linear model with our AMATUS data first. We'll check assumptions and conduct exploratory data analysis.

As part of the AMATUS dataset, the original researchers already calculated a correlation matrix for the variables we're interested in. This will be great to identify variance inflation later on.

```{r}
library(ggplot2)
library(olsrr) #nice tools for linear regression
library(caret)
library(MASS)
library(dplyr)
library(car)

AMASplot<-ggplot(data=amatusclean,aes(x=score_AMAS_total,y=sum_arith_perf))
AMASplot<-AMASplot+geom_point(fill="lightskyblue1",color="black")
AMASplot<-AMASplot+geom_count(show.legend = TRUE)
AMASplot<-AMASplot+scale_y_continuous(breaks=seq(0, 50, 5),limits=c(0,50))
AMASplot<-AMASplot+geom_smooth(method = "lm", se = FALSE) +
  geom_smooth(method = "loess", se = FALSE) 
AMASplot

#modestly linear, right tail deviates. 

```

```{r}
lassoamatus<-amatusclean
ageplot<-ggplot(data=lassoamatus,aes(x=age,y=sum_arith_perf))
ageplot<-ageplot+geom_point(fill="lightskyblue1",color="black")
ageplot<-ageplot+geom_count(show.legend = TRUE)
ageplot<-ageplot+scale_y_continuous(breaks=seq(0, 50, 5),limits=c(0,50))
ageplot<-ageplot+geom_smooth(method = "lm", se = FALSE) 
ageplot<-ageplot+geom_smooth(method = "loess", se = FALSE)
ageplot
#notreallylinear. A log transformation makes it look a little better.
```

```{r}
bfiplot<-ggplot(data=lassoamatus,aes(x=score_BFI_N,y=sum_arith_perf))
bfiplot<-bfiplot+geom_point(fill="lightskyblue1",color="black")
bfiplot<-bfiplot+geom_count(show.legend = TRUE)
bfiplot<-bfiplot+scale_y_continuous(breaks=seq(0, 50, 5),limits=c(0,50))
bfiplot<-bfiplot+geom_smooth(method = "lm", se = FALSE) +
geom_smooth(method = "loess", se = FALSE) 
bfiplot
#Pretty damn linear.
```

```{r}
gadplot<-ggplot(data=lassoamatus,aes(x=score_GAD,y=sum_arith_perf))
gadplot<-gadplot+geom_point(fill="lightskyblue1",color="black")
gadplot<-gadplot+geom_count(show.legend = TRUE)
gadplot<-gadplot+scale_y_continuous(breaks=seq(0, 50, 5),limits=c(0,50))
gadplot<-gadplot+geom_smooth(method = "lm", se = FALSE) +
geom_smooth(method = "loess", se = FALSE) 
gadplot
#modestly linear?
```

```{r}
PISAplot<-ggplot(data=lassoamatus,aes(x=score_PISA_ME,y=sum_arith_perf))
PISAplot<-PISAplot+geom_point(fill="lightskyblue1",color="black")
PISAplot<-PISAplot+geom_count(show.legend = TRUE)
PISAplot<-PISAplot+scale_y_continuous(breaks=seq(0, 50, 5),limits=c(0,50))
PISAplot<-PISAplot+geom_smooth(method = "lm", se = FALSE) +
  geom_smooth(method = "loess", se = FALSE) 
PISAplot
#modestly linear.
```

```{r}
STAIplot<-ggplot(data=lassoamatus,aes(x=score_STAI_state_short,y=sum_arith_perf))
STAIplot<-STAIplot+geom_point(fill="lightskyblue1",color="black")
STAIplot<-STAIplot+geom_count(show.legend = TRUE)
STAIplot<-STAIplot+scale_y_continuous(breaks=seq(0, 50, 5),limits=c(0,50))
STAIplot<-STAIplot+geom_smooth(method = "lm", se = FALSE) +
  geom_smooth(method = "loess", se = FALSE) 
STAIplot
#modestly linear, big deviation at right tail.
```

```{r}
TAIplot<-ggplot(data=lassoamatus,aes(x=score_TAI_short,y=sum_arith_perf))
TAIplot<-TAIplot+geom_point(fill="lightskyblue1",color="black")
TAIplot<-TAIplot+geom_count(show.legend = TRUE)
TAIplot<-TAIplot+scale_y_continuous(breaks=seq(0, 50, 5),limits=c(0,50))
TAIplot<-TAIplot+geom_smooth(method = "lm", se = FALSE) +
  geom_smooth(method = "loess", se = FALSE) 
TAIplot
#Pretty linear, tiny deviation at right tail.
```

```{r}
SDQ_Mplot<-ggplot(data=lassoamatus,aes(x=score_SDQ_M,y=sum_arith_perf))
SDQ_Mplot<-SDQ_Mplot+geom_point(fill="lightskyblue1",color="black")
SDQ_Mplot<-SDQ_Mplot+geom_count(show.legend = TRUE)
SDQ_Mplot<-SDQ_Mplot+scale_y_continuous(breaks=seq(0, 50, 5),limits=c(0,50))
SDQ_Mplot<-SDQ_Mplot+geom_smooth(method = "lm", se = FALSE) +
  geom_smooth(method = "loess", se = FALSE) 
SDQ_Mplot
##Vert linear, small deviation at left tail.

```

```{r}
SDQ_Lplot<-ggplot(data=lassoamatus,aes(x=score_SDQ_L,y=sum_arith_perf))
SDQ_Lplot<-SDQ_Lplot+geom_point(fill="lightskyblue1",color="black")
SDQ_Lplot<-SDQ_Lplot+geom_count(show.legend = TRUE)
SDQ_Lplot<-SDQ_Lplot+scale_y_continuous(breaks=seq(0, 50, 5),limits=c(0,50))
SDQ_Lplot<-SDQ_Lplot+geom_smooth(method = "lm", se = FALSE) +
  geom_smooth(method = "loess", se = FALSE) 
SDQ_Lplot
#doesn't seem really linear, to be honest.
```

## Running the Model

Normally, you should naturally transform your variables to the desired distributions then run the model. In order to show you why you should do that, I will first run the model with no transformations.

```{r}
set.seed (150)#I'm going to put this before every random generation just for clarity. Probably should've put it before the data partition.
inTrain<-createDataPartition(amatusclean$sum_arith_perf, p=0.7, list=FALSE)#0.7 selected to have a decent number of N in the test set

#create training vs test data.
training2<-amatusclean[inTrain,]
test2<-amatusclean[-inTrain,]
View(training2)
View(test2)


#Extracting the dataframe to keep things simple.
numeric_predictors <- c("score_BFI_N", "score_AMAS_total", "age", "score_GAD", 
                        "score_PISA_ME", "score_STAI_state_short", 
                        "score_TAI_short", "score_SDQ_L", "score_SDQ_M")
categorical_variable <- "sex"
response_variable <- "sum_arith_perf"

# Create a new dataframe by scaling and centering numeric predictors
library(dplyr)
scaled_training2 <- training2 %>%
    select(all_of(numeric_predictors), all_of(categorical_variable), all_of(response_variable))

scaled_training2 <- scaled_training2 %>%
    mutate(across(where(is.numeric) & !all_of(c("sum_arith_perf", "sex")), 
                  ~ scale(.) %>% as.vector())) 


# View the scaled data
View(scaled_training2)




lambdamodel<-lm(sum_arith_perf~score_AMAS_total+sex+age+score_BFI_N+score_GAD+score_PISA_ME+score_STAI_state_short+score_TAI_short+score_SDQ_M+score_SDQ_L, data=scaled_training2) #we're calling it lambda for reasons to be made abundantly clear shortly
summary(lambdamodel)
##see residual/fitted plots, qq plots, and much more
ols_plot_diagnostics(lambdamodel)
##see residuals by variable, mostly for linearity.
car::residualPlots(lambdamodel,
                   pch=20, col="gray",
                   fitted = F,
                   ask = F, layout = c(3,2),
                   tests = F)
##see residuals by variable, mostly for heteroskedasticity, notice the fitted argument
car::residualPlots(lambdamodel,
                   pch=20, col="gray",
                   ask = F, layout = c(3,2),
                    fitted=T,
                   tests = F)

ggplot(lassoamatus, aes(x = sum_arith_perf)) +
  geom_histogram(binwidth = 0.5, fill = "blue", color = "black") +
  labs(title = "Histogram of Values", x = "Value", y = "Frequency")
```

Like we mentioned in the first document, the dependent variable was skewed right. So is our distribution of residuals, unsurprisingly. We're gonna need to try a transformation of the DV. Let's try a Box Cox transformation to center the varia- wait, we have two observations that are 0 and Box Cox doesn't work with those! We probably wouldn't lose variability if we just cut the two 0s. But that's lazy. Let's try an extremely similar transformation, a Yeo-Johnson transformation that can indeed handle values of 0.

```{r}
library(bestNormalize)
library(car)
#optimal lambda for Yeo-Johnson transformation? Let's find it.
a<-boxCox(lambdamodel, family="yjPower", plotit = TRUE)
optimal_lambda2 <- a$x[which.max(a$y)]
optimal_lambda2
#.58585856

#Let's visualize it too. 
crPlots(lambdamodel,
             pch=20, col="gray",
             smooth = list(smoother=car::gamLine))



# Apply the Yeo-Johnson transformation manually
yj_transform <- yeojohnson(scaled_training2$sum_arith_perf, lambda = optimal_lambda)

scaled_training2$sum_arith_yj <- yj_transform$x.t
# View the transformed data
View(scaled_training2)

ggplot(scaled_training2, aes(x = sum_arith_yj)) +
  geom_histogram(binwidth = 0.3, fill = "blue", color = "black") +
  labs(title = "Histogram of Values", x = "Value", y = "Frequency")

View(scaled_training2)


```

```{r}
#| echo: false
transformedmodel<-lm(sum_arith_yj~score_AMAS_total+sex+age+score_BFI_N+score_GAD+score_PISA_ME+score_STAI_state_short+score_TAI_short+score_SDQ_M+score_SDQ_L, data=scaled_training2)
summary(transformedmodel)
ols_plot_diagnostics(transformedmodel)
vif(transformedmodel)
##see residuals by variable, mostly for linearity
car::residualPlots(transformedmodel,
                   pch=20, col="gray",
                   fitted = F,
                   ask = F, layout = c(3,2),
                   tests = F)
##see residuals by variable, mostly for heteroskedasticity, notice the fitted argument
car::residualPlots(transformedmodel,
                   pch=20, col="gray",
                   ask = F, layout = c(3,2), fitted=T,
                   tests = F)
```

This model looks so much better it's not even funny. Much better resid/fitted plot, much better QQ plot (though with some issues we'll mention in a second), and of course a much more reasonable DV plot.VIF numbers seem reasonable, but we'll keep an eye on it. Notice the residual histogram has an issue - there is a tail of low residual values (which can be spotted near the bottom of the residual/fitted graph.) This indicates the model is systematically underfitting certain values around 0. The QQ plot shows that our distribution is pretty normal, but there are a couple causes for concern, namely the over/under estimation at theoretical quantile values around 2 and -2, respectively. This probably indicates some polynomial terms may be necessary to add to the model, but I will need to confirm.

Just for fun, we'll also try a model just cutting the outliers and also run a model with no SDQ_L.

```{r}
#no outlier model.
scaled_training3<-scaled_training2
scaled_training3<-scaled_training2[-c(502, 10, 477), ]
transformedmodel2<-lm(sum_arith_yj~score_AMAS_total+sex+age+score_BFI_N+score_GAD+score_PISA_ME+score_STAI_state_short+score_TAI_short+score_SDQ_M+score_SDQ_L, data=scaled_training3)
ols_plot_diagnostics(transformedmodel2)

#####No SDQ_L model.
transformedmodel3<-lm(sum_arith_yj~score_AMAS_total+sex+age+score_BFI_N+score_GAD+score_PISA_ME+score_STAI_state_short+score_TAI_short+score_SDQ_M, data=scaled_training2)
ols_plot_diagnostics(transformedmodel3)

View(scaled_training2)

a<-boxCox(lambdamodel, family="yjPower", plotit = TRUE)
optimal_lambda2 <- a$x[which.max(a$y)]
optimal_lambda2
```

Removing the outliers cleaned up the numbers a little but didn't address the issue of mild nonlinearity at certain points for the regression model. Cutting SDQ_L did the same thing, made the QQ plot a little nicer but not so much that we don't notice that weird overfitting at the higher end of the theoretical quantiles. We'll see if we can clean up these variables a little bit more before running our models.

Okay, LASSO. Let's just run it and see what we get.

```{r}
library
R.version.string
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
library(glmnet)
##we need to turn the dataframe into a matrix
x2<- model.matrix(~score_AMAS_total+sex+age+score_BFI_N+score_GAD+score_PISA_ME+score_STAI_state_short+score_TAI_short+score_SDQ_M+score_SDQ_L - 1, data = scaled_training2)

x2 <- x2[, -3]
y2<-scaled_training2$sum_arith_yj
y2
set.seed(151)
cv_model <- cv.glmnet(x2, y2, alpha = 1) 

plot(cv_model)

#The upper and lower bars are the MSE +- standard error.
set.seed(151)
best_lambda <- cv_model$lambda.min
print(best_lambda)
#after we set the lambda, we run the model!
set.seed(152)
lassomodel <- glmnet(x2, y2, alpha = 1, lambda =best_lambda)

lassomodel

print(coef(lassomodel))
View(test2)

#SECOND TRY - NOW WITH LAMBDA that minimizes CV error plus one standard error.
set.seed(151)
best_lambda2 <- cv_model$lambda.1se
print(best_lambda2)
#after we set the lambda, we run the model!
set.seed(152)
lassomodel2 <- glmnet(x2, y2, alpha = 1, lambda = best_lambda2)

lassomodel2
print(coef(lassomodel2))
View(test2)
#This may be a better model, tbh. Let's test later.
##put it back in the linear model for fun
linearlassomodel<-lm(sum_arith_yj~scale(score_AMAS_total)+sex+scale(score_PISA_ME)+scale(score_STAI_state_short)+scale(score_SDQ_M)+scale(score_SDQ_L), data=scaled_training2)
summary(linearlassomodel)
ols_plot_diagnostics(linearlassomodel)
##GOOD IDEA: CHECK FOR DIAGNOSTIC PLOTS (AND STATS) OF YOUR TWO LASSO MODELS. SEE IF THERE ARE MANUAL PLOT COMMANDS FOR LASSO (PARTICULARLY RESIDS), COULD BE SOMETHING YOU WRITE OR WRITE UP WELL.
##CHECK FOR OTHER THINGS LIKE ROBUST LASSO/ADAPTIVE LASSO ETC. SEE WHAT WOULD FIT YOUR DATA BEST. PROBABLY SOMETHING THAT GIVES ROBUST ERRORS DUE TO YOUR MILD HETEROSKEDASTICITY.
#THE ROBUSTIFICATION OF THE LASSO AND ELASTIC NET DISSERTATION BY (?MATT MULTACH)



```

Keep in mind that the cross-validation we used is attempting to estimate the value of lambda that leads to the minimal prediction error. In other words, the lambda is being selected to predict the best, NOT to select the best model. The lambda that is chosen using cross validation is chosen using MSE, which may lead to a "bigger" model than if we were trying to optimize the lambda to "produce the correct model"/"select the relevant predictors".

(more here: https://stats.stackexchange.com/questions/353185/why-using-cross-validation-is-not-a-good-option-for-lasso-regression)

You can "I have had much better luck applying Lasso(ðœ†) to select the model and then fitting by least squares, then applying cross-validation to this entire procedure to select ðœ†. It's still not ideal, but it is a big improvement."

or this "for the particular training fold, CV is applied again to find an optimal ðœ† in that iteration; or apply CV in two batches, first a set of optimization-CV to find an optimal ðœ† then fix that value when fitting LASSO models in a another batch set of validation-CV steps to assess model error."

Let's also try turning up the lambda, - I think this will give us a better model

Let's try including the selected variables of both values of lambda in a random forest model and see the models' stats.

```{r}
library(caret)
####TESTING ACCURACY OF MODEL ON TEST SET!
#First, we'll make sure we have a spare test set in case anything goes horribly wrong.
test2.0<-test2
#We'll also save the actuals so we have them.
actuals <- test2$sum_arith_perf
View(actuals)
View(test2)
View(test2.0)
#first, find scaling parameters.

scale(training2$score_BFI_N) #mean 23.41085, 6.200986
scale(training2$score_AMAS_total)#18.08333, 6.487242
scale(training2$age) #23.37016, 4.241794
scale(training2$score_GAD) #12.48256, 3.958239
scale(training2$score_PISA_ME)#20.58915, 3.328284
scale(training2$score_STAI_state_short) #8.372093, 2.966662
scale(training2$score_TAI_short)#11.01163, 3.716646
scale(training2$score_SDQ_L)#13.7655, 2.350222
scale(training2$score_SDQ_M)#11.69186, 3.078269
#We then apply the exact same scaling to the test set. Since the test set is unseen data, we apply the TRAINING scaling parameters to the TEST set. We also do this so our predictions are not nonsensical.

# Define the desired means and SDs for each predictor
scaling_params <- list(
  score_BFI_N = list(mean = 23.41085, sd = 6.200986),
  score_AMAS_total = list(mean = 18.08333, sd = 6.487242),
  age = list(mean = 23.37016, sd = 4.241794),
  score_GAD = list(mean = 12.48256, sd = 3.958239),
  score_PISA_ME = list(mean = 20.58915, sd = 3.328284),
  score_STAI_state_short = list(mean = 8.372093, sd = 2.966662),
  score_TAI_short = list(mean = 11.01163, sd = 3.716646),
  score_SDQ_L = list(mean = 13.7655, sd = 2.350222),
  score_SDQ_M = list(mean = 11.69186, sd = 3.078269)
)

# Apply scaling to 'age' and 'score_gad' using 'predictor' as the loop variable
# Apply scaling to each variable in the test dataset
for (predictor in names(scaling_params)) {
  if (predictor %in% colnames(test2)) {  # Check if the predictor exists in the dataset
    new_mean <- scaling_params[[predictor]]$mean
    new_sd <- scaling_params[[predictor]]$sd
    
    # C
    # Overwrite the original predictor with the scaled version
    test2[[predictor]] <- (test2[[predictor]] - new_mean) / new_sd
  } else {
    warning(paste("Variable", predictor, "not found in test dataset."))
  }
}

View(test2)
View(training2)
library(caret)

control <- trainControl(method="repeatedcv", number=10, repeats=3, search = "grid")
tunegrid <- expand.grid(.mtry=c(2:8))
set.seed(100)
rf_lasso <- train(sum_arith_perf~sex+score_TAI_short+score_STAI_state_short+score_PISA_ME+score_SDQ_M+score_SDQ_L, data=scaled_training2, method="rf", importance=TRUE, tuneLength=10, trControl=control)

print(rf_lasso)

View(test2)
View(scaled_training2)

predictionslassomodel<-predict(rf_lasso,test2)
validationlassomodel<-data.frame(R2 = R2(predictionslassomodel, test2 $ sum_arith_perf), 
		RMSE = RMSE(predictionslassomodel, test2 $ sum_arith_perf), 
		MAE = MAE(predictionslassomodel, test2 $ sum_arith_perf))#these statistical functions are in caret technically under trainControl(), which passes this argument through defaultSummary() I believe.
print(validationlassomodel)
#middling prediction, pretty much the same as the two variable model actually.
##put it back in the linear model for fun
linearlassomodel2<-lm(sum_arith_yj~+sex+score_PISA_ME+score_STAI_state_short+score_SDQ_M, data=scaled_training2)
summary(linearlassomodel2)
ols_plot_diagnostics(linearlassomodel2)

#1se version
set.seed(101)
rf_lasso_1se <- train(sum_arith_perf~sex+score_PISA_ME+score_SDQ_M, data=scaled_training2, method="rf", importance=TRUE, tuneLength=10, trControl=control)
print(rf_lasso_1se)


predictionslasso1semodel<-predict(rf_lasso_1se,test2)
validationlassonoamasmodel<-data.frame(R2 = R2(predictionslasso1semodel, test2 $ sum_arith_perf), 
		RMSE = RMSE(predictionslasso1semodel, test2 $ sum_arith_perf), 
		MAE = MAE(predictionslasso1semodel, test2 $ sum_arith_perf))#these statistical functions are in caret technically under trainControl(), which passes this argument through defaultSummary() I believe.
print(validationlassonoamasmodel)



```

As we have determined across the two analysis methods we have used, age may be a useful variable to include in a strictly predictive model, but it does not seem to be a "key" variable in the sense that it has very little relationship with the dependent variable. Age may help clear up the picture of the dependent variable, but without sex, math self efficacy and math self-concept, it is essentially worthless.

It appears clear that the second LASSO model only reports the necessary variables to explain the DV's variance, whereas the first model includes covariates to produce an "optimal" model without too many covariates, but enough to have better predictive power than say, a 5 variable model. In fact, such a model is likely better at prediction than the fourth-selection model, which despite having the highest R squared of any model so far, may not generalize predictive ability as well as the first LASSO model outside of the relatively small test sample size. Across models, the inclusion of the age variable does not seem to contribute highly to the R squared of our models. The other seemingly key covariates of state anxiety and possibly text anxiety seem to add greater benefit in predictive modeling after we have naturally included the three main variables of sex, math self-efficacy, and math self-concept.

```{r}

```

```{r}
library(caret)
set.seed(101)
control <- trainControl(method="repeatedcv", number=10, repeats=3, search = "random")
mtry <- 2

rf_lasso2 <- train(sum_arith_perf~sex+scale(score_AMAS_total)+scale(score_STAI_state_short)+scale(score_PISA_ME)+scale(score_SDQ_M), data=training, method="rf", importance=TRUE, tuneLength=10, trControl=control)
print(rf_lasso2)

View(training2)
View(test2)


predictionslassomodel2<-predict(rf_lasso2,test)
validationlassomodel2<-data.frame(R2 = R2(predictionslassomodel2, test2$ sum_arith_perf), 
		RMSE = RMSE(predictionslassomodel2, test2$ sum_arith_perf), 
		MAE = MAE(predictionslassomodel2, test2$ sum_arith_perf))#these statistical functions are in caret technically under trainControl(), which passes this argument through defaultSummary() I believe.
print(validationlassomodel2)
#Insanely good prediction. Slight improvement on the two variable model. This is likely the best model if you're looking for 4-5 variables, I'd wager.
```
